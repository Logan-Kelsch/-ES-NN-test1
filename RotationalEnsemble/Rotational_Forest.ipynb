{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hello\n",
    "\n",
    "#OLD DATA PREPPING\n",
    "'''\n",
    "#Logan Kelsch + JJ\n",
    "'''\n",
    "'''\n",
    "/*\n",
    " *  taking this version of neural network training takes only a few features\n",
    " *          [ high, low, open, close, volume, TimeOfDay, DayOfWeek ]\n",
    " *  and uses common techniques described in 'features_creation.py' to largely\n",
    " *  expand dimensionality. \n",
    " *  As of writing this (11/24/24) there is limited expansion\n",
    "*/\n",
    "''' \n",
    "'''\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from funcs_data_process import *\n",
    "from feature_creation import *\n",
    "from performance_printout import *\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('betaset_tmp.csv')\n",
    "\n",
    "#---------------#\n",
    "    MODEL VARIABLES\n",
    "#---------------#\n",
    "\n",
    "\n",
    "indp_size       = 0.05       #size of indepentendent set of samples\n",
    "test_size       = 0.15       #size of validation set of samples\n",
    "num_isol_feats  = 0         #number of features to be left out of PCA transformation\n",
    "                            #   this function is depricated/needs updated\n",
    "use_PCA         = False      #bool for use of PCA feature decomp. and transformation\n",
    "comps_PCA       = 128        #number of primary components to use under PCA t.\n",
    "\n",
    "time_steps      = 5        #LSTM time steps used\n",
    "\n",
    "t_start         = 570       #start time for time based sample filtering         570 is 9:30am EST\n",
    "t_end           = 720       #end   time for time based sample filtering         645,720 are 10:45am,12:00pm EST\n",
    "\n",
    "params = model_params()\n",
    "params = get_model_params(\n",
    "    m_type          = 'Classification'      # {'Regression', 'Classification'}\n",
    "\n",
    "   ,target_time     = 5        #how many minutes in the future is the target\n",
    "   ,c_split_val     = 5\n",
    "   ,c_class_cnt     = 2\n",
    ")\n",
    "\n",
    "\n",
    "#drop unused target columns\n",
    "data = set_target(data, params)\n",
    "\n",
    "#collect all sample indices to be kept through time filter\n",
    "#two seperate functions to keep main and ind test isolated\n",
    "keep_ndx = grab_wanted_times(data.values, t_start, t_end, time_steps)\n",
    "\n",
    "data = data.drop(columns=return_name_collection())\n",
    "#data = data.drop(columns=fn_orig_time())\n",
    "\n",
    "#confirmation printout of all features/targets\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "params = update_class_weights(y, params)\n",
    "\n",
    "if(params.model_type=='Classification'):\n",
    "    y, label_encoder = y_preprocess(params, y)\n",
    "\n",
    "X_nrm = normalize_from_tt_split(X, X, test_size)\n",
    "if(use_PCA):\n",
    "    X = reform_with_PCA_isolated(X_nrm, X_nrm, test_size, num_isol_feats, comps_PCA)\n",
    "#X, y = reformat_to_lstm(X, y, time_steps)\n",
    "\n",
    "print('\\nX shape == {}.'.format(X.shape))\n",
    "print('y shape == {}.\\n'.format(y.shape))\n",
    "\n",
    "\n",
    "print(f'Raw Sample Count:\\t{len(X)}')\n",
    "X,     y     = filter_times(X,     y,     keep_ndx)\n",
    "print(f'Remaining Sample Count:\\t{len(X)}')\n",
    "\n",
    "#split all samples 3 ways into training, and testing\n",
    "#   and split all testing into validation and independent\n",
    "X_train, X_val, X_ind, y_train, y_val, y_ind =\\\n",
    "    split_into_train_val_ind(X, y, test_size, indp_size, time_steps)\n",
    "\n",
    "if(params.model_type=='Classification'):\n",
    "    inv_trn_y = label_encoder.inverse_transform(y_train)\n",
    "    params = update_class_weights(inv_trn_y, params)\n",
    "\n",
    "print('\\nX_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))\n",
    "print('X_val shape == {}.'.format(X_val.shape))\n",
    "print('y_val shape == {}.'.format(y_val.shape))\n",
    "print('X_ind shape == {}.'.format(X_ind.shape))\n",
    "print('y_ind shape == {}.\\n'.format(y_ind.shape))\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
