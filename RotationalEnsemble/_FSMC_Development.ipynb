{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file is for the penultimate development of this 4-Level Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRAINING SUBSECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE Load in the training data for level-0 models'''\n",
    "\n",
    "import _Data_Processing\n",
    "import joblib\n",
    "from importlib import reload\n",
    "\n",
    "'''\n",
    "this function returns 7 objects:\n",
    "\tdata: sets of data as X_train, X_val, X_ind, y_train, y_val, y_ind\n",
    "\tfeature_list: list of all feature names with a column index (dict)\n",
    "'''\n",
    "\n",
    "reload(_Data_Processing)\n",
    "\n",
    "lstm_format = False\n",
    "\n",
    "X, X_train, X_val, X_ind,\\\n",
    "y, y_train, y_val, y_ind,\\\n",
    "feature_subsets, scaler =\\\n",
    "_Data_Processing.preprocess_data(\n",
    "\tfile_name   =\t\t'spx_large_stoch.csv'\n",
    "\t,indp_size  =\t\t0.01\n",
    "\t,test_size  =\t\t0.20\n",
    "\t,shfl_splt\t=\t\tFalse\n",
    "\t,t_start    =\t\t600\n",
    "\t,t_end\t\t=\t\t780\n",
    "\t,mod_type\t=\t\t'Stochastic_Classification'\n",
    "    #stoch_class format is #classes,#mins-for-range,#mins-in-future\n",
    "\t,target_t\t=\t\t'4_60_15'\n",
    "\t,num_class\t=\t\t4\n",
    "\t,split_val\t=\t\t5\n",
    "\t,verbose\t=\t\t1\n",
    "\t,scaler\t\t=\t\t'Standard'\n",
    "\t,frmt_lstm\t=\t\tlstm_format\n",
    "\t,keep_price =\t\tTrue\n",
    "    ,keep_time\t=\t\tTrue\n",
    "\t,optm_data\t=\t\tTrue\n",
    "    ,indices\t=\t\t0\n",
    ")\n",
    "joblib.dump(scaler, 'scaler/tmp_sc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in feature_subsets:\n",
    "    print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE Shred the data to run parallel to desired level-0 structure'''\n",
    "\n",
    "import _Data_Rotating\n",
    "from importlib import reload\n",
    "'''\n",
    "This function returns 4 items.\n",
    "-\t2D array of X partitions, by featurespace and samplespace\n",
    "-\t1D array of index-lists for each featurespace-partition specific features\n",
    "-\t1D array of featurespace-partition specific rotation transformer functions\n",
    "-\t1D array of y partitions for each samplespace partition.\n",
    "'''\n",
    "\n",
    "reload(_Data_Rotating)\n",
    "\n",
    "X_train_parts, X_find_parts, X_trans_parts, y_parts = _Data_Rotating.rotate_partitions(\n",
    "\tX\t\t\t\t=\tX_train\n",
    "\t,y\t\t\t\t=\ty_train\n",
    "\t,n_feat_parts\t=\t1\n",
    "\t,feat_subsets\t=\tfeature_subsets\n",
    "\t,feat_part_type\t=\t'specific_subsets'\n",
    "    ,specific_sbsts\t=\t[0]\n",
    "\t,fraction_feats\t=\t1.0\n",
    "\t,no_feat_overlap=\tFalse\n",
    "\t,feats_for_all\t=\t[]\n",
    "\t,rotation_type\t=\t'PCA'\n",
    "\t,rotation_filter=\tFalse\n",
    "\t,filter_type\t=\t'Retention'\n",
    "\t,filter_value\t=\t0.95\n",
    "\t,n_sample_parts\t=\t1\n",
    "\t,smpl_part_type\t=\t'Even'\n",
    "\t,sample_overlap\t=\t0.9\n",
    "\t,sample_shuffle\t=\tFalse\n",
    "\t,lstm_format\t=\tlstm_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE Begin training for each level-0 model'''\n",
    "\n",
    "import _Modelset_Training\n",
    "from importlib import reload\n",
    "\n",
    "'''\n",
    "Function returns a 3D list of trained models that are fit to the resepective dataset.\n",
    "\tDim1:\tFeaturespace, here are the subsetctions/partitions for each unique set of features\n",
    "\tDim2:\tSamplespace, here are the subsetctions/partitions for each unique set of samples\n",
    "\tDim3:\tModelspace, here are the individual models trained on a unique partition\n",
    "'''\n",
    "reload(_Modelset_Training)\n",
    "\n",
    "models = _Modelset_Training.train_models(\n",
    "\tmodel_types\t=\t['nn']\n",
    "\t,data_parts\t=\tX_train_parts\n",
    "\t,findx_parts=\tX_find_parts\n",
    "\t,trans_parts=\tX_trans_parts\n",
    "\t,trgt_parts\t=\ty_parts\n",
    "\t,X_valid\t=\tX_val\n",
    "\t,y_valid\t=\ty_val\n",
    "\t,param_mode\t=\t'custom'\n",
    "\t,cst_mod_prm=\t[{\n",
    "\t\t'optimizer_type'\t:\t'Adam'\n",
    "\t\t,'optimizer_kwarg':{\n",
    "\t\t\t'learning_rate'\t:\t0.001\n",
    "\t\t\t#,'momentum'\t\t:\t0.9\n",
    "            #,'weight_decay'\t\t:\t0.0001\n",
    "\t\t\t}\n",
    "\t\t,'time_steps'\t\t:\t3\n",
    "\t\t,'LSTM'\t\t\t\t:\tFalse\n",
    "\t\t,'shuffle_train'\t:\tTrue\n",
    "\t\t,'epochs'\t\t\t:\t50\n",
    "\t\t,'rlr_patience'\t\t:\t20\n",
    "        ,'batch_size'\t\t:\t4\n",
    "\t\t,'architecture'\t:\t'default_deep'\n",
    "\t\t#,'external_cw'\t:\t_Utility.get_class_weights(y_train)\n",
    "\t\t}]\n",
    "\t,tnr_verbose=\tTrue\n",
    "\t,lstm_frmt\t=\tlstm_format\n",
    "\t,use_cls_wt\t=\tTrue\n",
    "    ,nn_kwargs\t=\t\n",
    "    \t{'predict_mode':'Multi-Class',\n",
    "\t\t 'class_count':4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.unique(y_parts[0]))\n",
    "print(y_parts[0][0].shape)\n",
    "print(type(y_parts[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE show performances of level-0 Models'''\n",
    "\n",
    "import _Model_Evaluation\n",
    "import _Neural_Net\n",
    "\n",
    "reload(_Model_Evaluation)\n",
    "reload(_Neural_Net)\n",
    "\n",
    "_Model_Evaluation.evaluate_models(\n",
    "\t\tmodels=models\n",
    "\t\t,X_findx=X_find_parts\n",
    "\t\t,X_trans=X_trans_parts\n",
    "\t\t,X_train=X_train_parts\n",
    "\t\t,y_train=y_parts\n",
    "\t\t,X_test=X_val\n",
    "\t\t,y_test=y_val\n",
    "\t\t,prfm_gnrl\t=\t'all'\n",
    "\t\t,prfm_stat\t=\t'all'\n",
    "\t\t,disp_mthd\t=\t'as_value'\n",
    "\t\t,test_whch\t=\t'independent'\n",
    "\t\t,pred_type\t=\t'classification'\n",
    "\t\t,lstm_frmt\t=\tlstm_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE begin training for level-1 model based off of train level-0 model predictions'''\n",
    "\n",
    "import _Metamodel\n",
    "from importlib import reload\n",
    "import _Utility\n",
    "\n",
    "reload(_Metamodel)\n",
    "\n",
    "metamodel, prediction_set = _Metamodel.train_test_meta_model(\n",
    "\tmodels\t\t=\tmodels\n",
    "\t,X_findx\t=\tX_find_parts\n",
    "\t,X_trans\t=\tX_trans_parts\n",
    "\t,X_test\t\t=\tX_val\n",
    "\t,y_test\t\t=\ty_val\n",
    "\t,val_size\t=\t0.2\n",
    "\t,shuffle\t=\tTrue\n",
    "\t,metam_type =\t'NN'\n",
    "\t,use_cls_wt =\tTrue\n",
    "\t,use_mm_params=\tTrue\n",
    "    ,meta_scaler=\t'Standard'\n",
    "\t,metam_params={\n",
    "\t\t'optimizer_type':\t'Adam'\n",
    "\t\t,'optimizer_kwarg':{\n",
    "\t\t\t'learning_rate'\t:\t0.00025\n",
    "            #,'weight_decay'\t:\t0.001\n",
    "\t\t\t#,'momentum':0.9\n",
    "\t\t\t}\n",
    "\t\t,'time_steps'\t\t:\t5\n",
    "\t\t,'LSTM'\t\t\t\t:\tFalse\n",
    "\t\t,'shuffle_train'\t:\tTrue\n",
    "\t\t,'epochs'\t\t\t:\t75\n",
    "\t\t,'batch_size'\t\t:\t30\n",
    "\t\t,'rlr_patience'\t\t:\t10\n",
    "\t\t,'architecture'\t:\t'default_deep'\n",
    "\t\t#,'custom_val_data'\t:\t(X_train, y_train)\n",
    "\t\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NOTE load in data for testing quality of level-1/2 models'''\n",
    "\n",
    "import _Data_Processing\n",
    "from importlib import reload\n",
    "reload(_Data_Processing)\n",
    "lstm_format = False\n",
    "X_test, _, _, __,\\\n",
    "y_test, _, ___, ____,\\\n",
    "feature_subsets, scaler =\\\n",
    "_Data_Processing.preprocess_data(\n",
    "\tfile_name   =\t\t'spx_test2.csv'\n",
    "\t,indp_size  =\t\t0.01\n",
    "\t,test_size  =\t\t0.01\n",
    "\t,shfl_splt\t=\t\tFalse\n",
    "\t,t_start    =\t\t645\n",
    "\t,t_end\t\t=\t\t800\n",
    "\t,mod_type\t=\t\t'Area_Classification'\n",
    "\t,target_t\t=\t\t60\n",
    "\t,num_class\t=\t\t2\n",
    "\t,split_val\t=\t\t5\n",
    "\t,verbose\t=\t\t1\n",
    "\t,scaler\t\t=\t\t'Custom'\n",
    "    ,cstm_scale\t=\t\tjoblib.load('scaler/tmp.joblib')\n",
    "\t,frmt_lstm\t=\t\tlstm_format\n",
    "\t,keep_price =\t\tTrue\n",
    "    ,indices\t=\t\t0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import _Master_Model\n",
    "reload(_Master_Model)\n",
    "reload(_Utility)\n",
    "from importlib import reload\n",
    "from sklearn.svm import SVC\n",
    "from _Utility import get_class_weights\n",
    "\n",
    "shallow_master = _Master_Model.Master(\n",
    "\tmodel_depth\t\t=\t2\n",
    "\t,all_models \t=\t[models, metamodel]\n",
    "\t,lvl0_formatters=\t[X_find_parts, X_trans_parts]\n",
    "\t,lvl1_formatters=\t[joblib.load('scaler/meta.joblib')]\n",
    ")\n",
    "\n",
    "X_blvl2 = X_val\n",
    "y_blvl2 = y_val\n",
    "\n",
    "pred_2 = shallow_master.master_predict(X_blvl2, mode='proba')\n",
    "cm_vals = _Utility.get_cm_values(y_blvl2, pred_2)\n",
    "\n",
    "_Utility.show_confusion_matrix(y_blvl2,pred_2,title=f'Level-1 Prediction\\nPrecision: {round(precision_score(y_blvl2,pred_2)*100, 2)}%')\n",
    "\n",
    "df = pd.DataFrame(X_blvl2)\n",
    "df['score'] = cm_vals\n",
    "df['target'] = y_blvl2\n",
    "kept_indices = df.index[~(df['score'] % 2 == 0)].tolist()\n",
    "df = df.drop(df[df['score']%2==0].index).reset_index(drop=True)\n",
    "df = df.drop(columns=['score']).reset_index(drop=True)\n",
    "#pd.set_option('display.max_rows',None)\n",
    "co = df.corr()['target'].drop('target')\n",
    "#print(co.sort_values())\n",
    "p = co.nlargest(5).index.tolist()\n",
    "n = co.nsmallest(5).index.tolist()\n",
    "feats = p+n\n",
    "\n",
    "df_pair = pd.DataFrame(X_blvl2)\n",
    "df_pair = df_pair.iloc[kept_indices].reset_index(drop=True)\n",
    "df_pair = df_pair.iloc[:, feats]\n",
    "\n",
    "X_svm = df_pair.values\n",
    "y_svm = df['target'].values\n",
    "\n",
    "y_one = y_blvl2[kept_indices]\n",
    "\n",
    "clf = SVC(kernel='linear',C=1.0,class_weight=_Utility.get_class_weights(y_one)).fit(X_svm, y_svm)\n",
    "\n",
    "y_svmpred = pred_2#clf.predict(X_svm)\n",
    "#'polishing' predictions based on if level-1 predicted (1)\n",
    "for p in range(len(y_svmpred)):\n",
    "\tif(y_svmpred[p] == 1):\n",
    "\t\ty_svmpred[p] = clf.predict(X[p,feats].reshape(1, -1))\n",
    "\n",
    "#_Utility.show_confusion_matrix(df['target'],y_svmpred,title=f'Level-2 Prediction\\nPrecision: {round(precision_score(y_svm, y_svmpred)*100, 2)}%')\n",
    "_Utility.show_confusion_matrix(y_blvl2,y_svmpred,title=f'Level-2 Prediction\\nPrecision: {round(precision_score(y_blvl2, y_svmpred)*100, 2)}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(_Master_Model)\n",
    "reload(_Utility)\n",
    "deep_master = _Master_Model.Master(\n",
    "\tmodel_depth\t\t=\t3\n",
    "\t,all_models \t=\t[models, metamodel.model, clf]\n",
    "\t,lvl0_formatters=\t[X_find_parts, X_trans_parts]\n",
    "    ,lvl2_formatters=\t[feats]\n",
    ")\n",
    "\n",
    "y_3pred = deep_master.master_predict(X_test)\n",
    "\n",
    "_Utility.show_confusion_matrix(y_test, y_3pred, title=f'Accuracy: {_Utility.get_accuracy(y_test, y_3pred)}\\nPrecision: {_Utility.get_precision(y_test, y_3pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _Master_Model\n",
    "from importlib import reload\n",
    "reload(_Master_Model)\n",
    "#deep_master = _Master_Model.Master(model_depth=3)\n",
    "#deep_master.load_model('pre63p2-645-800')\n",
    "deep_master.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _Time_Ensemble\n",
    "import _Master_Model\n",
    "import joblib\n",
    "import _Utility\n",
    "\n",
    "'''NOTE load in data for testing quality of level-1/2 models'''\n",
    "\n",
    "import _Data_Processing\n",
    "from importlib import reload\n",
    "reload(_Data_Processing)\n",
    "reload(_Master_Model)\n",
    "reload(_Time_Ensemble)\n",
    "lstm_format = False\n",
    "X_te, _, _, __,\\\n",
    "y_te, _, ___, ____,\\\n",
    "feature_subsets, scaler =\\\n",
    "_Data_Processing.preprocess_data(\n",
    "\tfile_name   =\t\t'spx_test2.csv'\n",
    "\t,indp_size  =\t\t0.01\n",
    "\t,test_size  =\t\t0.01\n",
    "\t,shfl_splt\t=\t\tFalse\n",
    "\t,t_start    =\t\t600\n",
    "\t,t_end\t\t=\t\t950\n",
    "\t,mod_type\t=\t\t'Area_Classification'\n",
    "\t,target_t\t=\t\t60\n",
    "\t,num_class\t=\t\t2\n",
    "\t,split_val\t=\t\t5\n",
    "\t,verbose\t=\t\t0\n",
    "\t,scaler\t\t=\t\t'Custom'\n",
    "    ,cstm_scale\t=\t\tjoblib.load('scaler/tmp.joblib')\n",
    "\t,frmt_lstm\t=\t\tlstm_format\n",
    "\t,keep_price =\t\tTrue\n",
    "    ,keep_time\t=\t\tTrue\n",
    "    ,indices\t=\t\t0\n",
    ")\n",
    "\n",
    "chronos_array = _Time_Ensemble.chronos_predict(X_te,                                                           \n",
    "['pre63p2-645-800','models/m55','models/m60','models/m50'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(_Utility)\n",
    "reload(_Time_Ensemble)\n",
    "print(type(chronos_array[0]))\n",
    "t_pred\t=\t_Time_Ensemble.chronos_fusion(master_predictions=chronos_array, fusion_method='mv',vote_var=3)\n",
    "_Utility.show_confusion_matrix(y_te, t_pred, title=f'Accuracy: {_Utility.get_accuracy(y_te, t_pred)}\\nPrecision: {_Utility.get_precision(y_te, t_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(_Utility)\n",
    "#reload(_Time_Ensemble)\n",
    "#vals = _Utility.graph_range(_Time_Ensemble.chronos_fusion, kw='vote_var', kw_range=range(0,5), show_graph=False, master_predictions=chronos_array,fusion_method='mv')\n",
    "#scores = [precision_score(y_te, vals[i]) for i in range(0,5)]\n",
    "#_Utility.plot_standard_line(scores, range(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from _Utility import get_name_from_fss\n",
    "#get_name_from_fss(fss=feature_subsets,index=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fs in feature_subsets:\n",
    "\tprint(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing chart of predictions\n",
    "X_raw, _, _, __,\\\n",
    "y_raw, _, ___, ____,\\\n",
    "feature_subsets, scaler =\\\n",
    "_Data_Processing.preprocess_data(\n",
    "\tfile_name   =\t\t'spx_test2.csv'\n",
    "\t,indp_size  =\t\t0.01\n",
    "\t,test_size  =\t\t0.01\n",
    "\t,shfl_splt\t=\t\tFalse\n",
    "\t,t_start    =\t\t600\n",
    "\t,t_end\t\t=\t\t950\n",
    "\t,mod_type\t=\t\t'Area_Classification'\n",
    "\t,target_t\t=\t\t60\n",
    "\t,num_class\t=\t\t2\n",
    "\t,split_val\t=\t\t5\n",
    "\t,verbose\t=\t\t0\n",
    "\t,scaler\t\t=\t\t'None'\n",
    "    ,cstm_scale\t=\t\tjoblib.load('scaler/tmp.joblib')\n",
    "\t,frmt_lstm\t=\t\tlstm_format\n",
    "\t,keep_price =\t\tTrue\n",
    "    ,keep_time\t=\t\tTrue\n",
    "    ,indices\t=\t\t0\n",
    ")\n",
    "\n",
    "reload(_Utility)\n",
    "signals = _Utility.show_predictions_chart(X_raw=X_raw,predictions=t_pred, t_start=600, t_end=900\n",
    "                                , add_chart=[517]\n",
    "                                , fss=feature_subsets\n",
    "                                , naked_features=False\n",
    "                                , signal=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "reload(_Utility)\n",
    "tpl = _Utility.backtester(\n",
    "    X_raw=X_raw,\n",
    "    signals=signals,\n",
    "    method='time_held',\n",
    "    value=(0,20),#(0,5,1.5,30),\n",
    "    fees=None)\n",
    "\n",
    "print(tpl)\n",
    "\n",
    "#_Utility.graph_range(function=_Utility.backtester,\n",
    "#\tkw='value',\n",
    "#\tkw_range=[(0, i, 2, 30) for i in range(1,20)],\n",
    "#    \n",
    "#\t\t#function specific parameteres\n",
    "#        method='RR_ratio',\n",
    "#\t\tX_raw=X_raw,\n",
    "#        signals=signals,\n",
    "#        fees=None\n",
    "#\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
