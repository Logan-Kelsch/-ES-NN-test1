{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just taking a gander at the data for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Loading in the data ''' \n",
    "#Logan Kelsch + JJ\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC  # Understanding data dimensionality\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load a portion of the dataset for simplicity (wanting to see general stuff and whatnot)\n",
    "csv_file_path = r\"C:\\Users\\jairi\\OneDrive\\Desktop\\Repos\\Stock-NN\\-ES-NN-test1\\CollectedData\\DATA_V4.0\\catted\\catted_2.csv\"\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Normalizing data\n",
    "normalizer = MinMaxScaler()\n",
    "X = normalizer.fit_transform(X)\n",
    "y = normalizer.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Simply split data into training and testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components Needed for 0.999 variance: 30\n",
      "New X shape (after transformation): (28716, 30)\n",
      "[[ 5.71711299e-02 -4.33819264e-04 -8.69864494e-04 ... -3.19228973e-03\n",
      "  -4.59288318e-03 -8.00636970e-03]\n",
      " [ 6.93838407e-01  8.19646708e-04  1.51648771e-03 ...  2.11112566e-03\n",
      "   3.23926076e-03  5.92615852e-03]\n",
      " [-1.14769396e-01 -1.25727518e-03 -2.30915080e-03 ... -4.93646416e-04\n",
      "  -8.48003762e-04 -1.62872126e-03]\n",
      " ...\n",
      " [ 1.22141275e-04 -3.93590500e-02 -2.65728168e-03 ... -4.78066478e-01\n",
      "   2.06100208e-01 -2.35849811e-02]\n",
      " [-1.70611473e-04 -1.62884508e-01  2.29021861e-02 ...  8.38974451e-02\n",
      "  -3.74867419e-02  8.86524175e-03]\n",
      " [ 8.30760792e-04 -4.57041397e-02 -9.62768360e-03 ...  6.26197062e-02\n",
      "  -2.79124203e-02  2.67252227e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Run PCA to reduce dimensionality and whatnot\n",
    "MINIMUM_VARIANCE = .999\n",
    "pca_classifier = PCA()\n",
    "pca_classifier.fit(X)\n",
    "cumulative_variance = np.cumsum(pca_classifier.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_variance >= MINIMUM_VARIANCE) + 1\n",
    "print(f\"Number of Components Needed for {MINIMUM_VARIANCE} variance: {n_components}\")\n",
    "\n",
    "# Now, transform dataset with given n_components\n",
    "pca = PCA(n_components=n_components)\n",
    "X_new = pca.fit_transform(X)\n",
    "print(f\"New X shape (after transformation): {X_new.shape}\")  # Showing reduced dimensionality\n",
    "selected_components = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model each feature against itself\n",
    "\n",
    "X_new_df = pd.DataFrame(X_new)  # Convert to DataFrame\n",
    "\n",
    "# Make a directory to save each of these graphs\n",
    "output_folder = \"featuregraphing1\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for col1 in X_new_df.columns:\n",
    "    for col2 in X_new_df.columns:\n",
    "        if col1 != col2:  # Avoid plotting a feature against itself\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.scatter(X_new_df[col1], X_new_df[col2], alpha=0.7)\n",
    "            plt.title(f'Scatterplot: {col1} vs {col2}')\n",
    "            plt.xlabel(col1)\n",
    "            plt.ylabel(col2)\n",
    "            plt.grid(True)\n",
    "            # Save to the output folder\n",
    "            plot_filename = os.path.join(output_folder, f\"{col1}vs{col2}.png\")\n",
    "            plt.savefig(plot_filename)\n",
    "            plt.close()  # Close to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph each feature to the target\n",
    "\n",
    "# Make a directory to save each of these graphs\n",
    "output_folder = \"featuregraphing2\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for col1 in X_new_df.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(X_new_df[col1], y, alpha=0.7)\n",
    "    plt.title(f'Scatterplot: {col1} vs target')\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    plt.grid(True)\n",
    "    # Save to the output folder\n",
    "    plot_filename = os.path.join(output_folder, f\"{col1}vs{col2}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()  # Close to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Creating a SVM Model for each feature to target to see what happens\n",
    "# NOTE: I need to do some more feature transformation (likely mapping outputs to classification) in order to do this lil analysis\n",
    "\n",
    "hella_svms = []\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_new_df, y)\n",
    "\n",
    "for col in X_train.columns:\n",
    "    smodel = SVC()  # Default arguments for now\n",
    "    smodel.fit(X_train[[col]], y_train)\n",
    "    y_pred = smodel.predict(X_val[[col]])\n",
    "    print(f\"Accuracy Score for column {col}: {accuracy_score(y_pred=y_pred, y_true=y_val)}\")\n",
    "    hella_svms.append(smodel)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
