{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import data_processing as dp\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('../spx_vhs.csv')\n",
    "\n",
    "data = data.drop(columns=['high','low','close','time','vel_15'])\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "X, y = dp.make_prll_trgt(data=data.values, index=6,offset=60)\n",
    "\n",
    "X = np.delete(X, 6, axis=1)\n",
    "\n",
    "use_lstm = True\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "if(use_lstm):\n",
    "    X, y = dp.reformat_to_lstm(X, y, 15)\n",
    "\n",
    "tod_mask = (X[:, 0] >= 600) & (X[:, 0] <= 900)\n",
    "\n",
    "#X = X[tod_mask]\n",
    "#y = y[tod_mask]\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, shuffle=False)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.025)\n",
    "opt1 = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(4, activation='tanh', recurrent_dropout=0.0, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(4, activation='tanh', recurrent_dropout=0.0),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Dense(64),\n",
    "\t\t#tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Activation('relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        #tf.keras.layers.Dense(8, activation='relu'),       \n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    rmse='root_mean_squared_error'\n",
    "\n",
    "    model.compile(optimizer=opt1, loss='mse', metrics=['mae','R2Score',rmse])\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_R2Score', patience=15, mode='max', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5, \n",
    "    patience=8, \n",
    "    min_lr=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "model = build_model()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=512, validation_split=0.2, verbose=1, shuffle=False, callbacks=[early_stopping,reduce_lr])\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(y<-5) + np.mean(y>5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import data_processing as dp\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('../spx_vhs.csv')\n",
    "\n",
    "data = data.drop(columns=['high','low','time','vel_15'])\n",
    "\n",
    "\n",
    "\n",
    "#X, y = dp.make_prll_trgt(data=data.values, index=5,offset=1)\n",
    "y = dp.make_vhs_target(data.values,close_index=0,s_index=6,mode='breakout')\n",
    "\n",
    "data = data.drop(columns=['close','volume'])\n",
    "\n",
    "print(data.columns)\n",
    "X = data.values\n",
    "\n",
    "y*=5000\n",
    "\n",
    "print(y.max(), y.min())\n",
    "\n",
    "#X = np.delete(X, 6, axis=1)\n",
    "\n",
    "use_lstm = True\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "#print((570 - scaler.mean_[1]) / scaler.scale_[1])\n",
    "#print((900 - scaler.mean_[1]) / scaler.scale_[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa81406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "t_start = 570\n",
    "t_end = 900\n",
    "\n",
    "\n",
    "# ─── prepare & split days ───────────────────────────────────────────\n",
    "# X: (N, F), y: (N,)\n",
    "\n",
    "# 1) infer day IDs\n",
    "times  = X[:, 0]\n",
    "day_id = np.concatenate([[0], np.cumsum(times[1:] < times[:-1])])\n",
    "\n",
    "# 2) find days with any data in [570, 900]\n",
    "#filtered_days = np.unique(day_id[(times >= (t_start - scaler.mean_[1]) / scaler.scale_[1]) & (times <= (t_end - scaler.mean_[1]) / scaler.scale_[1])])\n",
    "filtered_days = np.unique(day_id[(times >= t_start) & (times <= t_end)])\n",
    "\n",
    "#print(len(filtered_days))\n",
    "\n",
    "# 3) shuffle & 80/20 split\n",
    "np.random.shuffle(filtered_days)\n",
    "split       = int(len(filtered_days) * 0.8)\n",
    "train_days  = filtered_days[:split]\n",
    "test_days   = filtered_days[split:]\n",
    "\n",
    "# ─── build tf.data.Datasets ────────────────────────────────────────\n",
    "ds_train = (\n",
    "    dp.make_day_dataset(X, y, scaler,\n",
    "                     time_idx=0,\n",
    "                     min_time=570,\n",
    "                     max_time=900,\n",
    "                     to_lstm=True,\n",
    "                     seq_len=30,\n",
    "                     include_days=train_days)\n",
    "    .repeat()\n",
    "    .shuffle(buffer_size=max(len(train_days), 1))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "ds_test = (\n",
    "    dp.make_day_dataset(X, y, scaler,\n",
    "                     time_idx=0,\n",
    "                     min_time=570,\n",
    "                     max_time=900,\n",
    "                     to_lstm=True,\n",
    "                     seq_len=30,\n",
    "                     include_days=test_days)\n",
    "    .repeat()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# ─── define & compile model ────────────────────────────────────────\n",
    "\n",
    "for x_batch, y_batch in ds_train.take(3):\n",
    "    # x_batch.shape is (1, seq_len, F)\n",
    "    print(\"batch dim:\", x_batch.shape[0],\n",
    "          \"timesteps:\", x_batch.shape[1],\n",
    "          \"features:\", x_batch.shape[2])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "opt1 = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(32, activation='tanh', recurrent_dropout=0.0, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32, activation='tanh', recurrent_dropout=0.0),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Dense(64),\n",
    "\t\t#tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Activation('relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        #tf.keras.layers.Dense(32),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Activation('gelu'),\n",
    "        #tf.keras.layers.Dropout(0.1),\n",
    "        #tf.keras.layers.Dense(8, activation='relu'),       \n",
    "        tf.keras.layers.Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    rmse='root_mean_squared_error'\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae',rmse])\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=15, mode='max', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5, \n",
    "    patience=8, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# ─── fit with validation ────────────────────────────────────────────\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=len(train_days),\n",
    "    validation_data=ds_test,\n",
    "    validation_steps=len(test_days),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "\n",
    "'''plt.plot(epochs, history.history['R2Score'], 'y', label='Training R2')\n",
    "plt.plot(epochs, history.history['val_R2Score'], 'r', label='Validation R2')\n",
    "plt.title('Training and Validation R2Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()'''\n",
    "\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = model.predict(X_test) \n",
    "#y_pred = y_pred > 0.5 # Predictions to class indices\n",
    "'''\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d',cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Direction Classification')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('y_test')\n",
    "plt.show()\n",
    "\n",
    "#SCATTERPLOT #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  \n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.grid()\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('y_test')\n",
    "plt.show()\n",
    "#DIRECTIONAL ACCURACY #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  \n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "tp5, fp5, tn5, fn5 = 0, 0, 0, 0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i]>0):\n",
    "        if(y_test[i]>0):\n",
    "            tp+=1\n",
    "        if(y_test[i]<0):\n",
    "            fp+=1\n",
    "        if(y_pred[i]>=50):\n",
    "            if(y_test[i]>50):\n",
    "                tp5+=1\n",
    "            if(y_test[i]<50):\n",
    "                fp5+=1\n",
    "    if(y_pred[i]<0):\n",
    "        if(y_test[i]<0):\n",
    "            tn+=1\n",
    "        if(y_test[i]>0):\n",
    "            fn+=1\n",
    "        if(y_pred[i]<=-5):\n",
    "            if(y_test[i]<0):\n",
    "                tn5+=1\n",
    "            if(y_test[i]>0):\n",
    "                fn5+=1\n",
    "directionalAccuracy = ((tp+tn)/(tp+fp+tn+fn))*10000//1/100\n",
    "print('Directional Accuracy:\\t\\t',directionalAccuracy)\n",
    "directionalAccuracy5guess = ((tp5+tn5)/(tp5+fp5+tn5+fn5))*10000//1/100\n",
    "print('Directional Accuracy >(+/-)5:\\t',directionalAccuracy5guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeff600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) Plot training & validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'],  label='train_loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Get one batch from the test-set and predict\n",
    "x_test, y_test = next(iter(ds_test))     # x_test: (1, seq_len, feats), y_test: (1, seq_len)\n",
    "y_pred = model.predict(x_test)           # shape (1, seq_len)\n",
    "\n",
    "# flatten to 1D\n",
    "y_true_flat = tf.reshape(y_test,   [-1]).numpy()\n",
    "y_pred_flat = tf.reshape(y_pred,   [-1]).numpy()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(y_true_flat, 'o-', label='y_true')\n",
    "plt.plot(y_pred_flat, 'x--', label='y_pred')\n",
    "plt.xlabel('Time-step Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('True vs Predicted on One Test Batch')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Collect all true & predicted values at the sample‐level\n",
    "y_trues = []\n",
    "y_preds = []\n",
    "\n",
    "# Assume ds_test is your repeated test‐day dataset,\n",
    "# and test_days is the list/array of day IDs you split off.\n",
    "for i, (x_batch, y_batch) in enumerate(ds_test):\n",
    "    if i >= len(test_days):\n",
    "        break\n",
    "\n",
    "    # x_batch: shape (1, seq_len, n_feats)\n",
    "    # y_batch: shape (1, seq_len)\n",
    "    x_seq = x_batch.numpy()            # → (1, seq_len, n_feats)\n",
    "    y_true_seq = y_batch.numpy().reshape(-1)  # → (seq_len,)\n",
    "    \n",
    "    seq_len = x_seq.shape[1]\n",
    "    n_feats  = x_seq.shape[2]\n",
    "\n",
    "    # reshape to (seq_len, 1, n_feats) so model.predict gives one output per sample\n",
    "    x_samples = x_seq.reshape((seq_len, 1, n_feats))  # (seq_len, 1, n_feats)\n",
    "    \n",
    "    # Predict all samples in this day at once\n",
    "    y_pred_seq = model.predict(x_samples, verbose=0).reshape(-1)  # → (seq_len,)\n",
    "\n",
    "    y_trues.append(y_true_seq)\n",
    "    y_preds.append(y_pred_seq)\n",
    "\n",
    "# Concatenate across all days\n",
    "y_trues = np.concatenate(y_trues)\n",
    "y_preds = np.concatenate(y_preds)\n",
    "\n",
    "# Scatter plot of true vs. predicted\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_trues, y_preds, alpha=0.5)\n",
    "plt.grid()\n",
    "lims = [min(y_trues.min(), y_preds.min()),\n",
    "        max(y_trues.max(), y_preds.max())]\n",
    "#plt.plot(lims, lims, 'k--', linewidth=1)  # y = x reference\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted Scatter')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: compute R²\n",
    "print(f\"R² score: {r2_score(y_trues, y_preds):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
