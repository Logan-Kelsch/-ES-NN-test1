{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTED FEATURES: \n",
      "Index(['FT', 'FullK', 'diffKD', 'OB', 'OS', 'vol', 's15', 's30', 's60', 'ToD',\n",
      "       'Inertias', 'percBB', 'spreadRSI', 'ADX', 'RSI', 'Wpercent', 'acc'],\n",
      "      dtype='object')\n",
      "TESTING FOR: \n",
      "CLASS\n",
      "OCCURANCES IN RAW DATA FOR CLASS: \n",
      "{'_1dn': 54378, '_2dnR': 51266, '_3upR': 50613, '_4up': 51026}\n",
      "Smallest Class Size: 7269 \n",
      "\n",
      "OCCURANCES IN OPT DATA FOR CLASS: \n",
      "{'_1dn': 7269, '_2dnR': 7269, '_3upR': 7269, '_4up': 7269}\n"
     ]
    }
   ],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('multi.csv')\n",
    "\n",
    "#testing random feature drops\n",
    "#data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='CLASS')\n",
    "\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "#filtering before splitting could be useful if ABSOLUTELY mostly comprised of 'in'\n",
    "#MARKET HOURS!\n",
    "data = data.drop(data[data['ToD'] > 945].index)\n",
    "data = data.drop(data[data['ToD'] < 545].index)\n",
    "data = data.drop(data[data['vol'] < 10000].index)\n",
    "\n",
    "up_Rows = data.drop(data[data['CLASS'] != '_4up'].index)\n",
    "upRRows = data.drop(data[data['CLASS'] != '_3upR'].index)\n",
    "dnRRows = data.drop(data[data['CLASS'] != '_2dnR'].index)\n",
    "dn_Rows = data.drop(data[data['CLASS'] != '_1dn'].index)\n",
    "\n",
    "smallestClass = min(up_Rows.index.size, upRRows.index.size, dnRRows.index.size, dn_Rows.index.size)\n",
    "print('Smallest Class Size:',smallestClass,'\\n')\n",
    "\n",
    "up_Rows = up_Rows.iloc[0:smallestClass]\n",
    "upRRows = upRRows.iloc[0:smallestClass]\n",
    "dnRRows = dnRRows.iloc[0:smallestClass]\n",
    "dn_Rows = dn_Rows.iloc[0:smallestClass]\n",
    "\n",
    "optData = pd.concat([up_Rows, upRRows, dnRRows, dn_Rows],axis=0)\n",
    "\n",
    "print(\"OCCURANCES IN OPT DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(optData.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = optData.iloc[:, :-1].values\n",
    "y = optData.iloc[:, -1].values\n",
    "\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "data2 = pd.read_csv('polishcsv.csv')\n",
    "data2 = data2.drop(data2[data2['ToD'] > 945].index)\n",
    "data2 = data2.drop(data2[data2['ToD'] < 545].index)\n",
    "data2 = data2.drop(data2[data2['vol'] < 10000].index)\n",
    "X2 = data2.iloc[:, :-1].values\n",
    "y2 = data2.iloc[:, -1].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X2)\n",
    "\n",
    "y2 = labelencoder.fit_transform(y2)\n",
    "X2 = scaler.transform(X2)\n",
    "\n",
    "\n",
    "\n",
    "from keras.saving import get_custom_objects\n",
    "from keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "get_custom_objects().clear()\n",
    "\n",
    "#CUSTOM LOSS 1_______________________________________________________________________________________________\n",
    "@register_keras_serializable(name=\"custom_loss\")\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    true_class = K.argmax(y_true, axis=-1)\n",
    "    pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Create a matrix where misclassifications (farthest ones) are heavily penalized\n",
    "    inverse_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 0))   # True class 3, predicted class 0\n",
    "    )\n",
    "    bad1_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 2)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "    )\n",
    "    bad2_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 0)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 1))   # True class 3, predicted class 0\n",
    "    )\n",
    "    # Create a matrix where correct predictions for 0-0 and 3-3 are rewarded\n",
    "    extrema_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 0)),  # True class 0, predicted class 0\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 3)),  # True class 1, predicted class 1\n",
    "    )\n",
    "    fair_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 1)),  # True class 2, predicted class 2\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 2))   # True class 3, predicted class 3\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Standard categorical crossentropy loss\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Add an additional penalty for farthest misclassifications\n",
    "    pen_inverse = 4.0  # You can adjust this penalty factor\n",
    "    pen_inverse_loss = K.cast(inverse_misclass, tf.float32) * pen_inverse\n",
    "\n",
    "    pen_bad1 = 0.0  # You can adjust this penalty factor\n",
    "    pen_bad1_loss = K.cast(bad1_misclass, tf.float32) * pen_bad1\n",
    "\n",
    "    pen_bad2 = 0.0  # You can adjust this penalty factor\n",
    "    pen_bad2_loss = K.cast(bad2_misclass, tf.float32) * pen_bad2\n",
    "\n",
    "    # Add a reward for correct 0-0 and 3-3 predictions (negative penalty)\n",
    "    rew_extrema = 4.0  # You can adjust this reward factor\n",
    "    rew_extrema_gain = K.cast(extrema_correct, tf.float32) * rew_extrema\n",
    "\n",
    "    rew_fair = 0.0  # You can adjust this reward factor\n",
    "    rew_fair_gain = K.cast(fair_correct, tf.float32) * rew_fair\n",
    "\n",
    "    # Return the combined loss: base loss + penalties - rewards\n",
    "    return base_loss + (pen_inverse_loss + pen_bad1_loss + pen_bad2_loss) - (rew_extrema_gain + rew_fair_gain)\n",
    "\n",
    "#CUSTOM LOSS 2____________________________________________________________________________________________________\n",
    "@register_keras_serializable(name=\"focal_loss\")\n",
    "def focal_loss(gamma=2.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - gamma: Focusing parameter that adjusts the rate at which easy examples are down-weighted.\n",
    "             Default value is 2. Higher values make the loss more focused on hard examples.\n",
    "             \n",
    "    - alpha: Class balancing factor to balance the loss for each class. Default is 0.25.\n",
    "             Adjust this to address class imbalance. Can be a scalar or a list of weights\n",
    "             per class.\n",
    "    \"\"\"\n",
    "    #gamma = 2.0\n",
    "    alpha = classWeights\n",
    " \n",
    "    @register_keras_serializable(name=\"focal_loss_fixed\")\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the focal loss between ground truth (y_true) and predicted values (y_pred).\n",
    "        \n",
    "        Arguments:\n",
    "        - y_true: Tensor of true labels (one-hot encoded, shape = [batch_size, num_classes]).\n",
    "        - y_pred: Tensor of predicted probabilities (shape = [batch_size, num_classes]).\n",
    "        \n",
    "        Returns:\n",
    "        - loss: A scalar tensor representing the computed focal loss.\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0) or division by zero\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Compute the cross-entropy loss (standard loss component)\n",
    "        cross_entropy_loss = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Compute the modulating factor: (1 - p_t)^gamma\n",
    "        # where p_t is the predicted probability for the true class\n",
    "        modulating_factor = tf.pow(1. - y_pred, gamma)\n",
    "        \n",
    "        # Compute the final focal loss: alpha * modulating_factor * cross_entropy_loss\n",
    "        loss = alpha * modulating_factor * cross_entropy_loss\n",
    "        \n",
    "        # Reduce the loss along the batch dimension\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "#CUSTOM LOSS 3_____________________________________________________________________________________________\n",
    "@register_keras_serializable(name=\"weighted_rec_pre_loss\")\n",
    "def weighted_rec_pre_loss(func='wr',weight=5.0):\n",
    "    \"\"\"\n",
    "    Custom loss function to optimize for recall OR for precision.\n",
    "    func should equal wr or wp\n",
    "    for weighted recall or weighted precision\n",
    "    \"\"\"\n",
    "    @register_keras_serializable(name='wp_loss')\n",
    "    def wp_loss(y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0) or division by zero\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        # Standard binary cross-entropy\n",
    "        base_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        # Apply higher weight to positive samples (to penalize false negatives more)\n",
    "        loss = (1 - y_true) * weight * base_loss + y_true * base_loss\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    @register_keras_serializable(name=\"wr_loss\")\n",
    "    def wr_loss(y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0) or division by zero\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        # Standard binary cross-entropy\n",
    "        base_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        # Apply higher weight to positive samples (to penalize false negatives more)\n",
    "        loss = weight * y_true * base_loss + (1 - y_true) * base_loss\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(func=='wr'):\n",
    "        return wr_loss\n",
    "    else:\n",
    "        return wp_loss\n",
    "#CUSTOM LOSS 4_____________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#metric_ratio = 1.0  # Initialize globally\n",
    "@register_keras_serializable(name=\"met_ratio\")\n",
    "def met_ratio(y_true, y_pred, base_weight=1.0):\n",
    "    crnt_metric_ratio = metric_ratio\n",
    "    \n",
    "    # Clip predictions to prevent log(0) or division by zero\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    \n",
    "    # Standard binary cross-entropy loss\n",
    "    base_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "    \n",
    "    # Adjust weight using precision_ratio (balance training and validation precision)\n",
    "    adjusted_weight = base_weight / (crnt_metric_ratio + 1e-7)\n",
    "    \n",
    "    # Apply the dynamic weight to false positives\n",
    "    weighted_loss = (1 - y_true) * adjusted_weight * base_loss + y_true * base_loss\n",
    "    \n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "#END CUSTOM LOSSES__________________________________________________________________________________________\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('multi_oh8.keras')#,custom_objects={\"custom_loss\":custom_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the test set results\n",
    "y_true = np.argmax(y2, axis=1)  # Convert one-hot to class indices if needed\n",
    "\n",
    "predictions = loaded_model.predict(X2)\n",
    "class_confidence_adjustments = [ 1.0 , 1.0 , 1.0 , 1.0 ]\n",
    "adjusted_predictions = predictions * class_confidence_adjustments\n",
    "\n",
    "y_pred = np.argmax(adjusted_predictions, axis=1)  # Predictions to class indices\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(4), yticklabels=range(4))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for 4-Class Classification')\n",
    "plt.show()\n",
    "\n",
    "#PRINTS OUT ALL GUESSES\n",
    "#print(loaded_model.predict(X)[0][0])\n",
    "#[print(x) for x in loaded_model.predict(X)]\n",
    "#[print(x) for x in y_pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
