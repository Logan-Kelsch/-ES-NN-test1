{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('ES7-26-22_to_9-18-24.csv')\n",
    "\n",
    "\n",
    "#testing random feature drops\n",
    "data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='bull15')\n",
    "data = data.drop(columns='bear15')\n",
    "\n",
    "data = data.drop(columns='bull30')\n",
    "data = data.drop(columns='bear30')\n",
    "\n",
    "data = data.drop(columns='bull60')\n",
    "data = data.drop(columns='bear60')\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "NinRows = data.drop(data[data['bull15'] == 'in'].index)\n",
    "\n",
    "inRows = data.drop(data[data['bull15'] != 'in'].index)\n",
    "\n",
    "inRowsMatch = inRows.loc[0:17129]\n",
    "\n",
    "print('ins -\\t',inRows.index.size,'\\ninsMatch -\\t',\\\n",
    "      inRowsMatch.index.size,'\\nnon-ins -\\t',NinRows.index.size)\n",
    "\n",
    "optData = pd.concat([NinRows, inRowsMatch],axis=0)\n",
    "\n",
    "percIn = inRowsMatch.size/optData.size\n",
    "percNin = NinRows.size/optData.size\n",
    "weight_for_0 = percIn\n",
    "weight_for_1 = percNin\n",
    "cw = {0: weight_for_1, 1: weight_for_0}\n",
    "\n",
    "print(\"PERCENT & WEIGHTS:\\nINS\\t-\\t\",percIn*100,\" %\\nNon-INS\\t-\\t\",percNin*100,\" %\",sep='')\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(optData.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = optData.iloc[:, :-1].values\n",
    "y = optData.iloc[:, -1].values\n",
    "\n",
    "\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "#BUILD THE NEURAL NETWORK MODEL-------------------------------------------------------\n",
    "\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    0.2,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "opt1 = SGD(learning_rate=0.001)\n",
    "opt2  = tf.keras.optimizers.Adam(clipnorm=0.7)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([#currently 17 total features\n",
    "        tf.keras.layers.Dense(512, input_dim=16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    rmse='root_mean_squared_error'\n",
    "\n",
    "    model.compile(optimizer=opt3, loss='mse', metrics=['accuracy','AUC',rmse])\n",
    "    return model\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 15000\n",
    "\n",
    "model = build_model()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.25,\\\n",
    "                    shuffle=True, verbose=1, validation_data=(X_test, y_test),\\\n",
    "                    class_weight=cw)\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "#_, acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# Plot training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, loss, 'y', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, history.history['AUC'], 'y', label='Training AUC')\n",
    "plt.plot(epochs, history.history['val_AUC'], 'r', label='Validation AUC')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "#making a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n",
    "# Save the model\n",
    "model.save('epoch15k.keras')\n",
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Testing outputs for model (placed here so we don't have to retrain)'''\n",
    "preds = model.predict(X_train)\n",
    "print(preds[0:5])\n",
    "print(preds[10000:10005])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "#weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "#print(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
