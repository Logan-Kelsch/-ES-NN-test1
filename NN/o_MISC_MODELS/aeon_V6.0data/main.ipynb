{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logan Kelsch + JJ\n",
    "'''\n",
    "/*\n",
    " *  taking this version of neural network training takes only a few features\n",
    " *          [ high, low, open, close, volume, TimeOfDay, DayOfWeek ]\n",
    " *  and uses common techniques described in 'features_creation.py' to largely\n",
    " *  expand dimensionality. \n",
    " *  As of writing this (11/24/24) there is limited expansion\n",
    "*/\n",
    "''' \n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from funcs_data_process import *\n",
    "from feature_creation import *\n",
    "from performance_printout import *\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('betaset_tmp.csv')\n",
    "\n",
    "'''#---------------#\n",
    "    MODEL VARIABLES\n",
    "'''#---------------#\n",
    "\n",
    "\n",
    "indp_size       = 0.05       #size of indepentendent set of samples\n",
    "test_size       = 0.15       #size of validation set of samples\n",
    "num_isol_feats  = 0         #number of features to be left out of PCA transformation\n",
    "                            #   this function is depricated/needs updated\n",
    "use_PCA         = False      #bool for use of PCA feature decomp. and transformation\n",
    "comps_PCA       = 128        #number of primary components to use under PCA t.\n",
    "\n",
    "time_steps      = 5        #LSTM time steps used\n",
    "\n",
    "t_start         = 570       #start time for time based sample filtering         570 is 9:30am EST\n",
    "t_end           = 720       #end   time for time based sample filtering         645,720 are 10:45am,12:00pm EST\n",
    "\n",
    "params = model_params()\n",
    "params = get_model_params(\n",
    "    m_type          = 'Classification'      # {'Regression', 'Classification'}\n",
    "\n",
    "   ,target_time     = 5        #how many minutes in the future is the target\n",
    "   ,c_split_val     = 5\n",
    "   ,c_class_cnt     = 2\n",
    ")\n",
    "\n",
    "\n",
    "#drop unused target columns\n",
    "data = set_target(data, params)\n",
    "\n",
    "#collect all sample indices to be kept through time filter\n",
    "#two seperate functions to keep main and ind test isolated\n",
    "keep_ndx = grab_wanted_times(data.values, t_start, t_end, time_steps)\n",
    "\n",
    "data = data.drop(columns=return_name_collection())\n",
    "#data = data.drop(columns=fn_orig_time())\n",
    "\n",
    "#confirmation printout of all features/targets\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "params = update_class_weights(y, params)\n",
    "\n",
    "if(params.model_type=='Classification'):\n",
    "    y, label_encoder = y_preprocess(params, y)\n",
    "\n",
    "X_nrm = normalize_from_tt_split(X, X, test_size)\n",
    "if(use_PCA):\n",
    "    X = reform_with_PCA_isolated(X_nrm, X_nrm, test_size, num_isol_feats, comps_PCA)\n",
    "#X, y = reformat_to_lstm(X, y, time_steps)\n",
    "\n",
    "print('\\nX shape == {}.'.format(X.shape))\n",
    "print('y shape == {}.\\n'.format(y.shape))\n",
    "\n",
    "\n",
    "print(f'Raw Sample Count:\\t{len(X)}')\n",
    "X,     y     = filter_times(X,     y,     keep_ndx)\n",
    "print(f'Remaining Sample Count:\\t{len(X)}')\n",
    "\n",
    "#split all samples 3 ways into training, and testing\n",
    "#   and split all testing into validation and independent\n",
    "X_train, X_val, X_ind, y_train, y_val, y_ind =\\\n",
    "    split_into_train_val_ind(X, y, test_size, indp_size, time_steps)\n",
    "\n",
    "if(params.model_type=='Classification'):\n",
    "    inv_trn_y = label_encoder.inverse_transform(y_train)\n",
    "    params = update_class_weights(inv_trn_y, params)\n",
    "\n",
    "print('\\nX_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))\n",
    "print('X_val shape == {}.'.format(X_val.shape))\n",
    "print('y_val shape == {}.'.format(y_val.shape))\n",
    "print('X_ind shape == {}.'.format(X_ind.shape))\n",
    "print('y_ind shape == {}.\\n'.format(y_ind.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Here is the cool area of code, gonna try as many aeon things as possible to see what we can do\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from aeon.classification.sklearn import ContinuousIntervalTree\n",
    "\n",
    "def new_cit(max_depth):\n",
    "    if(params.model_type=='Regression'):\n",
    "        return ContinuousIntervalTree(max_depth=max_depth)\n",
    "    else:\n",
    "        return ContinuousIntervalTree(max_depth=max_depth)\n",
    "\n",
    "def new_dt(max_depth):\n",
    "    if(params.model_type=='Regression'):\n",
    "        return DecisionTreeRegressor(max_depth=max_depth)\n",
    "    else:\n",
    "        return DecisionTreeClassifier(max_depth=max_depth)\n",
    "#causes overfitting in rotating forest model\n",
    "def new_rf(max_depth, n_estimators_rf):\n",
    "    return RandomForestClassifier(n_estimators=n_estimators_rf\n",
    "                                      ,max_depth=max_depth\n",
    "                                      ,random_state=42)\n",
    "\n",
    "#   CONVOLUTION BASED\n",
    "from aeon.regression.convolution_based import HydraRegressor\n",
    "from aeon.regression.convolution_based import MultiRocketHydraRegressor\n",
    "from aeon.regression.convolution_based import RocketRegressor\n",
    "from aeon.regression.convolution_based import MiniRocketRegressor\n",
    "from aeon.regression.convolution_based import MultiRocketRegressor\n",
    "#   DISTANCE BASED\n",
    "from aeon.regression.distance_based import KNeighborsTimeSeriesRegressor\n",
    "#   INTERVAL BASED\n",
    "from aeon.regression.interval_based import TimeSeriesForestRegressor\n",
    "from aeon.classification.interval_based import TimeSeriesForestClassifier\n",
    "from aeon.classification.sklearn import RotationForestClassifier\n",
    "#   DEEPLEARNING BASED\n",
    "from aeon.classification.deep_learning import InceptionTimeClassifier\n",
    "#   SHAPELET BASED\n",
    "from aeon.classification.shapelet_based import ShapeletTransformClassifier\n",
    "#\tHYBRID\n",
    "from aeon.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model1 = HydraRegressor(n_kernels=8, n_groups=16, random_state=42)#n_jobs=-1, random_state=None)\n",
    "model2 = MultiRocketHydraRegressor(n_kernels= 2, n_groups=2, random_state=42) #NOTE# CANNOT WORK ON 16GB OF MEMORY END#NOTE#\n",
    "model3 = RocketRegressor(n_kernels=2048, estimator=None, n_jobs=-1)\n",
    "model4 = MiniRocketRegressor(n_kernels=512, max_dilations_per_kernel=16, n_jobs=-1)\n",
    "model5 = MultiRocketRegressor(n_kernels=4, max_dilations_per_kernel=2, n_features_per_kernel=32, n_jobs=-1)\n",
    "\n",
    "#jesus lord almight this thing is crazy, cannot balance classes, but like if that\n",
    "#becomes possible for this then this is first up. sticks to heavier class :(\n",
    "model10c = InceptionTimeClassifier()\n",
    "model16 = KNeighborsTimeSeriesRegressor(n_neighbors=16, distance='euclidean', n_jobs=-1)\n",
    "#model26 = TimeSeriesForestRegressor(base_estimator=new_dt(max_depth=9),n_estimators=4, random_state=42, min_interval_length=1, max_interval_length=20, contract_max_n_estimators=1, n_jobs=-1)\n",
    "#model26c = TimeSeriesForestClassifier(base_estimator=new_dt(max_depth=5),n_estimators=4, random_state=42, min_interval_length=1, max_interval_length=20, contract_max_n_estimators=1, n_jobs=-1)\n",
    "\n",
    "#56%accuracy with these stats\n",
    "model33c = RotationForestClassifier(base_estimator=new_dt(max_depth=4),\n",
    "                                    n_estimators=4\n",
    "                                    ,min_group=1\n",
    "                                    ,max_group=20\n",
    "                                    ,remove_proportion= 0.3\n",
    "                                    ,contract_max_n_estimators=500\n",
    "                                    ,random_state=42,n_jobs=-1)\n",
    "\n",
    "model31c = ShapeletTransformClassifier(n_shapelet_samples=5\n",
    "                                       ,max_shapelets=None\n",
    "                                       ,max_shapelet_length=None\n",
    "                                       ,estimator=new_dt(max_depth=2)\n",
    "                                       ,batch_size=500\n",
    "                                       ,random_state=0\n",
    "                                       ,n_jobs=-1)\n",
    "model17c = HIVECOTEV2(n_jobs=-1\n",
    "                      )\n",
    "\n",
    "model = model33c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_score_search(model=model,\n",
    "                   params=params,\n",
    "                   X_train=X_train,\n",
    "                   y_train=y_train,\n",
    "                   X_test=X_val,\n",
    "                   y_test=y_val,\n",
    "                   dynamic_param_name=  'max_depth'\n",
    "                   ,val_range=          range(1, 8)\\\n",
    "                            #np.arange(0.1, 1, 0.1)\\\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = True if params.model_type=='Classification' else False\n",
    "#only relevant to classifcation models\n",
    "t = 0.5\n",
    "#predict training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "#predict validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "#predict independent set\n",
    "y_pred_ind = model.predict(X_ind)\n",
    "#combine testing score\n",
    "score_type, score_train = score_model(y_train, y_pred_train, t, params)\n",
    "score_type, score_val = score_model(y_val, y_pred_val, t, params)\n",
    "score_type, score_ind = score_model(y_ind, y_pred_ind, t, params)\n",
    "score_test = (score_val*3+score_ind)/4\n",
    "print(f'Training {score_type}:\\n\\t{score_train}')\n",
    "print(f'Validation {score_type}: {score_val}')\n",
    "print(f'Independent {score_type}: {score_ind}')\n",
    "print(f'Combined Test {score_type}:\\n\\t{score_test}')\n",
    "\n",
    "graph_predictions(y_pred_train, y_train, params, 'Training')\n",
    "graph_predictions(y_pred_val, y_val, params, 'validation')\n",
    "graph_predictions(y_pred_ind, y_ind, params, 'independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'RotationForestClassifier_model1_pcaNOTIMES.sav'\n",
    "\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.cudnn.version())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".torch_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
