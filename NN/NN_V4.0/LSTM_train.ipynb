{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTED FEATURES: \n",
      "Index(['vel5', 'vel10', 'vel15', 'vel30', 'vel60', 'acc5', 'acc10', 'acc15',\n",
      "       'acc30', 'acc60', 'stoch12', 'stochDiff6012', 'RSIhl_diff',\n",
      "       'RSIhl_diffROC', 'HLdiff', 'HL2', 'H2L', 'HLdiff12', 'HLdiff21', 'vol',\n",
      "       'vol10', 'vol15', 'vol30', 'vol60', 'volD10', 'volD15', 'volD30',\n",
      "       'volD60', 'vpm5', 'vpm10', 'vpm15', 'vpm30', 'vpm60', 'ToD', 'DoW',\n",
      "       'mo'],\n",
      "      dtype='object')\n",
      "TESTING FOR: \n",
      "r5\n",
      "X shape == (224859, 10, 36).\n",
      "y shape == (224859,).\n",
      "Raw Sample Count: 224859\n",
      "65769\n",
      "24600\n",
      "Running on: GPU\n",
      "\n",
      "Epoch 1/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 66ms/step - R2Score: -2.4618 - loss: 0.0204 - root_mean_squared_error: 0.1416 - val_R2Score: -0.0696 - val_loss: 0.0063 - val_root_mean_squared_error: 0.0793 - learning_rate: 0.0075\n",
      "Epoch 2/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: -0.5602 - loss: 0.0093 - root_mean_squared_error: 0.0963 - val_R2Score: -0.0554 - val_loss: 0.0062 - val_root_mean_squared_error: 0.0788 - learning_rate: 0.0075\n",
      "Epoch 3/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.3525 - loss: 0.0081 - root_mean_squared_error: 0.0897 - val_R2Score: -0.0140 - val_loss: 0.0060 - val_root_mean_squared_error: 0.0772 - learning_rate: 0.0075\n",
      "Epoch 4/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: -0.2674 - loss: 0.0075 - root_mean_squared_error: 0.0869 - val_R2Score: 0.0074 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0764 - learning_rate: 0.0075\n",
      "Epoch 5/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: -0.1981 - loss: 0.0071 - root_mean_squared_error: 0.0845 - val_R2Score: -0.0056 - val_loss: 0.0059 - val_root_mean_squared_error: 0.0769 - learning_rate: 0.0075\n",
      "Epoch 6/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.1698 - loss: 0.0070 - root_mean_squared_error: 0.0835 - val_R2Score: 0.0027 - val_loss: 0.0059 - val_root_mean_squared_error: 0.0766 - learning_rate: 0.0075\n",
      "Epoch 7/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.1489 - loss: 0.0068 - root_mean_squared_error: 0.0827 - val_R2Score: 0.0110 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0763 - learning_rate: 0.0075\n",
      "Epoch 8/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: -0.1057 - loss: 0.0066 - root_mean_squared_error: 0.0811 - val_R2Score: 0.0163 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0761 - learning_rate: 0.0075\n",
      "Epoch 9/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: -0.0978 - loss: 0.0065 - root_mean_squared_error: 0.0809 - val_R2Score: 0.0158 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0761 - learning_rate: 0.0075\n",
      "Epoch 10/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0973 - loss: 0.0065 - root_mean_squared_error: 0.0808 - val_R2Score: 0.0118 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0762 - learning_rate: 0.0075\n",
      "Epoch 11/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0838 - loss: 0.0065 - root_mean_squared_error: 0.0803 - val_R2Score: 0.0203 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0759 - learning_rate: 0.0075\n",
      "Epoch 12/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0605 - loss: 0.0063 - root_mean_squared_error: 0.0795 - val_R2Score: 0.0212 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0759 - learning_rate: 0.0075\n",
      "Epoch 13/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: -0.0651 - loss: 0.0063 - root_mean_squared_error: 0.0796 - val_R2Score: 0.0192 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0759 - learning_rate: 0.0075\n",
      "Epoch 14/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0480 - loss: 0.0062 - root_mean_squared_error: 0.0790 - val_R2Score: 0.0232 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0758 - learning_rate: 0.0075\n",
      "Epoch 15/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0260 - loss: 0.0061 - root_mean_squared_error: 0.0782 - val_R2Score: 0.0196 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0759 - learning_rate: 0.0072\n",
      "Epoch 16/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: -0.0368 - loss: 0.0062 - root_mean_squared_error: 0.0786 - val_R2Score: 0.0264 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0757 - learning_rate: 0.0072\n",
      "Epoch 17/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: -0.0296 - loss: 0.0061 - root_mean_squared_error: 0.0783 - val_R2Score: 0.0291 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0756 - learning_rate: 0.0072\n",
      "Epoch 18/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - R2Score: -0.0226 - loss: 0.0061 - root_mean_squared_error: 0.0780 - val_R2Score: 0.0280 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0756 - learning_rate: 0.0072\n",
      "Epoch 19/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: -0.0185 - loss: 0.0061 - root_mean_squared_error: 0.0779 - val_R2Score: 0.0293 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0756 - learning_rate: 0.0072\n",
      "Epoch 20/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0221 - loss: 0.0061 - root_mean_squared_error: 0.0780 - val_R2Score: 0.0278 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0756 - learning_rate: 0.0072\n",
      "Epoch 21/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: -0.0083 - loss: 0.0060 - root_mean_squared_error: 0.0775 - val_R2Score: 0.0292 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0756 - learning_rate: 0.0072\n",
      "Epoch 22/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0182 - loss: 0.0061 - root_mean_squared_error: 0.0779 - val_R2Score: 0.0266 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0757 - learning_rate: 0.0072\n",
      "Epoch 23/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: -0.0088 - loss: 0.0060 - root_mean_squared_error: 0.0775 - val_R2Score: 0.0300 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0755 - learning_rate: 0.0072\n",
      "Epoch 24/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: -0.0024 - loss: 0.0060 - root_mean_squared_error: 0.0773 - val_R2Score: 0.0308 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0755 - learning_rate: 0.0072\n",
      "Epoch 25/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.0135 - loss: 0.0059 - root_mean_squared_error: 0.0766 - val_R2Score: 0.0320 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0755 - learning_rate: 0.0072\n",
      "Epoch 26/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.0034 - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_R2Score: 0.0306 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0755 - learning_rate: 0.0072\n",
      "Epoch 27/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0067 - loss: 0.0059 - root_mean_squared_error: 0.0769 - val_R2Score: 0.0314 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0755 - learning_rate: 0.0069\n",
      "Epoch 28/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0128 - loss: 0.0059 - root_mean_squared_error: 0.0767 - val_R2Score: 0.0326 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0754 - learning_rate: 0.0069\n",
      "Epoch 29/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.0041 - loss: 0.0059 - root_mean_squared_error: 0.0770 - val_R2Score: 0.0325 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0754 - learning_rate: 0.0069\n",
      "Epoch 30/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.0123 - loss: 0.0059 - root_mean_squared_error: 0.0767 - val_R2Score: 0.0322 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0754 - learning_rate: 0.0069\n",
      "Epoch 31/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0169 - loss: 0.0059 - root_mean_squared_error: 0.0765 - val_R2Score: 0.0352 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0069\n",
      "Epoch 32/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0076 - loss: 0.0059 - root_mean_squared_error: 0.0769 - val_R2Score: 0.0352 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0069\n",
      "Epoch 33/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0158 - loss: 0.0059 - root_mean_squared_error: 0.0765 - val_R2Score: 0.0339 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0754 - learning_rate: 0.0069\n",
      "Epoch 34/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.0105 - loss: 0.0059 - root_mean_squared_error: 0.0768 - val_R2Score: 0.0351 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0069\n",
      "Epoch 35/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0212 - loss: 0.0058 - root_mean_squared_error: 0.0763 - val_R2Score: 0.0364 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0069\n",
      "Epoch 36/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0229 - loss: 0.0058 - root_mean_squared_error: 0.0763 - val_R2Score: 0.0338 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0754 - learning_rate: 0.0069\n",
      "Epoch 37/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0153 - loss: 0.0059 - root_mean_squared_error: 0.0766 - val_R2Score: 0.0363 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0066\n",
      "Epoch 38/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.0234 - loss: 0.0058 - root_mean_squared_error: 0.0763 - val_R2Score: 0.0372 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0066\n",
      "Epoch 39/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0283 - loss: 0.0058 - root_mean_squared_error: 0.0761 - val_R2Score: 0.0372 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0066\n",
      "Epoch 40/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0301 - loss: 0.0058 - root_mean_squared_error: 0.0760 - val_R2Score: 0.0362 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0066\n",
      "Epoch 41/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0321 - loss: 0.0058 - root_mean_squared_error: 0.0759 - val_R2Score: 0.0363 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0066\n",
      "Epoch 42/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.0289 - loss: 0.0058 - root_mean_squared_error: 0.0760 - val_R2Score: 0.0368 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0753 - learning_rate: 0.0066\n",
      "Epoch 43/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0366 - loss: 0.0057 - root_mean_squared_error: 0.0757 - val_R2Score: 0.0388 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0066\n",
      "Epoch 44/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0333 - loss: 0.0058 - root_mean_squared_error: 0.0759 - val_R2Score: 0.0395 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0066\n",
      "Epoch 45/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0360 - loss: 0.0057 - root_mean_squared_error: 0.0758 - val_R2Score: 0.0394 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0066\n",
      "Epoch 46/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.0285 - loss: 0.0058 - root_mean_squared_error: 0.0761 - val_R2Score: 0.0375 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0066\n",
      "Epoch 47/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0437 - loss: 0.0057 - root_mean_squared_error: 0.0755 - val_R2Score: 0.0383 - val_loss: 0.0057 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0064\n",
      "Epoch 48/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0416 - loss: 0.0057 - root_mean_squared_error: 0.0755 - val_R2Score: 0.0405 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 49/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0353 - loss: 0.0057 - root_mean_squared_error: 0.0758 - val_R2Score: 0.0398 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 50/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 66ms/step - R2Score: 0.0440 - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_R2Score: 0.0400 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 51/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0439 - loss: 0.0057 - root_mean_squared_error: 0.0755 - val_R2Score: 0.0413 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 52/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.0479 - loss: 0.0057 - root_mean_squared_error: 0.0753 - val_R2Score: 0.0399 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 53/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0457 - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_R2Score: 0.0399 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 54/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.0348 - loss: 0.0057 - root_mean_squared_error: 0.0758 - val_R2Score: 0.0393 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0752 - learning_rate: 0.0064\n",
      "Epoch 55/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0455 - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_R2Score: 0.0413 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0064\n",
      "Epoch 56/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0541 - loss: 0.0056 - root_mean_squared_error: 0.0751 - val_R2Score: 0.0427 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0064\n",
      "Epoch 57/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0522 - loss: 0.0056 - root_mean_squared_error: 0.0751 - val_R2Score: 0.0417 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0061\n",
      "Epoch 58/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.0500 - loss: 0.0057 - root_mean_squared_error: 0.0752 - val_R2Score: 0.0433 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 59/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0510 - loss: 0.0057 - root_mean_squared_error: 0.0752 - val_R2Score: 0.0416 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0751 - learning_rate: 0.0061\n",
      "Epoch 60/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0506 - loss: 0.0057 - root_mean_squared_error: 0.0752 - val_R2Score: 0.0424 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 61/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0468 - loss: 0.0057 - root_mean_squared_error: 0.0753 - val_R2Score: 0.0438 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 62/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0460 - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_R2Score: 0.0434 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 63/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: 0.0568 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0434 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 64/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0455 - loss: 0.0057 - root_mean_squared_error: 0.0754 - val_R2Score: 0.0427 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 65/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0584 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0436 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 66/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.0588 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0442 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 67/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.0599 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0432 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 68/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0576 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0453 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0061\n",
      "Epoch 69/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0588 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0449 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0061\n",
      "Epoch 70/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0573 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0439 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0061\n",
      "Epoch 71/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: 0.0624 - loss: 0.0056 - root_mean_squared_error: 0.0747 - val_R2Score: 0.0454 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0061\n",
      "Epoch 72/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - R2Score: 0.0578 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0441 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0750 - learning_rate: 0.0059\n",
      "Epoch 73/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0620 - loss: 0.0056 - root_mean_squared_error: 0.0747 - val_R2Score: 0.0457 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0059\n",
      "Epoch 74/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0604 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0478 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0059\n",
      "Epoch 75/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: 0.0669 - loss: 0.0056 - root_mean_squared_error: 0.0745 - val_R2Score: 0.0471 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0059\n",
      "Epoch 76/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0597 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0477 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0059\n",
      "Epoch 77/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0604 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0477 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0059\n",
      "Epoch 78/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0645 - loss: 0.0056 - root_mean_squared_error: 0.0746 - val_R2Score: 0.0465 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0059\n",
      "Epoch 79/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0568 - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_R2Score: 0.0477 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0059\n",
      "Epoch 80/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - R2Score: 0.0728 - loss: 0.0055 - root_mean_squared_error: 0.0743 - val_R2Score: 0.0482 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0059\n",
      "Epoch 81/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - R2Score: 0.0602 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0458 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0749 - learning_rate: 0.0059\n",
      "Epoch 82/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0607 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0483 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 83/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0615 - loss: 0.0056 - root_mean_squared_error: 0.0748 - val_R2Score: 0.0496 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 84/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.0690 - loss: 0.0055 - root_mean_squared_error: 0.0745 - val_R2Score: 0.0478 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 85/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0655 - loss: 0.0056 - root_mean_squared_error: 0.0746 - val_R2Score: 0.0479 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 86/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0670 - loss: 0.0056 - root_mean_squared_error: 0.0745 - val_R2Score: 0.0497 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 87/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0734 - loss: 0.0055 - root_mean_squared_error: 0.0743 - val_R2Score: 0.0498 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 88/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - R2Score: 0.0751 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0512 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0056\n",
      "Epoch 89/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0691 - loss: 0.0055 - root_mean_squared_error: 0.0745 - val_R2Score: 0.0491 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 90/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0726 - loss: 0.0055 - root_mean_squared_error: 0.0743 - val_R2Score: 0.0501 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0056\n",
      "Epoch 91/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0682 - loss: 0.0056 - root_mean_squared_error: 0.0745 - val_R2Score: 0.0497 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0748 - learning_rate: 0.0056\n",
      "Epoch 92/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0680 - loss: 0.0056 - root_mean_squared_error: 0.0745 - val_R2Score: 0.0503 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 93/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0774 - loss: 0.0055 - root_mean_squared_error: 0.0741 - val_R2Score: 0.0501 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 94/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0699 - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_R2Score: 0.0504 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 95/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0753 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0515 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 96/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0757 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0515 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 97/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0691 - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_R2Score: 0.0509 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 98/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0707 - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_R2Score: 0.0508 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0054\n",
      "Epoch 99/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0707 - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_R2Score: 0.0533 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0054\n",
      "Epoch 100/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0653 - loss: 0.0056 - root_mean_squared_error: 0.0746 - val_R2Score: 0.0530 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0054\n",
      "Epoch 101/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0770 - loss: 0.0055 - root_mean_squared_error: 0.0741 - val_R2Score: 0.0537 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0054\n",
      "Epoch 102/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0708 - loss: 0.0055 - root_mean_squared_error: 0.0744 - val_R2Score: 0.0517 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0052\n",
      "Epoch 103/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0760 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0519 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0052\n",
      "Epoch 104/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0813 - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_R2Score: 0.0544 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0052\n",
      "Epoch 105/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0753 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0520 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0747 - learning_rate: 0.0052\n",
      "Epoch 106/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0764 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0540 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0052\n",
      "Epoch 107/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0808 - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_R2Score: 0.0533 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0052\n",
      "Epoch 108/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0811 - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_R2Score: 0.0539 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0052\n",
      "Epoch 109/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0769 - loss: 0.0055 - root_mean_squared_error: 0.0741 - val_R2Score: 0.0534 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0052\n",
      "Epoch 110/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0793 - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_R2Score: 0.0543 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0052\n",
      "Epoch 111/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0798 - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_R2Score: 0.0554 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0052\n",
      "Epoch 112/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0845 - loss: 0.0055 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0558 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 113/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0745 - loss: 0.0055 - root_mean_squared_error: 0.0742 - val_R2Score: 0.0575 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0050\n",
      "Epoch 114/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0829 - loss: 0.0055 - root_mean_squared_error: 0.0739 - val_R2Score: 0.0563 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 115/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0822 - loss: 0.0055 - root_mean_squared_error: 0.0739 - val_R2Score: 0.0560 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 116/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0854 - loss: 0.0054 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0533 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0050\n",
      "Epoch 117/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0791 - loss: 0.0055 - root_mean_squared_error: 0.0741 - val_R2Score: 0.0566 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 118/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0894 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0566 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 119/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0855 - loss: 0.0054 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0558 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 120/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0884 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0545 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0746 - learning_rate: 0.0050\n",
      "Epoch 121/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0794 - loss: 0.0055 - root_mean_squared_error: 0.0740 - val_R2Score: 0.0568 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0050\n",
      "Epoch 122/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0848 - loss: 0.0055 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0569 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0048\n",
      "Epoch 123/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0829 - loss: 0.0055 - root_mean_squared_error: 0.0739 - val_R2Score: 0.0582 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0048\n",
      "Epoch 124/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.0888 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0561 - val_loss: 0.0056 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0048\n",
      "Epoch 125/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0909 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0575 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0048\n",
      "Epoch 126/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0819 - loss: 0.0055 - root_mean_squared_error: 0.0739 - val_R2Score: 0.0595 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0048\n",
      "Epoch 127/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0890 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0568 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0745 - learning_rate: 0.0048\n",
      "Epoch 128/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0881 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0585 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0048\n",
      "Epoch 129/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0881 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0583 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0048\n",
      "Epoch 130/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0912 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0585 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0048\n",
      "Epoch 131/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0968 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0589 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0048\n",
      "Epoch 132/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0869 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0603 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 133/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.0892 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0595 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0046\n",
      "Epoch 134/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0863 - loss: 0.0054 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0603 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 135/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0847 - loss: 0.0055 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0598 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0046\n",
      "Epoch 136/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0952 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0605 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 137/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0846 - loss: 0.0055 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0597 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0046\n",
      "Epoch 138/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0890 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0600 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0046\n",
      "Epoch 139/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0905 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0585 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0744 - learning_rate: 0.0046\n",
      "Epoch 140/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0911 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0617 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 141/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0929 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0611 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 142/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.0917 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0621 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 143/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0908 - loss: 0.0054 - root_mean_squared_error: 0.0736 - val_R2Score: 0.0619 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 144/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0936 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0624 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 145/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0934 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0603 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 146/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.0920 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0640 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0046\n",
      "Epoch 147/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.0916 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0619 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0743 - learning_rate: 0.0046\n",
      "Epoch 148/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0880 - loss: 0.0054 - root_mean_squared_error: 0.0737 - val_R2Score: 0.0626 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0046\n",
      "Epoch 149/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - R2Score: 0.0947 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0627 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0046\n",
      "Epoch 150/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - R2Score: 0.0935 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0645 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0046\n",
      "Epoch 151/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0841 - loss: 0.0055 - root_mean_squared_error: 0.0738 - val_R2Score: 0.0639 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0044\n",
      "Epoch 152/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0926 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0636 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0044\n",
      "Epoch 153/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0922 - loss: 0.0054 - root_mean_squared_error: 0.0735 - val_R2Score: 0.0644 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742 - learning_rate: 0.0044\n",
      "Epoch 154/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.0994 - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_R2Score: 0.0664 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 155/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0985 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0667 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 156/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0974 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0663 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 157/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.0969 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0662 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 158/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1015 - loss: 0.0054 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0662 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 159/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - R2Score: 0.0979 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0671 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 160/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.0971 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0675 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0044\n",
      "Epoch 161/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1029 - loss: 0.0053 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0686 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0042\n",
      "Epoch 162/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.0962 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0669 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0042\n",
      "Epoch 163/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1011 - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_R2Score: 0.0660 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0042\n",
      "Epoch 164/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1035 - loss: 0.0053 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0676 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0042\n",
      "Epoch 165/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1009 - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_R2Score: 0.0681 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0042\n",
      "Epoch 166/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.0983 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0672 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0042\n",
      "Epoch 167/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1013 - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_R2Score: 0.0681 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0042\n",
      "Epoch 168/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0986 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0671 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0042\n",
      "Epoch 169/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1023 - loss: 0.0053 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0682 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0042\n",
      "Epoch 170/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1039 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0675 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0741 - learning_rate: 0.0042\n",
      "Epoch 171/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 68ms/step - R2Score: 0.0959 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0689 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 172/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1033 - loss: 0.0053 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0683 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 173/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.0947 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0693 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 174/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1044 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0700 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 175/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1005 - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_R2Score: 0.0690 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 176/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0997 - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_R2Score: 0.0703 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0041\n",
      "Epoch 177/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1060 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0695 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 178/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - R2Score: 0.0983 - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_R2Score: 0.0701 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 179/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: 0.1085 - loss: 0.0053 - root_mean_squared_error: 0.0729 - val_R2Score: 0.0698 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0740 - learning_rate: 0.0041\n",
      "Epoch 180/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.0961 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0710 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0041\n",
      "Epoch 181/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1029 - loss: 0.0053 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0714 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 182/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1030 - loss: 0.0053 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0720 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 183/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1040 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0712 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 184/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1016 - loss: 0.0054 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0718 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 185/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.0963 - loss: 0.0054 - root_mean_squared_error: 0.0734 - val_R2Score: 0.0728 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0039\n",
      "Epoch 186/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1052 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0723 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 187/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1070 - loss: 0.0053 - root_mean_squared_error: 0.0729 - val_R2Score: 0.0715 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 188/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1088 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0742 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0039\n",
      "Epoch 189/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1096 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0720 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0739 - learning_rate: 0.0039\n",
      "Epoch 190/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - R2Score: 0.1047 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0729 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0039\n",
      "Epoch 191/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1045 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0741 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 192/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1015 - loss: 0.0054 - root_mean_squared_error: 0.0731 - val_R2Score: 0.0743 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 193/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1171 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0744 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 194/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1109 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0747 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 195/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1067 - loss: 0.0053 - root_mean_squared_error: 0.0729 - val_R2Score: 0.0740 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 196/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - R2Score: 0.1103 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0742 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 197/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - R2Score: 0.1109 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0756 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0037\n",
      "Epoch 198/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1136 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0735 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 199/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1113 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0749 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0037\n",
      "Epoch 200/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1085 - loss: 0.0053 - root_mean_squared_error: 0.0729 - val_R2Score: 0.0757 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0037\n",
      "Epoch 201/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1050 - loss: 0.0053 - root_mean_squared_error: 0.0730 - val_R2Score: 0.0749 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0738 - learning_rate: 0.0036\n",
      "Epoch 202/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1160 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0767 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 203/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1124 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0766 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 204/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1110 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0768 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 205/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1143 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0765 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 206/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1202 - loss: 0.0052 - root_mean_squared_error: 0.0724 - val_R2Score: 0.0762 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 207/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1144 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0769 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 208/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1123 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0768 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 209/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - R2Score: 0.1084 - loss: 0.0053 - root_mean_squared_error: 0.0729 - val_R2Score: 0.0762 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0036\n",
      "Epoch 210/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.1222 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0782 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0036\n",
      "Epoch 211/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1116 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0777 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 212/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1112 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0787 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 213/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1130 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0797 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 214/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1093 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0775 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737 - learning_rate: 0.0035\n",
      "Epoch 215/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1149 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0791 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 216/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1140 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0813 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0035\n",
      "Epoch 217/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - R2Score: 0.1111 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0790 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 218/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1094 - loss: 0.0053 - root_mean_squared_error: 0.0728 - val_R2Score: 0.0791 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 219/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1195 - loss: 0.0052 - root_mean_squared_error: 0.0724 - val_R2Score: 0.0806 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0035\n",
      "Epoch 220/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1145 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0794 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 221/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.1071 - loss: 0.0053 - root_mean_squared_error: 0.0729 - val_R2Score: 0.0804 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0035\n",
      "Epoch 222/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1136 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0815 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0035\n",
      "Epoch 223/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1173 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0793 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0736 - learning_rate: 0.0035\n",
      "Epoch 224/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 55ms/step - R2Score: 0.1213 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0805 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 225/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - R2Score: 0.1135 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0825 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 226/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1143 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0825 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 227/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1212 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0822 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 228/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - R2Score: 0.1157 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0817 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 229/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1143 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0819 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 230/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1146 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0815 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 231/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1170 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0820 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 232/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1189 - loss: 0.0052 - root_mean_squared_error: 0.0724 - val_R2Score: 0.0805 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0735 - learning_rate: 0.0033\n",
      "Epoch 233/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step - R2Score: 0.1207 - loss: 0.0052 - root_mean_squared_error: 0.0724 - val_R2Score: 0.0827 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0033\n",
      "Epoch 234/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.1168 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0832 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 235/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1149 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0828 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 236/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1135 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0840 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 237/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - R2Score: 0.1202 - loss: 0.0052 - root_mean_squared_error: 0.0724 - val_R2Score: 0.0839 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 238/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1130 - loss: 0.0053 - root_mean_squared_error: 0.0727 - val_R2Score: 0.0833 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 239/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1166 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0844 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 240/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - R2Score: 0.1221 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0843 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 241/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1172 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0848 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 242/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1249 - loss: 0.0052 - root_mean_squared_error: 0.0722 - val_R2Score: 0.0840 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0032\n",
      "Epoch 243/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1219 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0859 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0032\n",
      "Epoch 244/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1243 - loss: 0.0052 - root_mean_squared_error: 0.0722 - val_R2Score: 0.0837 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0031\n",
      "Epoch 245/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1241 - loss: 0.0052 - root_mean_squared_error: 0.0722 - val_R2Score: 0.0855 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0031\n",
      "Epoch 246/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.1205 - loss: 0.0052 - root_mean_squared_error: 0.0724 - val_R2Score: 0.0840 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0031\n",
      "Epoch 247/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1237 - loss: 0.0052 - root_mean_squared_error: 0.0722 - val_R2Score: 0.0845 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0031\n",
      "Epoch 248/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - R2Score: 0.1293 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0859 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0031\n",
      "Epoch 249/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1180 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0859 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0031\n",
      "Epoch 250/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.1149 - loss: 0.0053 - root_mean_squared_error: 0.0726 - val_R2Score: 0.0847 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0031\n",
      "Epoch 251/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1219 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0860 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0031\n",
      "Epoch 252/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1176 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0863 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0031\n",
      "Epoch 253/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1211 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0869 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0031\n",
      "Epoch 254/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1181 - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_R2Score: 0.0849 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0734 - learning_rate: 0.0029\n",
      "Epoch 255/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1253 - loss: 0.0052 - root_mean_squared_error: 0.0722 - val_R2Score: 0.0857 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0029\n",
      "Epoch 256/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1259 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0870 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0029\n",
      "Epoch 257/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1216 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0878 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0029\n",
      "Epoch 258/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.1227 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0871 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0029\n",
      "Epoch 259/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1236 - loss: 0.0052 - root_mean_squared_error: 0.0722 - val_R2Score: 0.0874 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0733 - learning_rate: 0.0029\n",
      "Epoch 260/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1303 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0898 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0029\n",
      "Epoch 261/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1212 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0895 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0029\n",
      "Epoch 262/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1270 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0894 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0029\n",
      "Epoch 263/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.1286 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0888 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0029\n",
      "Epoch 264/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1285 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0885 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 265/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - R2Score: 0.1298 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0880 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 266/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1265 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0880 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 267/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.1231 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0891 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 268/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1269 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0883 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 269/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1291 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0895 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 270/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1262 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0897 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 271/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1227 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0905 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0028\n",
      "Epoch 272/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1314 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0892 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0028\n",
      "Epoch 273/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - R2Score: 0.1323 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0905 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0028\n",
      "Epoch 274/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1215 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0888 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0027\n",
      "Epoch 275/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1299 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0898 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0027\n",
      "Epoch 276/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1315 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0916 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 277/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1292 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0900 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0732 - learning_rate: 0.0027\n",
      "Epoch 278/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1321 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0902 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 279/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: 0.1265 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0922 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 280/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1326 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0911 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 281/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1275 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0915 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 282/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1231 - loss: 0.0052 - root_mean_squared_error: 0.0723 - val_R2Score: 0.0919 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 283/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1310 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0920 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0027\n",
      "Epoch 284/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1332 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0915 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0026\n",
      "Epoch 285/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1339 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0935 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 286/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1301 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0905 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0731 - learning_rate: 0.0026\n",
      "Epoch 287/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1323 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0929 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 288/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - R2Score: 0.1325 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0934 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 289/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1359 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0939 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 290/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1309 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0950 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 291/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1308 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0939 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 292/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - R2Score: 0.1305 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0939 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 293/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1358 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0941 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0026\n",
      "Epoch 294/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1301 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0943 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 295/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1329 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0946 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 296/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - R2Score: 0.1350 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0931 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 297/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1355 - loss: 0.0052 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0931 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 298/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1274 - loss: 0.0052 - root_mean_squared_error: 0.0721 - val_R2Score: 0.0945 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 299/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - R2Score: 0.1310 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0946 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 300/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - R2Score: 0.1329 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0933 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 301/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - R2Score: 0.1300 - loss: 0.0052 - root_mean_squared_error: 0.0720 - val_R2Score: 0.0954 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0025\n",
      "Epoch 302/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 62ms/step - R2Score: 0.1360 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0949 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0025\n",
      "Epoch 303/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 87ms/step - R2Score: 0.1342 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0964 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0025\n",
      "Epoch 304/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - R2Score: 0.1355 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0953 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 305/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - R2Score: 0.1311 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0958 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 306/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - R2Score: 0.1427 - loss: 0.0051 - root_mean_squared_error: 0.0714 - val_R2Score: 0.0952 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 307/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - R2Score: 0.1355 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0938 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0730 - learning_rate: 0.0024\n",
      "Epoch 308/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - R2Score: 0.1364 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0955 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 309/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1362 - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_R2Score: 0.0952 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 310/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1351 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0972 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 311/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1395 - loss: 0.0051 - root_mean_squared_error: 0.0716 - val_R2Score: 0.0961 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 312/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - R2Score: 0.1346 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0964 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 313/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - R2Score: 0.1313 - loss: 0.0052 - root_mean_squared_error: 0.0719 - val_R2Score: 0.0968 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 314/2000\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - R2Score: 0.1347 - loss: 0.0052 - root_mean_squared_error: 0.0718 - val_R2Score: 0.0965 - val_loss: 0.0053 - val_root_mean_squared_error: 0.0729 - learning_rate: 0.0024\n",
      "Epoch 315/2000\n",
      "\u001b[1m120/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - R2Score: 0.1412 - loss: 0.0051 - root_mean_squared_error: 0.0713"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('catted_1-8.csv')\n",
    "\n",
    "#data = data[:1000]\n",
    "\n",
    "#      'Dr1' 'Dr3' 'Mr1' 'Mr3' \n",
    "testFor = 'r5'\n",
    "timeSteps = 10\n",
    "tType = testFor[0]\n",
    "#testing random feature drops\n",
    "#TREND\n",
    "\n",
    "data = data.drop(columns=['FT','FT.1'])\n",
    "#data = data.drop(columns=['FT'])\n",
    "#--------------------------------------\n",
    "#targets-------------------------------\n",
    "#r----  1   2   3   5   10  15  30  60\n",
    "match testFor:\n",
    "    case 'r1':\n",
    "        data = data.drop(columns=['r2','r3','r5','r10','r15','r30','r60'])\n",
    "    case 'r2':\n",
    "        data = data.drop(columns=['r1','r3','r5','r10','r15','r30','r60'])\n",
    "    case 'r3':\n",
    "        data = data.drop(columns=['r1','r2','r5','r10','r15','r30','r60'])\n",
    "    case 'r5':\n",
    "        data = data.drop(columns=['r1','r2','r3','r10','r15','r30','r60'])\n",
    "    case 'r10':\n",
    "        data = data.drop(columns=['r1','r2','r3','r5','r15','r30','r60'])\n",
    "    case 'r15':\n",
    "        data = data.drop(columns=['r1','r2','r3','r5','r10','r30','r60'])\n",
    "    case 'r30':\n",
    "        data = data.drop(columns=['r1','r2','r3','r5','r10','r15','r60'])\n",
    "    case 'r60':\n",
    "        data = data.drop(columns=['r1','r2','r3','r5','r10','r15','r30'])\n",
    "#data['Dr3_Model'] = 0\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "#setting data for LSTM\n",
    "def reformat_to_lstm(X, time_steps=timeSteps):\n",
    "    X_lstm, y_lstm = [], []\n",
    "    \n",
    "    for i in range(time_steps, len(X)):\n",
    "        # Collect previous time_steps rows for X\n",
    "        X_lstm.append(X[i-time_steps:i])  \n",
    "        # The corresponding y value for the last time step in the sequence\n",
    "    \n",
    "    X_lstm = np.array(X_lstm)\n",
    "    \n",
    "    return X_lstm\n",
    "# Standardize the features\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = RobustScaler()\n",
    "scaler3 = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler1.fit(X)\n",
    "X = scaler1.transform(X)\n",
    "\n",
    "X = reformat_to_lstm(X, timeSteps)\n",
    "y = y[timeSteps:]\n",
    "y = np.array(y)\n",
    "\n",
    "#print(X[0])\n",
    "#print(y[0])\n",
    "\n",
    "print('X shape == {}.'.format(X.shape))\n",
    "print('y shape == {}.'.format(y.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_zero_mo_samples(X, y):\n",
    "    # Get the 'MO' column (index 34 for 0-based indexing) for all time steps and samples\n",
    "    non_zero_indices = (X[:, timeSteps-1, len(Xfeatures)-1] >= 0)\n",
    "    # Filter X and y using these indices\n",
    "    X_filtered = X[non_zero_indices]\n",
    "    y_filtered = y[non_zero_indices]\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "def remove_extra_filter(X, y):\n",
    "    indices = (X[:, timeSteps-1, len(Xfeatures)-3] >= -0.321405)#-3 is ToD, this value is 9:30am\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    indices = (X[:, timeSteps-1, len(Xfeatures)-3] <= 0.0366699)#-3 is ToD, this value is 12:00pm\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    return X, y\n",
    "print(f'Raw Sample Count: {len(X)}')\n",
    "X, y = remove_zero_mo_samples(X, y)\n",
    "print(len(X))\n",
    "X, y = remove_extra_filter(X, y)\n",
    "print(len(X))\n",
    "\n",
    "#mos = X[:, timeSteps-1, len(Xfeatures)-1].mean()\n",
    "#print(mos)\n",
    "#print(len(X))\n",
    "#print(X[0])\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#LEARNING RATES____________________________________________________________________________________________\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    #good rough val to start, .25, good val to end at .0015.\n",
    "    #5k epoch should be: .25, 8565, .9995, true\n",
    "    0.01,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.997,\n",
    "    staircase=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.96, \n",
    "    patience=10, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "#LOSS FUNCTION\n",
    "from keras.saving import get_custom_objects\n",
    "from keras.saving import register_keras_serializable\n",
    "get_custom_objects().clear()\n",
    "#CUSTOM LOSS 1_______________________________________________________________________________________________\n",
    "from keras.src import ops\n",
    "from keras.src.losses.loss import squeeze_or_expand_to_same_rank\n",
    "@register_keras_serializable(name=\"skew_loss\")\n",
    "def skew_loss(y_true,y_pred,sFact=4):\n",
    "    #return ops.mean(ops.square((y_pred-y_true)*(1+sFact*tf.cast(((y_true>0 & y_pred<y_true) | (y_true<0 & y_pred>y_true)),tf.float32))),axis=-1)\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n",
    "    #y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n",
    "    error = ops.subtract(y_pred, y_true)\n",
    "    a = ops.convert_to_tensor(ops.cast(y_pred >= 0.05,tf.float32), dtype=tf.float32)\n",
    "    b = ops.convert_to_tensor(ops.cast(y_pred <= -0.05,tf.float32), dtype=tf.float32)\n",
    "    c = ops.convert_to_tensor(ops.cast(y_true >= y_pred,tf.float32), dtype=tf.float32)\n",
    "    d = ops.convert_to_tensor(ops.cast(y_true <= y_pred,tf.float32), dtype=tf.float32)\n",
    "    e = ops.convert_to_tensor(ops.cast(y_true >= 0.05,tf.float32), dtype=tf.float32)\n",
    "    f = ops.convert_to_tensor(ops.cast(y_true <= -0.05,tf.float32), dtype=tf.float32)\n",
    "    h = ops.convert_to_tensor(0.2, dtype=error.dtype)\n",
    "    return ops.mean(\n",
    "        ops.where(\n",
    "            a*c*e+b*d*f==1,# or (b and d),\n",
    "            h*ops.square(error),\n",
    "            ops.square(error)\n",
    "        ))\n",
    "\n",
    "opt1 = SGD(learning_rate=0.01)\n",
    "opt2  = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "opt4 = SGD(learning_rate=0.0075, momentum=0.9)\n",
    "\n",
    "#BUILD AND LOAD MODEL__________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#print(X_train.shape[0]/time_steps)\n",
    "#X_train = np.reshape(X_train,((X_train.shape[0]//time_steps), time_steps, 35))  # Reshape to (batch_size, 5 time steps, 35 features)\n",
    "#y_train = y.reshape(1,-1)\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_LSTM_model():\n",
    "    #time_steps=5\n",
    "    n_features=len(Xfeatures)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],X_train.shape[2])),\n",
    "        tf.keras.layers.LSTM(512, activation='tanh', recurrent_dropout=0.2, return_sequences=True),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LSTM(256, activation='tanh', recurrent_dropout=0.2),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='linear'),#,kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='linear'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        #tf.keras.layers.Dense(64, activation='linear'),\n",
    "        #tf.keras.layers.Dense(256, activation='linear',kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        #tf.keras.layers.Dropout(0.4),\n",
    "        #tf.keras.layers.Dense(128, activation='linear'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=opt4,\n",
    "                  loss='mse'\n",
    "                  ,metrics=['R2Score','root_mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "def load_model():\n",
    "    #loaded_model = tf.keras.models.load_model('tupleTrain.keras', custom_objects={'custom_loss':custom_loss})\n",
    "    loaded_model = tf.keras.models.load_model('r30_10s_LSTM_5.keras')\n",
    "    loaded_model.compile(optimizer=opt4,\n",
    "                         loss='mse'\n",
    "                         , metrics=['R2Score','root_mean_squared_error'])\n",
    "    return loaded_model\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=45, mode='min', restore_best_weights=True)\n",
    "\n",
    "cmp = 'C'\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    cmp = 'G'\n",
    "with tf.device('/'+cmp+'PU:0'):\n",
    "    print('Running on: '+cmp+'PU\\n')\n",
    "    model = build_LSTM_model()\n",
    "    loaded_model = load_model()\n",
    "    used_model = model\n",
    "    history = used_model.fit(X_train, y_train, epochs=epochs,\\\n",
    "                        shuffle=False, verbose=1, validation_data=(X_test, y_test),\\\n",
    "                        batch_size=128,callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "# LOSS\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "\n",
    "plt.plot(epochs, history.history['R2Score'], 'y', label='Training R2')\n",
    "plt.plot(epochs, history.history['val_R2Score'], 'r', label='Validation R2')\n",
    "plt.title('Training and Validation R2Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = used_model.predict(X_test) \n",
    "\n",
    "\n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "plt.xlim(-.25,.25)\n",
    "plt.ylim(-.25,.25)\n",
    "plt.ylabel('y_test')\n",
    "ax = plt.gca()\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = x_vals  # Since y = x\n",
    "plt.plot(x_vals, y_vals, '-', color='black', label='y = x', linewidth=0.5)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0,color='black',linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "#SCATTERPLOT #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  \n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.grid()\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.ylabel('y_test')\n",
    "ax = plt.gca()\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = x_vals  # Since y = x\n",
    "plt.plot(x_vals, y_vals, '-', color='black', label='y = x', linewidth=0.5)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0,color='black',linewidth=0.5)\n",
    "plt.show()\n",
    "#DIRECTIONAL ACCURACY #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  \n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "tp5, fp5, tn5, fn5 = 0, 0, 0, 0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i]>0):\n",
    "        if(y_test[i]>0):\n",
    "            tp+=1\n",
    "        if(y_test[i]<0):\n",
    "            fp+=1\n",
    "        if(y_pred[i]>=5):\n",
    "            if(y_test[i]>0):\n",
    "                tp5+=1\n",
    "            if(y_test[i]<0):\n",
    "                fp5+=1\n",
    "    if(y_pred[i]<0):\n",
    "        if(y_test[i]<0):\n",
    "            tn+=1\n",
    "        if(y_test[i]>0):\n",
    "            fn+=1\n",
    "        if(y_pred[i]<=-5):\n",
    "            if(y_test[i]<0):\n",
    "                tn5+=1\n",
    "            if(y_test[i]>0):\n",
    "                fn5+=1\n",
    "directionalAccuracy = ((tp+tn)/(tp+fp+tn+fn))*10000//1/100\n",
    "print('Directional Accuracy:\\t\\t',directionalAccuracy)\n",
    "directionalAccuracy5guess = ((tp5+tn5)/(tp5+fp5+tn5+fn5))*10000//1/100\n",
    "print('Directional Accuracy >(+/-)5:\\t',directionalAccuracy5guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_model.save('r60_10s_LSTM_5.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3019, 48, 31) \n",
      "y shape: (3019,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=LSTM_testtest.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m y_vals \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX shape:\u001b[39m\u001b[38;5;124m'\u001b[39m,X_vals\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124my shape:\u001b[39m\u001b[38;5;124m'\u001b[39m,y_vals\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 56\u001b[0m scores \u001b[38;5;241m=\u001b[39m walk_forward_validation(X_test, y_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM_testtest.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m avgScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores:\n",
      "Cell \u001b[1;32mIn[78], line 33\u001b[0m, in \u001b[0;36mwalk_forward_validation\u001b[1;34m(X, y, model, n_splits, test_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[:train_end], y[train_end:train_end \u001b[38;5;241m+\u001b[39m test_set_size]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Ensure that your model is recompiled and retrained in each fold\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model_copy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[0;32m     34\u001b[0m model_copy\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Assuming MSE for regression\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Fit the model on the current training set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:193\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    190\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=LSTM_testtest.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def walk_forward_validation(X, y, model, n_splits=20, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for an LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): 3D array of features with shape (n_samples, time_steps, n_features)\n",
    "    y (np.ndarray): 1D array of labels with shape (n_samples,)\n",
    "    model (tf.keras.Model): Compiled LSTM model\n",
    "    n_splits (int): Number of walk-forward splits\n",
    "    test_size (float): Proportion of the data to use as the test set in each split\n",
    "\n",
    "    Returns:\n",
    "    list: MSE scores for each split\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    test_set_size = int(test_size * n_samples)\n",
    "    \n",
    "    mse_scores = []\n",
    "\n",
    "    # Split the data into n_splits segments\n",
    "    for i in range(n_splits):\n",
    "        # Define the index range for the training set (everything before the test set)\n",
    "        train_end = int((i + 1) * (n_samples - test_set_size) / n_splits)\n",
    "\n",
    "        # Define the test set\n",
    "        X_train, X_test = X[:train_end], X[train_end:train_end + test_set_size]\n",
    "        y_train, y_test = y[:train_end], y[train_end:train_end + test_set_size]\n",
    "\n",
    "        # Ensure that your model is recompiled and retrained in each fold\n",
    "        model_copy = tf.keras.models.load_model(model)\n",
    "        model_copy.compile(optimizer='adam', loss='mse')  # Assuming MSE for regression\n",
    "\n",
    "        # Fit the model on the current training set\n",
    "        model_copy.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model_copy.predict(X_test)\n",
    "\n",
    "        # Calculate the mean squared error for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "        print(f'Fold {i+1}/{n_splits}, MSE: {mse}')\n",
    "\n",
    "    return mse_scores\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have already defined `X`, `y`, and compiled your LSTM model\n",
    "# mse_scores = walk_forward_validation(X, y, lstm_model, n_splits=5, test_size=0.2)\n",
    "X_vals = X_test\n",
    "y_vals = y_test\n",
    "print('X shape:',X_vals.shape,'\\ny shape:',y_vals.shape)\n",
    "scores = walk_forward_validation(X_test, y_test, 'LSTM_testtest.keras')\n",
    "avgScore = 0\n",
    "for score in scores:\n",
    "    avgScore+=score\n",
    "avgScore/=len(scores)\n",
    "\n",
    "print('Average MSE:',avgScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vel5</th>\n",
       "      <th>vel10</th>\n",
       "      <th>vel15</th>\n",
       "      <th>vel30</th>\n",
       "      <th>vel60</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc10</th>\n",
       "      <th>acc15</th>\n",
       "      <th>acc30</th>\n",
       "      <th>acc60</th>\n",
       "      <th>...</th>\n",
       "      <th>volD60</th>\n",
       "      <th>vpm5</th>\n",
       "      <th>vpm10</th>\n",
       "      <th>vpm15</th>\n",
       "      <th>vpm30</th>\n",
       "      <th>vpm60</th>\n",
       "      <th>ToD</th>\n",
       "      <th>DoW</th>\n",
       "      <th>MO</th>\n",
       "      <th>Dr1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vel5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696269</td>\n",
       "      <td>0.566629</td>\n",
       "      <td>0.404792</td>\n",
       "      <td>0.292516</td>\n",
       "      <td>0.717757</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.396644</td>\n",
       "      <td>0.279971</td>\n",
       "      <td>0.209327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040104</td>\n",
       "      <td>0.399169</td>\n",
       "      <td>0.295397</td>\n",
       "      <td>0.252108</td>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>-0.006288</td>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.030087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel10</th>\n",
       "      <td>0.696269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.806148</td>\n",
       "      <td>0.574039</td>\n",
       "      <td>0.409941</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.715761</td>\n",
       "      <td>0.566122</td>\n",
       "      <td>0.401907</td>\n",
       "      <td>0.289201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056399</td>\n",
       "      <td>0.261988</td>\n",
       "      <td>0.431247</td>\n",
       "      <td>0.351803</td>\n",
       "      <td>0.258081</td>\n",
       "      <td>0.170056</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>-0.030580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel15</th>\n",
       "      <td>0.566629</td>\n",
       "      <td>0.806148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702885</td>\n",
       "      <td>0.499764</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.711311</td>\n",
       "      <td>0.494297</td>\n",
       "      <td>0.351262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063678</td>\n",
       "      <td>0.211814</td>\n",
       "      <td>0.339981</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.313587</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>-0.010801</td>\n",
       "      <td>-0.006752</td>\n",
       "      <td>-0.021263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel30</th>\n",
       "      <td>0.404792</td>\n",
       "      <td>0.574039</td>\n",
       "      <td>0.702885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706232</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.707956</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>0.146675</td>\n",
       "      <td>0.227690</td>\n",
       "      <td>0.295953</td>\n",
       "      <td>0.459010</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>-0.015124</td>\n",
       "      <td>-0.005017</td>\n",
       "      <td>-0.009727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel60</th>\n",
       "      <td>0.292516</td>\n",
       "      <td>0.409941</td>\n",
       "      <td>0.499764</td>\n",
       "      <td>0.706232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040721</td>\n",
       "      <td>0.105265</td>\n",
       "      <td>0.158133</td>\n",
       "      <td>0.204836</td>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.452845</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>-0.019204</td>\n",
       "      <td>-0.009746</td>\n",
       "      <td>-0.002594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc5</th>\n",
       "      <td>0.717757</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>0.301999</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.012246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc10</th>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.715761</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395404</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.320990</td>\n",
       "      <td>0.125447</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.004608</td>\n",
       "      <td>-0.026688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc15</th>\n",
       "      <td>0.396644</td>\n",
       "      <td>0.566122</td>\n",
       "      <td>0.711311</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.395404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004650</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021239</td>\n",
       "      <td>0.152865</td>\n",
       "      <td>0.253008</td>\n",
       "      <td>0.335755</td>\n",
       "      <td>-0.012689</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>-0.020336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc30</th>\n",
       "      <td>0.279971</td>\n",
       "      <td>0.401907</td>\n",
       "      <td>0.494297</td>\n",
       "      <td>0.707956</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>-0.004650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056943</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.163888</td>\n",
       "      <td>0.213747</td>\n",
       "      <td>0.339243</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.011194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc60</th>\n",
       "      <td>0.209327</td>\n",
       "      <td>0.289201</td>\n",
       "      <td>0.351262</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>0.072164</td>\n",
       "      <td>0.108224</td>\n",
       "      <td>0.140920</td>\n",
       "      <td>0.214447</td>\n",
       "      <td>0.323144</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch12</th>\n",
       "      <td>0.377976</td>\n",
       "      <td>0.478170</td>\n",
       "      <td>0.541923</td>\n",
       "      <td>0.635179</td>\n",
       "      <td>0.636602</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>0.099823</td>\n",
       "      <td>0.134281</td>\n",
       "      <td>0.262216</td>\n",
       "      <td>0.451728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065496</td>\n",
       "      <td>0.302808</td>\n",
       "      <td>0.408260</td>\n",
       "      <td>0.475668</td>\n",
       "      <td>0.566148</td>\n",
       "      <td>0.538376</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>-0.010189</td>\n",
       "      <td>-0.007544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stochDiff6012</th>\n",
       "      <td>0.400457</td>\n",
       "      <td>0.507834</td>\n",
       "      <td>0.565856</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.442418</td>\n",
       "      <td>0.065284</td>\n",
       "      <td>0.129730</td>\n",
       "      <td>0.194854</td>\n",
       "      <td>0.417469</td>\n",
       "      <td>0.403139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091359</td>\n",
       "      <td>0.251414</td>\n",
       "      <td>0.350791</td>\n",
       "      <td>0.411406</td>\n",
       "      <td>0.479059</td>\n",
       "      <td>0.379592</td>\n",
       "      <td>0.012056</td>\n",
       "      <td>-0.008342</td>\n",
       "      <td>-0.010490</td>\n",
       "      <td>-0.009730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSIhl_diff</th>\n",
       "      <td>0.029998</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>-0.003205</td>\n",
       "      <td>-0.030880</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>0.019105</td>\n",
       "      <td>0.023221</td>\n",
       "      <td>0.026310</td>\n",
       "      <td>-0.011032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592633</td>\n",
       "      <td>0.023018</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.180068</td>\n",
       "      <td>-0.020364</td>\n",
       "      <td>-0.138535</td>\n",
       "      <td>-0.006143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSIhl_diffROC</th>\n",
       "      <td>0.035536</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>-0.002526</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>0.047545</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>-0.002836</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268701</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>-0.001552</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002434</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>-0.010908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol</th>\n",
       "      <td>-0.034446</td>\n",
       "      <td>-0.061487</td>\n",
       "      <td>-0.072707</td>\n",
       "      <td>-0.096548</td>\n",
       "      <td>-0.092960</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>-0.006791</td>\n",
       "      <td>-0.043577</td>\n",
       "      <td>-0.028476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402226</td>\n",
       "      <td>-0.020267</td>\n",
       "      <td>-0.025863</td>\n",
       "      <td>-0.031096</td>\n",
       "      <td>-0.041812</td>\n",
       "      <td>-0.050274</td>\n",
       "      <td>0.037538</td>\n",
       "      <td>0.037112</td>\n",
       "      <td>0.546729</td>\n",
       "      <td>-0.012484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol10</th>\n",
       "      <td>-0.024375</td>\n",
       "      <td>-0.050011</td>\n",
       "      <td>-0.068859</td>\n",
       "      <td>-0.097354</td>\n",
       "      <td>-0.098282</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.006932</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.039403</td>\n",
       "      <td>-0.031610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294425</td>\n",
       "      <td>-0.019573</td>\n",
       "      <td>-0.025661</td>\n",
       "      <td>-0.031523</td>\n",
       "      <td>-0.043353</td>\n",
       "      <td>-0.052847</td>\n",
       "      <td>0.043069</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.573243</td>\n",
       "      <td>-0.011404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol15</th>\n",
       "      <td>-0.019875</td>\n",
       "      <td>-0.039884</td>\n",
       "      <td>-0.059506</td>\n",
       "      <td>-0.094437</td>\n",
       "      <td>-0.101208</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.009675</td>\n",
       "      <td>-0.032366</td>\n",
       "      <td>-0.032690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>-0.019550</td>\n",
       "      <td>-0.025131</td>\n",
       "      <td>-0.031419</td>\n",
       "      <td>-0.044165</td>\n",
       "      <td>-0.054638</td>\n",
       "      <td>0.048250</td>\n",
       "      <td>0.040585</td>\n",
       "      <td>0.591939</td>\n",
       "      <td>-0.010286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol30</th>\n",
       "      <td>-0.015631</td>\n",
       "      <td>-0.028293</td>\n",
       "      <td>-0.040582</td>\n",
       "      <td>-0.079453</td>\n",
       "      <td>-0.105122</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>-0.007295</td>\n",
       "      <td>-0.033583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>-0.019288</td>\n",
       "      <td>-0.024235</td>\n",
       "      <td>-0.030175</td>\n",
       "      <td>-0.044407</td>\n",
       "      <td>-0.057573</td>\n",
       "      <td>0.062838</td>\n",
       "      <td>0.043465</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>-0.011967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol60</th>\n",
       "      <td>-0.016707</td>\n",
       "      <td>-0.027100</td>\n",
       "      <td>-0.036276</td>\n",
       "      <td>-0.061492</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.019555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050688</td>\n",
       "      <td>-0.021449</td>\n",
       "      <td>-0.026577</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.043558</td>\n",
       "      <td>-0.058607</td>\n",
       "      <td>0.091407</td>\n",
       "      <td>0.046908</td>\n",
       "      <td>0.642290</td>\n",
       "      <td>-0.013805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD10</th>\n",
       "      <td>-0.045696</td>\n",
       "      <td>-0.039452</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.020902</td>\n",
       "      <td>-0.009628</td>\n",
       "      <td>-0.025341</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.023755</td>\n",
       "      <td>-0.019910</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545014</td>\n",
       "      <td>-0.035195</td>\n",
       "      <td>-0.014472</td>\n",
       "      <td>-0.019168</td>\n",
       "      <td>-0.013615</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>-0.044984</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.018263</td>\n",
       "      <td>0.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD15</th>\n",
       "      <td>-0.046466</td>\n",
       "      <td>-0.054973</td>\n",
       "      <td>-0.045964</td>\n",
       "      <td>-0.031205</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>-0.011361</td>\n",
       "      <td>-0.042100</td>\n",
       "      <td>-0.033771</td>\n",
       "      <td>-0.030971</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701749</td>\n",
       "      <td>-0.036479</td>\n",
       "      <td>-0.028739</td>\n",
       "      <td>-0.026636</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-0.012053</td>\n",
       "      <td>-0.060366</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.019381</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD30</th>\n",
       "      <td>-0.045175</td>\n",
       "      <td>-0.061389</td>\n",
       "      <td>-0.065355</td>\n",
       "      <td>-0.053528</td>\n",
       "      <td>-0.022932</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>-0.025325</td>\n",
       "      <td>-0.038965</td>\n",
       "      <td>-0.052715</td>\n",
       "      <td>-0.017033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894664</td>\n",
       "      <td>-0.034789</td>\n",
       "      <td>-0.036498</td>\n",
       "      <td>-0.042076</td>\n",
       "      <td>-0.034900</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.088542</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.026703</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD60</th>\n",
       "      <td>-0.040104</td>\n",
       "      <td>-0.056399</td>\n",
       "      <td>-0.063678</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>-0.040721</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>-0.021239</td>\n",
       "      <td>-0.056943</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034729</td>\n",
       "      <td>-0.040941</td>\n",
       "      <td>-0.049075</td>\n",
       "      <td>-0.054675</td>\n",
       "      <td>-0.046031</td>\n",
       "      <td>-0.136767</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>-0.002607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm5</th>\n",
       "      <td>0.399169</td>\n",
       "      <td>0.261988</td>\n",
       "      <td>0.211814</td>\n",
       "      <td>0.146675</td>\n",
       "      <td>0.105265</td>\n",
       "      <td>0.301999</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.152865</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.072164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.599395</td>\n",
       "      <td>0.477815</td>\n",
       "      <td>0.329781</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>-0.019992</td>\n",
       "      <td>-0.005129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm10</th>\n",
       "      <td>0.295397</td>\n",
       "      <td>0.431247</td>\n",
       "      <td>0.339981</td>\n",
       "      <td>0.227690</td>\n",
       "      <td>0.158133</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>0.320990</td>\n",
       "      <td>0.253008</td>\n",
       "      <td>0.163888</td>\n",
       "      <td>0.108224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040941</td>\n",
       "      <td>0.599395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739195</td>\n",
       "      <td>0.474857</td>\n",
       "      <td>0.292856</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.009220</td>\n",
       "      <td>-0.024274</td>\n",
       "      <td>-0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm15</th>\n",
       "      <td>0.252108</td>\n",
       "      <td>0.351803</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.295953</td>\n",
       "      <td>0.204836</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.125447</td>\n",
       "      <td>0.335755</td>\n",
       "      <td>0.213747</td>\n",
       "      <td>0.140920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049075</td>\n",
       "      <td>0.477815</td>\n",
       "      <td>0.739195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>-0.010489</td>\n",
       "      <td>-0.029213</td>\n",
       "      <td>-0.003253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm30</th>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.258081</td>\n",
       "      <td>0.313587</td>\n",
       "      <td>0.459010</td>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>-0.012689</td>\n",
       "      <td>0.339243</td>\n",
       "      <td>0.214447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054675</td>\n",
       "      <td>0.329781</td>\n",
       "      <td>0.474857</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.598460</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>-0.039529</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm60</th>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.170056</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.452845</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>0.323144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046031</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.292856</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.598460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>-0.013850</td>\n",
       "      <td>-0.050378</td>\n",
       "      <td>-0.003207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ToD</th>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136767</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.006287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DoW</th>\n",
       "      <td>-0.006288</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>-0.010801</td>\n",
       "      <td>-0.015124</td>\n",
       "      <td>-0.019204</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>-0.009220</td>\n",
       "      <td>-0.010489</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>-0.013850</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>-0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>-0.006752</td>\n",
       "      <td>-0.005017</td>\n",
       "      <td>-0.009746</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.004608</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>-0.019992</td>\n",
       "      <td>-0.024274</td>\n",
       "      <td>-0.029213</td>\n",
       "      <td>-0.039529</td>\n",
       "      <td>-0.050378</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr1</th>\n",
       "      <td>-0.030087</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.021263</td>\n",
       "      <td>-0.009727</td>\n",
       "      <td>-0.002594</td>\n",
       "      <td>-0.012246</td>\n",
       "      <td>-0.026688</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>-0.011194</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.005129</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>-0.003253</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.003207</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   vel5     vel10     vel15     vel30     vel60      acc5  \\\n",
       "vel5           1.000000  0.696269  0.566629  0.404792  0.292516  0.717757   \n",
       "vel10          0.696269  1.000000  0.806148  0.574039  0.409941 -0.000033   \n",
       "vel15          0.566629  0.806148  1.000000  0.702885  0.499764  0.007407   \n",
       "vel30          0.404792  0.574039  0.702885  1.000000  0.706232  0.007091   \n",
       "vel60          0.292516  0.409941  0.499764  0.706232  1.000000  0.009827   \n",
       "acc5           0.717757 -0.000033  0.007407  0.007091  0.009827  1.000000   \n",
       "acc10          0.488963  0.715761  0.289400  0.011623  0.012296 -0.013115   \n",
       "acc15          0.396644  0.566122  0.711311  0.000013  0.004736  0.003432   \n",
       "acc30          0.279971  0.401907  0.494297  0.707956 -0.000029  0.000202   \n",
       "acc60          0.209327  0.289201  0.351262  0.498343  0.702112  0.011057   \n",
       "stoch12        0.377976  0.478170  0.541923  0.635179  0.636602  0.062749   \n",
       "stochDiff6012  0.400457  0.507834  0.565856  0.607925  0.442418  0.065284   \n",
       "RSIhl_diff     0.029998  0.018920  0.014242 -0.003205 -0.030880  0.023451   \n",
       "RSIhl_diffROC  0.035536  0.002013  0.002452 -0.002526 -0.000746  0.047545   \n",
       "vol           -0.034446 -0.061487 -0.072707 -0.096548 -0.092960  0.011658   \n",
       "vol10         -0.024375 -0.050011 -0.068859 -0.097354 -0.098282  0.014531   \n",
       "vol15         -0.019875 -0.039884 -0.059506 -0.094437 -0.101208  0.010969   \n",
       "vol30         -0.015631 -0.028293 -0.040582 -0.079453 -0.105122  0.005635   \n",
       "vol60         -0.016707 -0.027100 -0.036276 -0.061492 -0.100000  0.002979   \n",
       "volD10        -0.045696 -0.039452 -0.031596 -0.020902 -0.009628 -0.025341   \n",
       "volD15        -0.046466 -0.054973 -0.045964 -0.031205 -0.013122 -0.011361   \n",
       "volD30        -0.045175 -0.061389 -0.065355 -0.053528 -0.022932 -0.003340   \n",
       "volD60        -0.040104 -0.056399 -0.063678 -0.069082 -0.040721 -0.001119   \n",
       "vpm5           0.399169  0.261988  0.211814  0.146675  0.105265  0.301999   \n",
       "vpm10          0.295397  0.431247  0.339981  0.227690  0.158133 -0.006754   \n",
       "vpm15          0.252108  0.351803  0.446810  0.295953  0.204836  0.009999   \n",
       "vpm30          0.187244  0.258081  0.313587  0.459010  0.309917  0.010539   \n",
       "vpm60          0.122011  0.170056  0.208718  0.297900  0.452845  0.005033   \n",
       "ToD            0.007345  0.010114  0.012498  0.017696  0.025193  0.000411   \n",
       "DoW           -0.006288 -0.008862 -0.010801 -0.015124 -0.019204 -0.000164   \n",
       "MO            -0.005597 -0.006926 -0.006752 -0.005017 -0.009746 -0.001095   \n",
       "Dr1           -0.030087 -0.030580 -0.021263 -0.009727 -0.002594 -0.012246   \n",
       "\n",
       "                  acc10     acc15     acc30     acc60  ...    volD60  \\\n",
       "vel5           0.488963  0.396644  0.279971  0.209327  ... -0.040104   \n",
       "vel10          0.715761  0.566122  0.401907  0.289201  ... -0.056399   \n",
       "vel15          0.289400  0.711311  0.494297  0.351262  ... -0.063678   \n",
       "vel30          0.011623  0.000013  0.707956  0.498343  ... -0.069082   \n",
       "vel60          0.012296  0.004736 -0.000029  0.702112  ... -0.040721   \n",
       "acc5          -0.013115  0.003432  0.000202  0.011057  ... -0.001119   \n",
       "acc10          1.000000  0.395404  0.004165  0.011784  ... -0.014793   \n",
       "acc15          0.395404  1.000000 -0.004650  0.001396  ... -0.021239   \n",
       "acc30          0.004165 -0.004650  1.000000  0.003486  ... -0.056943   \n",
       "acc60          0.011784  0.001396  0.003486  1.000000  ... -0.031837   \n",
       "stoch12        0.099823  0.134281  0.262216  0.451728  ... -0.065496   \n",
       "stochDiff6012  0.129730  0.194854  0.417469  0.403139  ... -0.091359   \n",
       "RSIhl_diff     0.019105  0.023221  0.026310 -0.011032  ... -0.592633   \n",
       "RSIhl_diffROC  0.004153  0.005933 -0.002836 -0.001631  ... -0.268701   \n",
       "vol           -0.008032 -0.006791 -0.043577 -0.028476  ...  0.402226   \n",
       "vol10          0.006932 -0.000592 -0.039403 -0.031610  ...  0.294425   \n",
       "vol15          0.016860  0.009675 -0.032366 -0.032690  ...  0.211861   \n",
       "vol30          0.012062  0.021460 -0.007295 -0.033583  ...  0.074166   \n",
       "vol60          0.005914  0.009768  0.012955 -0.019555  ... -0.050688   \n",
       "volD10        -0.032221 -0.023755 -0.019910 -0.006221  ...  0.545014   \n",
       "volD15        -0.042100 -0.033771 -0.030971 -0.009095  ...  0.701749   \n",
       "volD30        -0.025325 -0.038965 -0.052715 -0.017033  ...  0.894664   \n",
       "volD60        -0.014793 -0.021239 -0.056943 -0.031837  ...  1.000000   \n",
       "vpm5           0.191872  0.152865  0.102190  0.072164  ... -0.034729   \n",
       "vpm10          0.320990  0.253008  0.163888  0.108224  ... -0.040941   \n",
       "vpm15          0.125447  0.335755  0.213747  0.140920  ... -0.049075   \n",
       "vpm30          0.007287 -0.012689  0.339243  0.214447  ... -0.054675   \n",
       "vpm60          0.002171 -0.000914 -0.030884  0.323144  ... -0.046031   \n",
       "ToD            0.000022  0.000070 -0.000141  0.000812  ... -0.136767   \n",
       "DoW           -0.000255 -0.000236 -0.002206 -0.001878  ...  0.002594   \n",
       "MO            -0.004608 -0.004520  0.002634  0.000152  ...  0.055587   \n",
       "Dr1           -0.026688 -0.020336 -0.011194  0.000266  ... -0.002607   \n",
       "\n",
       "                   vpm5     vpm10     vpm15     vpm30     vpm60       ToD  \\\n",
       "vel5           0.399169  0.295397  0.252108  0.187244  0.122011  0.007345   \n",
       "vel10          0.261988  0.431247  0.351803  0.258081  0.170056  0.010114   \n",
       "vel15          0.211814  0.339981  0.446810  0.313587  0.208718  0.012498   \n",
       "vel30          0.146675  0.227690  0.295953  0.459010  0.297900  0.017696   \n",
       "vel60          0.105265  0.158133  0.204836  0.309917  0.452845  0.025193   \n",
       "acc5           0.301999 -0.006754  0.009999  0.010539  0.005033  0.000411   \n",
       "acc10          0.191872  0.320990  0.125447  0.007287  0.002171  0.000022   \n",
       "acc15          0.152865  0.253008  0.335755 -0.012689 -0.000914  0.000070   \n",
       "acc30          0.102190  0.163888  0.213747  0.339243 -0.030884 -0.000141   \n",
       "acc60          0.072164  0.108224  0.140920  0.214447  0.323144  0.000812   \n",
       "stoch12        0.302808  0.408260  0.475668  0.566148  0.538376  0.012769   \n",
       "stochDiff6012  0.251414  0.350791  0.411406  0.479059  0.379592  0.012056   \n",
       "RSIhl_diff     0.023018  0.023544  0.025213  0.017613  0.001776  0.180068   \n",
       "RSIhl_diffROC  0.008598 -0.001552  0.000344 -0.004357 -0.002434  0.005355   \n",
       "vol           -0.020267 -0.025863 -0.031096 -0.041812 -0.050274  0.037538   \n",
       "vol10         -0.019573 -0.025661 -0.031523 -0.043353 -0.052847  0.043069   \n",
       "vol15         -0.019550 -0.025131 -0.031419 -0.044165 -0.054638  0.048250   \n",
       "vol30         -0.019288 -0.024235 -0.030175 -0.044407 -0.057573  0.062838   \n",
       "vol60         -0.021449 -0.026577 -0.031855 -0.043558 -0.058607  0.091407   \n",
       "volD10        -0.035195 -0.014472 -0.019168 -0.013615 -0.008109 -0.044984   \n",
       "volD15        -0.036479 -0.028739 -0.026636 -0.019305 -0.012053 -0.060366   \n",
       "volD30        -0.034789 -0.036498 -0.042076 -0.034900 -0.024396 -0.088542   \n",
       "volD60        -0.034729 -0.040941 -0.049075 -0.054675 -0.046031 -0.136767   \n",
       "vpm5           1.000000  0.599395  0.477815  0.329781  0.207445  0.004742   \n",
       "vpm10          0.599395  1.000000  0.739195  0.474857  0.292856  0.003320   \n",
       "vpm15          0.477815  0.739195  1.000000  0.612409  0.383153  0.007189   \n",
       "vpm30          0.329781  0.474857  0.612409  1.000000  0.598460  0.012997   \n",
       "vpm60          0.207445  0.292856  0.383153  0.598460  1.000000  0.024778   \n",
       "ToD            0.004742  0.003320  0.007189  0.012997  0.024778  1.000000   \n",
       "DoW           -0.007764 -0.009220 -0.010489 -0.012419 -0.013850  0.000294   \n",
       "MO            -0.019992 -0.024274 -0.029213 -0.039529 -0.050378  0.044723   \n",
       "Dr1           -0.005129 -0.003280 -0.003253  0.000037 -0.003207  0.006287   \n",
       "\n",
       "                    DoW        MO       Dr1  \n",
       "vel5          -0.006288 -0.005597 -0.030087  \n",
       "vel10         -0.008862 -0.006926 -0.030580  \n",
       "vel15         -0.010801 -0.006752 -0.021263  \n",
       "vel30         -0.015124 -0.005017 -0.009727  \n",
       "vel60         -0.019204 -0.009746 -0.002594  \n",
       "acc5          -0.000164 -0.001095 -0.012246  \n",
       "acc10         -0.000255 -0.004608 -0.026688  \n",
       "acc15         -0.000236 -0.004520 -0.020336  \n",
       "acc30         -0.002206  0.002634 -0.011194  \n",
       "acc60         -0.001878  0.000152  0.000266  \n",
       "stoch12       -0.014696 -0.010189 -0.007544  \n",
       "stochDiff6012 -0.008342 -0.010490 -0.009730  \n",
       "RSIhl_diff    -0.020364 -0.138535 -0.006143  \n",
       "RSIhl_diffROC  0.000161  0.001681 -0.010908  \n",
       "vol            0.037112  0.546729 -0.012484  \n",
       "vol10          0.039122  0.573243 -0.011404  \n",
       "vol15          0.040585  0.591939 -0.010286  \n",
       "vol30          0.043465  0.622879 -0.011967  \n",
       "vol60          0.046908  0.642290 -0.013805  \n",
       "volD10         0.000776  0.018263  0.002834  \n",
       "volD15         0.001110  0.019381  0.000511  \n",
       "volD30         0.001874  0.026703  0.001492  \n",
       "volD60         0.002594  0.055587 -0.002607  \n",
       "vpm5          -0.007764 -0.019992 -0.005129  \n",
       "vpm10         -0.009220 -0.024274 -0.003280  \n",
       "vpm15         -0.010489 -0.029213 -0.003253  \n",
       "vpm30         -0.012419 -0.039529  0.000037  \n",
       "vpm60         -0.013850 -0.050378 -0.003207  \n",
       "ToD            0.000294  0.044723  0.006287  \n",
       "DoW            1.000000  0.001395 -0.000859  \n",
       "MO             0.001395  1.000000 -0.005728  \n",
       "Dr1           -0.000859 -0.005728  1.000000  \n",
       "\n",
       "[32 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
