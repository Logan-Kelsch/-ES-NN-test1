{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('multi.csv')\n",
    "\n",
    "#testing random feature drops\n",
    "#data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='CLASS')\n",
    "\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "#filtering before splitting could be useful if ABSOLUTELY mostly comprised of 'in'\n",
    "#MARKET HOURS!\n",
    "data = data.drop(data[data['ToD'] > 950].index)\n",
    "data = data.drop(data[data['ToD'] < 560].index)\n",
    "#OTHER MODIFICATIONS\n",
    "#data = data.drop(data[data['feature'] condition].index)\n",
    "data = data.drop(data[data['vol'] < 10000].index)\n",
    "\n",
    "up_Rows = data.drop(data[data['CLASS'] != '_4up'].index)\n",
    "upRRows = data.drop(data[data['CLASS'] != '_3upR'].index)\n",
    "dnRRows = data.drop(data[data['CLASS'] != '_2dnR'].index)\n",
    "dn_Rows = data.drop(data[data['CLASS'] != '_1dn'].index)\n",
    "\n",
    "smallestClass = min(up_Rows.index.size, upRRows.index.size, dnRRows.index.size, dn_Rows.index.size)\n",
    "print('Smallest Class Size:',smallestClass,'\\n')\n",
    "\n",
    "up_Rows = up_Rows.iloc[0:smallestClass]\n",
    "upRRows = upRRows.iloc[0:smallestClass]\n",
    "dnRRows = dnRRows.iloc[0:smallestClass]\n",
    "dn_Rows = dn_Rows.iloc[0:smallestClass]\n",
    "\n",
    "optData = pd.concat([up_Rows, upRRows, dnRRows, dn_Rows],axis=0)\n",
    "\n",
    "print(\"OCCURANCES IN OPT DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(optData.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "#percIn = data.size/(inRows.size*2)\n",
    "#percNin = data.size/(NinRows.size*2)\n",
    "weight_for_0 = .25\n",
    "weight_for_1 = .25\n",
    "weight_for_2 = .25\n",
    "weight_for_3 = .25\n",
    "cw = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3}\n",
    "classWeights = list(cw.values())\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = optData.iloc[:, :-1].values\n",
    "y = optData.iloc[:, -1].values\n",
    "\n",
    "#SMOTE OVERSAMPLING________________\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X, y = smote.fit_resample(X,y)\n",
    "#print('\\n[PRE-SPLIT] Resampled Data size:',X.size,'--',y.size)\n",
    "\n",
    "#__________________________________\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)#42, stratify=y)\n",
    "\n",
    "# one-hot encode ? \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train, num_classes=4)\n",
    "#y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "#RESAMPLED DATA- POST SPLIT---------------------------------------------------------\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train,y_train)\n",
    "#print('\\nResampled Data size:',X_resampled.size)\n",
    "\n",
    "#BUILD THE NEURAL NETWORK MODEL-------------------------------------------------------\n",
    "\n",
    "#CUSTOM CALLBACK FOR PRECISION RATIO TRAINING VS VALIDATION--------------------------------------------------\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "metric_ratio = tf.Variable(1.0, dtype=tf.float32, name=\"metric_ratio\")\n",
    "\n",
    "class MetricBalancingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, metric='accuracy'):\n",
    "        super(MetricBalancingCallback, self).__init__()\n",
    "        self.metric = metric\n",
    "        self.train_metric = 0\n",
    "        self.val_metric = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Based on the metric, get corresponding values from logs\n",
    "        #print(f\"\\nEpoch {epoch + 1} logs: {logs}\")\n",
    "        if self.metric == 'precision':\n",
    "            self.train_metric = logs.get('precision')\n",
    "            self.val_metric = logs.get('val_precision')\n",
    "        elif self.metric == 'recall':\n",
    "            self.train_metric = logs.get('recall')\n",
    "            self.val_metric = logs.get('val_recall')\n",
    "        elif self.metric == 'accuracy':\n",
    "            self.train_metric = logs.get('accuracy')\n",
    "            self.val_metric = logs.get('val_accuracy')\n",
    "        \n",
    "        # Optionally print the values for monitoring\n",
    "        print(f\"\\nEpoch {epoch + 1} - Train {self.metric.capitalize()}: {self.train_metric:.4f} - Val {self.metric.capitalize()}: {self.val_metric:.4f}\")\n",
    "        # Dynamically adjust the metric ratio\n",
    "        #global metric_ratio\n",
    "        if self.val_metric and self.train_metric:\n",
    "            ratio = self.train_metric / (self.val_metric + 1e-7)\n",
    "        else:\n",
    "            ratio = 1.0  # Fallback in case metrics aren't available\n",
    "\n",
    "        # Store metric_ratio globally using Keras backend\n",
    "        K.set_value(metric_ratio,ratio)\n",
    "        #K.set_value(self.metric_ratio_var, ratio)\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Initialize the metric_ratio variable in the backend at the beginning of training\n",
    "        self.metric_ratio_var = K.variable(1.0, name=\"metric_ratio\")\n",
    "        \n",
    "\n",
    "from keras.saving import get_custom_objects\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "get_custom_objects().clear()\n",
    "\n",
    "#CUSTOM LOSS 1_______________________________________________________________________________________________\n",
    "@register_keras_serializable(name=\"custom_loss\")\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    true_class = K.argmax(y_true, axis=-1)\n",
    "    pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Create a matrix where misclassifications (farthest ones) are heavily penalized\n",
    "    inverse_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 0))   # True class 3, predicted class 0\n",
    "    )\n",
    "    bad1_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 2)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "    )\n",
    "    bad2_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 0)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 1))   # True class 3, predicted class 0\n",
    "    )\n",
    "    # Create a matrix where correct predictions for 0-0 and 3-3 are rewarded\n",
    "    extrema_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 0)),  # True class 0, predicted class 0\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 3)),  # True class 1, predicted class 1\n",
    "    )\n",
    "    fair_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 1)),  # True class 2, predicted class 2\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 2))   # True class 3, predicted class 3\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Standard categorical crossentropy loss\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Add an additional penalty for farthest misclassifications\n",
    "    pen_inverse = 4.0  # You can adjust this penalty factor\n",
    "    pen_inverse_loss = K.cast(inverse_misclass, tf.float32) * pen_inverse\n",
    "\n",
    "    pen_bad1 = 0.0  # You can adjust this penalty factor\n",
    "    pen_bad1_loss = K.cast(bad1_misclass, tf.float32) * pen_bad1\n",
    "\n",
    "    pen_bad2 = 0.0  # You can adjust this penalty factor\n",
    "    pen_bad2_loss = K.cast(bad2_misclass, tf.float32) * pen_bad2\n",
    "\n",
    "    # Add a reward for correct 0-0 and 3-3 predictions (negative penalty)\n",
    "    rew_extrema = 4.0  # You can adjust this reward factor\n",
    "    rew_extrema_gain = K.cast(extrema_correct, tf.float32) * rew_extrema\n",
    "\n",
    "    rew_fair = 0.0  # You can adjust this reward factor\n",
    "    rew_fair_gain = K.cast(fair_correct, tf.float32) * rew_fair\n",
    "\n",
    "    # Return the combined loss: base loss + penalties - rewards\n",
    "    return base_loss + (pen_inverse_loss + pen_bad1_loss + pen_bad2_loss) - (rew_extrema_gain + rew_fair_gain)\n",
    "\n",
    "#CUSTOM LOSS 2____________________________________________________________________________________________________\n",
    "@register_keras_serializable(name=\"focal_loss\")\n",
    "def focal_loss(gamma=2.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - gamma: Focusing parameter that adjusts the rate at which easy examples are down-weighted.\n",
    "             Default value is 2. Higher values make the loss more focused on hard examples.\n",
    "             \n",
    "    - alpha: Class balancing factor to balance the loss for each class. Default is 0.25.\n",
    "             Adjust this to address class imbalance. Can be a scalar or a list of weights\n",
    "             per class.\n",
    "    \"\"\"\n",
    "    #gamma = 2.0\n",
    "    alpha = classWeights\n",
    " \n",
    "    @register_keras_serializable(name=\"focal_loss_fixed\")\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the focal loss between ground truth (y_true) and predicted values (y_pred).\n",
    "        \n",
    "        Arguments:\n",
    "        - y_true: Tensor of true labels (one-hot encoded, shape = [batch_size, num_classes]).\n",
    "        - y_pred: Tensor of predicted probabilities (shape = [batch_size, num_classes]).\n",
    "        \n",
    "        Returns:\n",
    "        - loss: A scalar tensor representing the computed focal loss.\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0) or division by zero\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Compute the cross-entropy loss (standard loss component)\n",
    "        cross_entropy_loss = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Compute the modulating factor: (1 - p_t)^gamma\n",
    "        # where p_t is the predicted probability for the true class\n",
    "        modulating_factor = tf.pow(1. - y_pred, gamma)\n",
    "        \n",
    "        # Compute the final focal loss: alpha * modulating_factor * cross_entropy_loss\n",
    "        loss = alpha * modulating_factor * cross_entropy_loss\n",
    "        \n",
    "        # Reduce the loss along the batch dimension\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "#CUSTOM LOSS 3_____________________________________________________________________________________________\n",
    "@register_keras_serializable(name=\"weighted_rec_pre_loss\")\n",
    "def weighted_rec_pre_loss(func='wr',weight=5.0):\n",
    "    \"\"\"\n",
    "    Custom loss function to optimize for recall OR for precision.\n",
    "    func should equal wr or wp\n",
    "    for weighted recall or weighted precision\n",
    "    \"\"\"\n",
    "    @register_keras_serializable(name='wp_loss')\n",
    "    def wp_loss(y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0) or division by zero\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        # Standard binary cross-entropy\n",
    "        base_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        # Apply higher weight to positive samples (to penalize false negatives more)\n",
    "        loss = (1 - y_true) * weight * base_loss + y_true * base_loss\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    @register_keras_serializable(name=\"wr_loss\")\n",
    "    def wr_loss(y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0) or division by zero\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        # Standard binary cross-entropy\n",
    "        base_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        # Apply higher weight to positive samples (to penalize false negatives more)\n",
    "        loss = weight * y_true * base_loss + (1 - y_true) * base_loss\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(func=='wr'):\n",
    "        return wr_loss\n",
    "    else:\n",
    "        return wp_loss\n",
    "#CUSTOM LOSS 4_____________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#metric_ratio = 1.0  # Initialize globally\n",
    "@register_keras_serializable(name=\"met_ratio\")\n",
    "def met_ratio(y_true, y_pred, base_weight=1.0):\n",
    "    crnt_metric_ratio = metric_ratio\n",
    "    \n",
    "    # Clip predictions to prevent log(0) or division by zero\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    \n",
    "    # Standard binary cross-entropy loss\n",
    "    base_loss = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "    \n",
    "    # Adjust weight using precision_ratio (balance training and validation precision)\n",
    "    adjusted_weight = base_weight / (crnt_metric_ratio + 1e-7)\n",
    "    \n",
    "    # Apply the dynamic weight to false positives\n",
    "    weighted_loss = (1 - y_true) * adjusted_weight * base_loss + y_true * base_loss\n",
    "    \n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "#END CUSTOM LOSSES__________________________________________________________________________________________\n",
    "\n",
    "#LEARNING RATES____________________________________________________________________________________________\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    #good rough val to start, .25, good val to end at .0015.\n",
    "    #5k epoch should be: .25, 8565, .9995, true\n",
    "    0.05,\n",
    "    decay_steps=465,\n",
    "    decay_rate=.99,\n",
    "    staircase=True)\n",
    "\n",
    "opt1 = SGD(learning_rate=0.0001)\n",
    "opt2  = tf.keras.optimizers.Adam(clipnorm=0.7)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "#BUILD AND LOAD MODEL__________________________________________________________________________________________\n",
    "\n",
    "metric_callback = MetricBalancingCallback(metric='precision')\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([#currently 17 total features\n",
    "        tf.keras.layers.Dense(2048),#kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.32),\n",
    "        tf.keras.layers.Dense(1024),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.32),\n",
    "        tf.keras.layers.Dense(256),#,  kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.08),\n",
    "        tf.keras.layers.Dense(64),#, kernel_regularizer=tf.keras.regularizers.l2(0.05)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.04),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    #AUC=tf.keras.metrics.AUC(curve='PR')\n",
    "    met = ['precision','recall','accuracy']\n",
    "    model.compile(optimizer=opt3,\n",
    "                  loss=met_ratio\n",
    "                  ,metrics=met)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    #loaded_model = tf.keras.models.load_model('tupleTrain.keras', custom_objects={'custom_loss':custom_loss})\n",
    "    loaded_model = tf.keras.models.load_model('multi_test1.keras')\n",
    "    met = ['accuracy','precision','recall']\n",
    "    loaded_model.compile(optimizer=opt3,\n",
    "                         loss=met_ratio\n",
    "                         , metrics=met)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_recall', patience=250, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = build_model()\n",
    "loaded_model = load_model()\n",
    "history = loaded_model.fit(X_train, y_train, epochs=epochs, validation_split=0.2,\\\n",
    "                    shuffle=True, verbose=1, validation_data=(X_test, y_test),\\\n",
    "                    class_weight=cw, callbacks=[metric_callback])\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "#_, acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# LOSS\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "plt.plot(epochs, history.history['accuracy'], 'y', label='Training acc')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# AUC\n",
    "plt.plot(epochs, history.history['AUC'], 'y', label='Training AUC')\n",
    "plt.plot(epochs, history.history['val_AUC'], 'r', label='Validation AUC')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# PRECISION\n",
    "plt.plot(epochs, history.history['precision'], 'y', label='Training Precision')\n",
    "plt.plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# RECALL\n",
    "plt.plot(epochs, history.history['recall'], 'y', label='Training Recall')\n",
    "plt.plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# TPR\n",
    "'''\n",
    "TPR = history.history['TruePositives']/(history.history['TruePositives']+history.history['TrueNegatives'])\n",
    "val_TPR = history.history['val_TruePositives']/(history.history['val_TruePositives']+history.history['val_TrueNegatives'])\n",
    "plt.plot(epochs, TPR, 'y', label='Training TPR')\n",
    "plt.plot(epochs, val_TPR, 'r', label='Validation TPR')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('TP Rate')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "#predicting the test set results\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert one-hot to class indices if needed\n",
    "y_pred = np.argmax(loaded_model.predict(X_test), axis=1)  # Predictions to class indices\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=range(4), yticklabels=range(4))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for 4-Class Classification')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "#model.save('epoch15k.keras')\n",
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "loaded_model.save('multi_test1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = cw.values()\n",
    "print(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
