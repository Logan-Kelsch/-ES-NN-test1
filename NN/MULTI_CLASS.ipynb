{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTED FEATURES: \n",
      "Index(['FT', 'FullK', 'diffKD', 'OB', 'OS', 'vol', 's15', 's30', 's60', 'ToD',\n",
      "       'Inertias', 'percBB', 'spreadRSI', 'ADX', 'RSI', 'Wpercent', 'acc'],\n",
      "      dtype='object')\n",
      "TESTING FOR: \n",
      "CLASS\n",
      "OCCURANCES IN RAW DATA FOR CLASS: \n",
      "{'_1dn': 53217, '_2dnR': 50171, '_3upR': 49588, '_4up': 49938}\n",
      "Smallest Class Size: 7258 \n",
      "\n",
      "OCCURANCES IN OPT DATA FOR CLASS: \n",
      "{'_1dn': 7258, '_2dnR': 7258, '_3upR': 7258, '_4up': 7258}\n",
      "Epoch 1/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - Accuracy: 0.5374 - F1Score: 0.5365 - Precision: 0.6208 - Recall: 0.4073 - loss: -0.0109 - val_Accuracy: 0.6566 - val_F1Score: 0.6559 - val_Precision: 0.7605 - val_Recall: 0.5152 - val_loss: -0.3757\n",
      "Epoch 2/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - Accuracy: 0.5400 - F1Score: 0.5393 - Precision: 0.6187 - Recall: 0.4089 - loss: -0.0090 - val_Accuracy: 0.6544 - val_F1Score: 0.6540 - val_Precision: 0.7611 - val_Recall: 0.5152 - val_loss: -0.3637\n",
      "Epoch 3/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5441 - F1Score: 0.5433 - Precision: 0.6244 - Recall: 0.4181 - loss: -0.0114 - val_Accuracy: 0.6539 - val_F1Score: 0.6534 - val_Precision: 0.7596 - val_Recall: 0.5132 - val_loss: -0.3636\n",
      "Epoch 4/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5424 - F1Score: 0.5416 - Precision: 0.6212 - Recall: 0.4172 - loss: -0.0102 - val_Accuracy: 0.6540 - val_F1Score: 0.6537 - val_Precision: 0.7570 - val_Recall: 0.5154 - val_loss: -0.3625\n",
      "Epoch 5/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5366 - F1Score: 0.5358 - Precision: 0.6209 - Recall: 0.4157 - loss: -0.0065 - val_Accuracy: 0.6544 - val_F1Score: 0.6540 - val_Precision: 0.7587 - val_Recall: 0.5133 - val_loss: -0.3640\n",
      "Epoch 6/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5399 - F1Score: 0.5386 - Precision: 0.6240 - Recall: 0.4080 - loss: -0.0104 - val_Accuracy: 0.6544 - val_F1Score: 0.6540 - val_Precision: 0.7595 - val_Recall: 0.5118 - val_loss: -0.3640\n",
      "Epoch 7/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5400 - F1Score: 0.5389 - Precision: 0.6202 - Recall: 0.4109 - loss: -0.0111 - val_Accuracy: 0.6540 - val_F1Score: 0.6536 - val_Precision: 0.7572 - val_Recall: 0.5118 - val_loss: -0.3632\n",
      "Epoch 8/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5403 - F1Score: 0.5397 - Precision: 0.6234 - Recall: 0.4121 - loss: -0.0084 - val_Accuracy: 0.6559 - val_F1Score: 0.6556 - val_Precision: 0.7607 - val_Recall: 0.5120 - val_loss: -0.3669\n",
      "Epoch 9/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5365 - F1Score: 0.5360 - Precision: 0.6226 - Recall: 0.4140 - loss: -0.0060 - val_Accuracy: 0.6537 - val_F1Score: 0.6532 - val_Precision: 0.7591 - val_Recall: 0.5096 - val_loss: -0.3644\n",
      "Epoch 10/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5430 - F1Score: 0.5420 - Precision: 0.6266 - Recall: 0.4116 - loss: -0.0106 - val_Accuracy: 0.6540 - val_F1Score: 0.6536 - val_Precision: 0.7578 - val_Recall: 0.5113 - val_loss: -0.3613\n",
      "Epoch 11/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5500 - F1Score: 0.5494 - Precision: 0.6394 - Recall: 0.4236 - loss: -0.0177 - val_Accuracy: 0.6530 - val_F1Score: 0.6525 - val_Precision: 0.7583 - val_Recall: 0.5111 - val_loss: -0.3621\n",
      "Epoch 12/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - Accuracy: 0.5396 - F1Score: 0.5387 - Precision: 0.6250 - Recall: 0.4093 - loss: -0.0119 - val_Accuracy: 0.6513 - val_F1Score: 0.6508 - val_Precision: 0.7553 - val_Recall: 0.5092 - val_loss: -0.3571\n",
      "Epoch 13/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5412 - F1Score: 0.5406 - Precision: 0.6318 - Recall: 0.4153 - loss: -0.0094 - val_Accuracy: 0.6521 - val_F1Score: 0.6517 - val_Precision: 0.7591 - val_Recall: 0.5085 - val_loss: -0.3588\n",
      "Epoch 14/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5394 - F1Score: 0.5384 - Precision: 0.6227 - Recall: 0.4142 - loss: -0.0096 - val_Accuracy: 0.6525 - val_F1Score: 0.6520 - val_Precision: 0.7575 - val_Recall: 0.5099 - val_loss: -0.3577\n",
      "Epoch 15/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - Accuracy: 0.5417 - F1Score: 0.5410 - Precision: 0.6209 - Recall: 0.4119 - loss: -0.0116 - val_Accuracy: 0.6521 - val_F1Score: 0.6517 - val_Precision: 0.7553 - val_Recall: 0.5092 - val_loss: -0.3581\n",
      "Epoch 16/1000\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - Accuracy: 0.5395 - F1Score: 0.5381 - Precision: 0.6241 - Recall: 0.4118 - loss: -0.0104 - val_Accuracy: 0.6523 - val_F1Score: 0.6519 - val_Precision: 0.7564 - val_Recall: 0.5085 - val_loss: -0.3590\n",
      "Epoch 17/1000\n",
      "\u001b[1m526/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - Accuracy: 0.5454 - F1Score: 0.5449 - Precision: 0.6220 - Recall: 0.4136 - loss: -0.0126"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 261\u001b[0m\n\u001b[0;32m    259\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[0;32m    260\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m load_model()\n\u001b[1;32m--> 261\u001b[0m history \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\\\n\u001b[0;32m    262\u001b[0m                     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test),\\\n\u001b[0;32m    263\u001b[0m                     class_weight\u001b[38;5;241m=\u001b[39mcw, callbacks\u001b[38;5;241m=\u001b[39mearly_stopping)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m#_, acc = model.evaluate(X_test, y_test)\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m#print(\"Accuracy = \", (acc * 100.0), \"%\")\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# LOSS\u001b[39;00m\n\u001b[0;32m    271\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('multi.csv')\n",
    "\n",
    "#testing random feature drops\n",
    "#data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='CLASS')\n",
    "\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "#filtering before splitting could be useful if ABSOLUTELY mostly comprised of 'in'\n",
    "#MARKET HOURS!\n",
    "data = data.drop(data[data['ToD'] > 950].index)\n",
    "data = data.drop(data[data['ToD'] < 545].index)\n",
    "#OTHER MODIFICATIONS\n",
    "#data = data.drop(data[data['feature'] condition].index)\n",
    "data = data.drop(data[data['vol'] < 10000].index)\n",
    "\n",
    "up_Rows = data.drop(data[data['CLASS'] != '_4up'].index)\n",
    "upRRows = data.drop(data[data['CLASS'] != '_3upR'].index)\n",
    "dnRRows = data.drop(data[data['CLASS'] != '_2dnR'].index)\n",
    "dn_Rows = data.drop(data[data['CLASS'] != '_1dn'].index)\n",
    "\n",
    "smallestClass = min(up_Rows.index.size, upRRows.index.size, dnRRows.index.size, dn_Rows.index.size)\n",
    "print('Smallest Class Size:',smallestClass,'\\n')\n",
    "\n",
    "up_Rows = up_Rows.iloc[0:smallestClass]\n",
    "upRRows = upRRows.iloc[0:smallestClass]\n",
    "dnRRows = dnRRows.iloc[0:smallestClass]\n",
    "dn_Rows = dn_Rows.iloc[0:smallestClass]\n",
    "\n",
    "optData = pd.concat([up_Rows, upRRows, dnRRows, dn_Rows],axis=0)\n",
    "\n",
    "print(\"OCCURANCES IN OPT DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(optData.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "#percIn = data.size/(inRows.size*2)\n",
    "#percNin = data.size/(NinRows.size*2)\n",
    "weight_for_0 = .25\n",
    "weight_for_1 = .25\n",
    "weight_for_2 = .25\n",
    "weight_for_3 = .25\n",
    "cw = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3}\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = optData.iloc[:, :-1].values\n",
    "y = optData.iloc[:, -1].values\n",
    "\n",
    "#SMOTE OVERSAMPLING________________\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X, y = smote.fit_resample(X,y)\n",
    "#print('\\n[PRE-SPLIT] Resampled Data size:',X.size,'--',y.size)\n",
    "\n",
    "#__________________________________\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) #stratify=y,\n",
    "\n",
    "# one-hot encode ? \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train, num_classes=4)\n",
    "#y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "#RESAMPLED DATA- POST SPLIT---------------------------------------------------------\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train,y_train)\n",
    "#print('\\nResampled Data size:',X_resampled.size)\n",
    "\n",
    "#BUILD THE NEURAL NETWORK MODEL-------------------------------------------------------\n",
    "\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    #good rough val to start, .25, good val to end at .0015.\n",
    "    #5k epoch should be: .25, 8565, .9995, true\n",
    "    0.004,\n",
    "    decay_steps=82,\n",
    "    decay_rate=0.996,\n",
    "    staircase=True)\n",
    "\n",
    "opt1 = SGD(learning_rate=0.0001)\n",
    "opt2  = tf.keras.optimizers.Adam(clipnorm=0.7)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "from keras.saving import get_custom_objects\n",
    "from keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "get_custom_objects().clear()\n",
    "@register_keras_serializable(name=\"custom_loss\")\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    true_class = K.argmax(y_true, axis=-1)\n",
    "    pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Create a matrix where misclassifications (farthest ones) are heavily penalized\n",
    "    inverse_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 0))   # True class 3, predicted class 0\n",
    "    )\n",
    "    bad1_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 2)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "    )\n",
    "    bad2_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 0)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 1))   # True class 3, predicted class 0\n",
    "    )\n",
    "    # Create a matrix where correct predictions for 0-0 and 3-3 are rewarded\n",
    "    extrema_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 0)),  # True class 0, predicted class 0\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 3)),  # True class 1, predicted class 1\n",
    "    )\n",
    "    fair_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 1)),  # True class 2, predicted class 2\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 2))   # True class 3, predicted class 3\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Standard categorical crossentropy loss\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Add an additional penalty for farthest misclassifications\n",
    "    pen_inverse = 2.0  # You can adjust this penalty factor\n",
    "    pen_inverse_loss = K.cast(inverse_misclass, tf.float32) * pen_inverse\n",
    "\n",
    "    pen_bad1 = 1.0  # You can adjust this penalty factor\n",
    "    pen_bad1_loss = K.cast(bad1_misclass, tf.float32) * pen_bad1\n",
    "\n",
    "    pen_bad2 = 1.0  # You can adjust this penalty factor\n",
    "    pen_bad2_loss = K.cast(bad2_misclass, tf.float32) * pen_bad2\n",
    "\n",
    "    # Add a reward for correct 0-0 and 3-3 predictions (negative penalty)\n",
    "    rew_extrema = 2.0  # You can adjust this reward factor\n",
    "    rew_extrema_gain = K.cast(extrema_correct, tf.float32) * rew_extrema\n",
    "\n",
    "    rew_fair = 1.0  # You can adjust this reward factor\n",
    "    rew_fair_gain = K.cast(fair_correct, tf.float32) * rew_fair\n",
    "\n",
    "    # Return the combined loss: base loss + penalties - rewards\n",
    "    return base_loss + (pen_inverse_loss + pen_bad1_loss + pen_bad2_loss) - (rew_extrema_gain + rew_fair_gain)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([#currently 17 total features\n",
    "        tf.keras.layers.Dense(1024),#kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1024),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.32),\n",
    "        tf.keras.layers.Dense(512),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.32),\n",
    "        tf.keras.layers.Dense(512),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        #tf.keras.layers.Dropout(0.15),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    #AUC=tf.keras.metrics.AUC(curve='PR')\n",
    "    met = ['Accuracy','Precision','Recall']\n",
    "    model.compile(optimizer=opt3, loss=custom_loss, metrics=met)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    #loaded_model = tf.keras.models.load_model('tupleTrain.keras', custom_objects={'custom_loss':custom_loss})\n",
    "    loaded_model = tf.keras.models.load_model('multi_oh5.keras')\n",
    "    met = ['Accuracy','Precision','Recall','F1Score']\n",
    "    loaded_model.compile(optimizer=opt3, loss=custom_loss, metrics=met)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "early_stopping = []#EarlyStopping(monitor='Precision', patience=100, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = build_model()\n",
    "loaded_model = load_model()\n",
    "history = loaded_model.fit(X_train, y_train, epochs=epochs, validation_split=0.2,\\\n",
    "                    shuffle=True, verbose=1, validation_data=(X_test, y_test),\\\n",
    "                    class_weight=cw, callbacks=early_stopping)\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "#_, acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# LOSS\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "plt.plot(epochs, history.history['Accuracy'], 'y', label='Training acc')\n",
    "plt.plot(epochs, history.history['val_Accuracy'], 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# AUC\n",
    "plt.plot(epochs, history.history['AUC'], 'y', label='Training AUC')\n",
    "plt.plot(epochs, history.history['val_AUC'], 'r', label='Validation AUC')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# PRECISION\n",
    "plt.plot(epochs, history.history['Precision'], 'y', label='Training Precision')\n",
    "plt.plot(epochs, history.history['val_Precision'], 'r', label='Validation Precision')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# RECALL\n",
    "plt.plot(epochs, history.history['Recall'], 'y', label='Training Recall')\n",
    "plt.plot(epochs, history.history['val_Recall'], 'r', label='Validation Recall')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# TPR\n",
    "'''\n",
    "TPR = history.history['TruePositives']/(history.history['TruePositives']+history.history['TrueNegatives'])\n",
    "val_TPR = history.history['val_TruePositives']/(history.history['val_TruePositives']+history.history['val_TrueNegatives'])\n",
    "plt.plot(epochs, TPR, 'y', label='Training TPR')\n",
    "plt.plot(epochs, val_TPR, 'r', label='Validation TPR')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('TP Rate')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "#predicting the test set results\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert one-hot to class indices if needed\n",
    "y_pred = np.argmax(loaded_model.predict(X_test), axis=1)  # Predictions to class indices\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(4), yticklabels=range(4))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for 4-Class Classification')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "#model.save('epoch15k.keras')\n",
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "loaded_model.save('multi_oh3453453453.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
