{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('multi.csv')\n",
    "\n",
    "#testing random feature drops\n",
    "#data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='CLASS')\n",
    "\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "#filtering before splitting could be useful if ABSOLUTELY mostly comprised of 'in'\n",
    "data = data.drop(data[data['ToD'] > 955].index)\n",
    "data = data.drop(data[data['ToD'] < 480].index)\n",
    "\n",
    "up_Rows = data.drop(data[data['CLASS'] != '_4up'].index)\n",
    "upRRows = data.drop(data[data['CLASS'] != '_3upR'].index)\n",
    "dnRRows = data.drop(data[data['CLASS'] != '_2dnR'].index)\n",
    "dn_Rows = data.drop(data[data['CLASS'] != '_1dn'].index)\n",
    "\n",
    "smallestClass = min(up_Rows.index.size, upRRows.index.size, dnRRows.index.size, dn_Rows.index.size)\n",
    "\n",
    "print('Smallest Class Size:',smallestClass,'\\n')\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "up_Rows = up_Rows.iloc[0:smallestClass]\n",
    "upRRows = upRRows.iloc[0:smallestClass]\n",
    "dnRRows = dnRRows.iloc[0:smallestClass]\n",
    "dn_Rows = dn_Rows.iloc[0:smallestClass]\n",
    "\n",
    "optData = pd.concat([up_Rows, upRRows, dnRRows, dn_Rows],axis=0)\n",
    "\n",
    "print(\"OCCURANCES IN OPT DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(optData.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "#percIn = data.size/(inRows.size*2)\n",
    "#percNin = data.size/(NinRows.size*2)\n",
    "weight_for_0 = .25\n",
    "weight_for_1 = .25\n",
    "weight_for_2 = .25\n",
    "weight_for_3 = .25\n",
    "cw = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3}\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = optData.iloc[:, :-1].values\n",
    "y = optData.iloc[:, -1].values\n",
    "\n",
    "#SMOTE OVERSAMPLING________________\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X, y = smote.fit_resample(X,y)\n",
    "#print('\\n[PRE-SPLIT] Resampled Data size:',X.size,'--',y.size)\n",
    "\n",
    "#__________________________________\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) #stratify=y,\n",
    "\n",
    "# one-hot encode ? \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#y_train = to_categorical(y_train, num_classes=4)\n",
    "#y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "#RESAMPLED DATA- POST SPLIT---------------------------------------------------------\n",
    "\n",
    "#smote = SMOTE()\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train,y_train)\n",
    "#print('\\nResampled Data size:',X_resampled.size)\n",
    "\n",
    "#BUILD THE NEURAL NETWORK MODEL-------------------------------------------------------\n",
    "\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    #good rough val to start, .25, good val to end at .0015.\n",
    "    #5k epoch should be: .25, 8565, .9995, true\n",
    "    0.01,\n",
    "    decay_steps=1202,\n",
    "    decay_rate=0.998,\n",
    "    staircase=True)\n",
    "\n",
    "opt1 = SGD(learning_rate=0.0001)\n",
    "opt2  = tf.keras.optimizers.Adam(clipnorm=0.7)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "from keras.saving import get_custom_objects\n",
    "from keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "get_custom_objects().clear()\n",
    "@register_keras_serializable(name=\"custom_loss\")\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    true_class = K.argmax(y_true, axis=-1)\n",
    "    pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Create a matrix where misclassifications (farthest ones) are heavily penalized\n",
    "    inverse_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 0))   # True class 3, predicted class 0\n",
    "    )\n",
    "    bad1_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 2)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 3)),  # True class 0, predicted class 3\n",
    "    )\n",
    "    bad2_misclass = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 0)),  # True class 0, predicted class 3\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 1))   # True class 3, predicted class 0\n",
    "    )\n",
    "    # Create a matrix where correct predictions for 0-0 and 3-3 are rewarded\n",
    "    extrema_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 0), K.equal(pred_class, 0)),  # True class 0, predicted class 0\n",
    "        tf.logical_and(K.equal(true_class, 3), K.equal(pred_class, 3)),  # True class 1, predicted class 1\n",
    "    )\n",
    "    fair_correct = tf.logical_or(\n",
    "        tf.logical_and(K.equal(true_class, 1), K.equal(pred_class, 1)),  # True class 2, predicted class 2\n",
    "        tf.logical_and(K.equal(true_class, 2), K.equal(pred_class, 2))   # True class 3, predicted class 3\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Standard categorical crossentropy loss\n",
    "    base_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Add an additional penalty for farthest misclassifications\n",
    "    pen_inverse = 1.0  # You can adjust this penalty factor\n",
    "    pen_inverse_loss = K.cast(inverse_misclass, tf.float32) * pen_inverse\n",
    "\n",
    "    pen_bad1 = 0.0  # You can adjust this penalty factor\n",
    "    pen_bad1_loss = K.cast(bad1_misclass, tf.float32) * pen_bad1\n",
    "\n",
    "    pen_bad2 = 0.0  # You can adjust this penalty factor\n",
    "    pen_bad2_loss = K.cast(bad2_misclass, tf.float32) * pen_bad2\n",
    "\n",
    "    # Add a reward for correct 0-0 and 3-3 predictions (negative penalty)\n",
    "    rew_extrema = 1.0  # You can adjust this reward factor\n",
    "    rew_extrema_gain = K.cast(extrema_correct, tf.float32) * rew_extrema\n",
    "\n",
    "    rew_fair = 0.5  # You can adjust this reward factor\n",
    "    rew_fair_gain = K.cast(fair_correct, tf.float32) * rew_fair\n",
    "\n",
    "    # Return the combined loss: base loss + penalties - rewards\n",
    "    return base_loss + (pen_inverse_loss + pen_bad1_loss + pen_bad2_loss) - (rew_extrema_gain + rew_fair_gain)\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([#currently 17 total features\n",
    "        tf.keras.layers.Dense(1024),#kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(1024),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(512),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(512),#, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('leaky_relu'),\n",
    "        tf.keras.layers.Dropout(0.15),\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    #AUC=tf.keras.metrics.AUC(curve='PR')\n",
    "    met = ['Accuracy','Precision','Recall','F1Score']\n",
    "    model.compile(optimizer=opt3, loss=custom_loss, metrics=met)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    #loaded_model = tf.keras.models.load_model('tupleTrain.keras', custom_objects={'custom_loss':custom_loss})\n",
    "    loaded_model = tf.keras.models.load_model('multi_openhours.keras')\n",
    "    met = ['Accuracy','Precision','Recall','F1Score']\n",
    "    loaded_model.compile(optimizer=opt3, loss=custom_loss, metrics=met)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "early_stopping = []#EarlyStopping(monitor='Precision', patience=100, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = build_model()\n",
    "loaded_model = load_model()\n",
    "history = loaded_model.fit(X_train, y_train, epochs=epochs, validation_split=0.2,\\\n",
    "                    shuffle=True, verbose=1, validation_data=(X_test, y_test),\\\n",
    "                    class_weight=cw, callbacks=early_stopping)\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "#_, acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# LOSS\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "plt.plot(epochs, history.history['Accuracy'], 'y', label='Training acc')\n",
    "plt.plot(epochs, history.history['val_Accuracy'], 'r', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# AUC\n",
    "plt.plot(epochs, history.history['AUC'], 'y', label='Training AUC')\n",
    "plt.plot(epochs, history.history['val_AUC'], 'r', label='Validation AUC')\n",
    "plt.title('Training and Validation AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "'''\n",
    "# PRECISION\n",
    "plt.plot(epochs, history.history['Precision'], 'y', label='Training Precision')\n",
    "plt.plot(epochs, history.history['val_Precision'], 'r', label='Validation Precision')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# RECALL\n",
    "plt.plot(epochs, history.history['Recall'], 'y', label='Training Recall')\n",
    "plt.plot(epochs, history.history['val_Recall'], 'r', label='Validation Recall')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# TPR\n",
    "'''\n",
    "TPR = history.history['TruePositives']/(history.history['TruePositives']+history.history['TrueNegatives'])\n",
    "val_TPR = history.history['val_TruePositives']/(history.history['val_TruePositives']+history.history['val_TrueNegatives'])\n",
    "plt.plot(epochs, TPR, 'y', label='Training TPR')\n",
    "plt.plot(epochs, val_TPR, 'r', label='Validation TPR')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('TP Rate')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "#predicting the test set results\n",
    "y_true = np.argmax(y_test, axis=1)  # Convert one-hot to class indices if needed\n",
    "y_pred = np.argmax(loaded_model.predict(X_test), axis=1)  # Predictions to class indices\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(4), yticklabels=range(4))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for 4-Class Classification')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "#model.save('epoch15k.keras')\n",
    "# Load the model\n",
    "#loaded_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('fgdfgdfgdfg.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
