{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('catted_1.csv')\n",
    "\n",
    "#      'Dr1' 'Dr3' 'Mr1' 'Mr3' \n",
    "testFor = 'Dr1'\n",
    "timeSteps = 6\n",
    "tType = testFor[0]\n",
    "#testing random feature drops\n",
    "#TREND\n",
    "\n",
    "data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='vel5')\n",
    "#data = data.drop(columns='vel10')\n",
    "#data = data.drop(columns='vel15')\n",
    "#data = data.drop(columns='vel30')\n",
    "#data = data.drop(columns='vel60')\n",
    "#data = data.drop(columns='acc5')\n",
    "#data = data.drop(columns='acc10')\n",
    "\n",
    "\n",
    "#data = data.drop(columns='stoch12')\n",
    "\n",
    "#data = data.drop(columns='RSIhl_diff')\n",
    "#data = data.drop(columns='RSIhl_diffROC')\n",
    "\n",
    "\n",
    "#PARTICIPATION\n",
    "\n",
    "\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='vol10')\n",
    "#data = data.drop(columns='vol15')\n",
    "#data = data.drop(columns='vol30')\n",
    "\n",
    "#data = data.drop(columns='ToD')\n",
    "\n",
    "\n",
    "#data = data.drop(columns='acc15')\n",
    "#data = data.drop(columns='stochDiff6012')\n",
    "#data = data.drop(columns='DoW')\n",
    "'''\n",
    "data = data.drop(columns='acc30')\n",
    "data = data.drop(columns='acc60')\n",
    "data = data.drop(columns='vol60')\n",
    "data = data.drop(columns='volD10')\n",
    "data = data.drop(columns='volD15')\n",
    "data = data.drop(columns='volD30')\n",
    "data = data.drop(columns='volD60')\n",
    "data = data.drop(columns='vpm5')\n",
    "data = data.drop(columns='vpm10')\n",
    "data = data.drop(columns='vpm15')\n",
    "data = data.drop(columns='vpm30')\n",
    "data = data.drop(columns='vpm60')\n",
    "'''\n",
    "\n",
    "#CALENDAR\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "#SOLUTION------------------------------\n",
    "data = data.drop(columns=['Mc1','Mc3','Dc1','Dc3'])\n",
    "match testFor:\n",
    "    case 'Dr1':\n",
    "        data = data.drop(columns='Dr3')\n",
    "        data = data.drop(columns='Mr1')\n",
    "        data = data.drop(columns='Mr3')\n",
    "    case 'Dr3':\n",
    "        data = data.drop(columns='Dr1')\n",
    "        data = data.drop(columns='Mr1')\n",
    "        data = data.drop(columns='Mr3')\n",
    "    case 'Mr1':\n",
    "        data = data.drop(columns='Dr1')\n",
    "        data = data.drop(columns='Dr3')\n",
    "        data = data.drop(columns='Mr3')\n",
    "    case 'Mr3':\n",
    "        data = data.drop(columns='Dr1')\n",
    "        data = data.drop(columns='Dr3')\n",
    "        data = data.drop(columns='Mr1')\n",
    "\n",
    "data = data.drop(columns='YM_diff')\n",
    "data = data.drop(columns='NQ_diff')\n",
    "data = data.drop(columns='volNQdiff')\n",
    "data = data.drop(columns='volYMdiff')\n",
    "data = data.drop(columns='FT.1')\n",
    "data = data.drop(columns='FT.2')\n",
    "\n",
    "\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "#print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "#unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "#print(dict(zip(unique,counts)))\n",
    "\n",
    "#filtering before splitting could be useful if ABSOLUTELY mostly comprised of 'in'\n",
    "#MARKET HOURS!\n",
    "#data = data.drop(data[data['ToD'] > 950].index)\n",
    "#data = data.drop(data[data['ToD'] < 560].index)\n",
    "#OTHER MODIFICATIONS\n",
    "#data = data.drop(data[data['feature'] condition].index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "#setting data for LSTM\n",
    "def reformat_to_lstm(X, time_steps=timeSteps):\n",
    "    X_lstm, y_lstm = [], []\n",
    "    \n",
    "    for i in range(time_steps, len(X)):\n",
    "        # Collect previous time_steps rows for X\n",
    "        X_lstm.append(X[i-time_steps:i])  \n",
    "        # The corresponding y value for the last time step in the sequence\n",
    "    \n",
    "    X_lstm = np.array(X_lstm)\n",
    "    \n",
    "    return X_lstm\n",
    "\n",
    "#timeSteps = 5\n",
    "#X = reformat_lstm(X, timeSteps)\n",
    "#y = y[timeSteps+4:]\n",
    "\n",
    "#print(X[0])\n",
    "# Standardize the features\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = RobustScaler()\n",
    "scaler3 = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler1.fit(X)\n",
    "X = scaler1.transform(X)\n",
    "#scaler3.fit(X)\n",
    "#X = scaler3.transform(X)\n",
    "\n",
    "X = reformat_to_lstm(X, timeSteps)\n",
    "y = y[timeSteps:]\n",
    "y = np.array(y)\n",
    "\n",
    "#print(X[0])\n",
    "#print(y[0])\n",
    "\n",
    "print('X shape == {}.'.format(X.shape))\n",
    "print('y shape == {}.'.format(y.shape))\n",
    "\n",
    "mos = X[:, timeSteps-1, len(Xfeatures)-1].mean()\n",
    "print(mos)\n",
    "mos = X[:, timeSteps-1, len(Xfeatures)-1].min()\n",
    "print(mos)\n",
    "mos = X[:, timeSteps-1, len(Xfeatures)-1].max()\n",
    "print(mos)\n",
    "print(len(X))\n",
    "\n",
    "\n",
    "def remove_zero_mo_samples(X, y):\n",
    "    # Get the 'MO' column (index 34 for 0-based indexing) for all time steps and samples\n",
    "    non_zero_indices = (X[:, timeSteps-1, len(Xfeatures)-1] >= 0)\n",
    "    # Filter X and y using these indices\n",
    "    X_filtered = X[non_zero_indices]\n",
    "    y_filtered = y[non_zero_indices]\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "X, y = remove_zero_mo_samples(X, y)\n",
    "\n",
    "mos = X[:, timeSteps-1, len(Xfeatures)-1].mean()\n",
    "print(mos)\n",
    "print(len(X))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#LEARNING RATES____________________________________________________________________________________________\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    #good rough val to start, .25, good val to end at .0015.\n",
    "    #5k epoch should be: .25, 8565, .9995, true\n",
    "    0.01,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.997,\n",
    "    staircase=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.75, \n",
    "    patience=10, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "opt1 = SGD(learning_rate=0.01)\n",
    "opt2  = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "opt3 = SGD(learning_rate=lr_schedule)\n",
    "opt4 = SGD(learning_rate=0.005, momentum=0.9)\n",
    "\n",
    "#BUILD AND LOAD MODEL__________________________________________________________________________________________\n",
    "\n",
    "\n",
    "#print(X_train.shape[0]/time_steps)\n",
    "#X_train = np.reshape(X_train,((X_train.shape[0]//time_steps), time_steps, 35))  # Reshape to (batch_size, 5 time steps, 35 features)\n",
    "#y_train = y.reshape(1,-1)\n",
    "\n",
    "def build_LSTM_model():\n",
    "    #time_steps=5\n",
    "    n_features=len(Xfeatures)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],X_train.shape[2])),\n",
    "        #tf.keras.layers.Activation('tanh'),\n",
    "        #tf.keras.layers.LSTM(256, activation='tanh', recurrent_dropout=0.2, return_sequences=True),\n",
    "        #tf.keras.layers.LSTM(128, activation='tanh', recurrent_dropout=0.2),\n",
    "        tf.keras.layers.LSTM(512, activation='tanh', recurrent_dropout=0.2),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Activation('leaky_relu'),\n",
    "        #tf.keras.layers.Dense(2048),#,kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        #tf.keras.layers.Activation('leaky_relu'),\n",
    "        #tf.keras.layers.Dropout(0.2),\n",
    "        #tf.keras.layers.Dense(4, activation='linear'),\n",
    "        #tf.keras.layers.Dropout(0.40),\n",
    "        #tf.keras.layers.Dense(64, activation='linear'),\n",
    "        tf.keras.layers.Dropout(0.20),\n",
    "        tf.keras.layers.Dense(512, activation='linear'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        #tf.keras.layers.Dense(128, activation='leaky_relu'),#,kernel_initializer='he_normal',kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=opt4,\n",
    "                  loss='mse'\n",
    "                  ,metrics=['R2Score','root_mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    #loaded_model = tf.keras.models.load_model('tupleTrain.keras', custom_objects={'custom_loss':custom_loss})\n",
    "    loaded_model = tf.keras.models.load_model('LSTM_Dr3_6step_10.keras')\n",
    "    loaded_model.compile(optimizer=opt4,\n",
    "                         loss='mse'\n",
    "                         , metrics=['R2Score','root_mean_squared_error'])\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "#TRAIN THE MODEL WITH CUSTOMIZABLE EPOCHS-------------------------------------------------------\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_R2Score', patience=35, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = build_LSTM_model()\n",
    "loaded_model = load_model()\n",
    "history = model.fit(X_train, y_train, epochs=epochs,\\\n",
    "                    shuffle=False, verbose=1, validation_data=(X_test, y_test),\\\n",
    "                    batch_size=12,callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "#EVALUATE THE MODEL AND VISUALIZE RESULTS-------------------------------------------------------\n",
    "\n",
    "#_, acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# LOSS\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, history.history['loss'], 'y', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "# ACCURACY\n",
    "\n",
    "plt.plot(epochs, history.history['R2Score'], 'y', label='Training R2')\n",
    "plt.plot(epochs, history.history['val_R2Score'], 'r', label='Validation R2')\n",
    "plt.title('Training and Validation R2Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2Score')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#predicting the test set results\n",
    "y_pred = model.predict(X_test) \n",
    "#y_pred = y_pred > 0.5 # Predictions to class indices\n",
    "'''\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d',cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Direction Classification')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "match tType:\n",
    "    case 'D':\n",
    "        plt.xlim(-5,5)\n",
    "        plt.ylim(-5,5)\n",
    "    case _:\n",
    "        plt.xlim(0,5)\n",
    "        plt.ylim(0,5)\n",
    "plt.ylabel('y_test')\n",
    "plt.show()\n",
    "\n",
    "#SCATTERPLOT #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  \n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.grid()\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "match tType:\n",
    "    case 'D':\n",
    "        plt.xlim(-20,20)\n",
    "        plt.ylim(-20,20)\n",
    "    case _:\n",
    "        plt.xlim(0,5)\n",
    "        plt.ylim(0,5)\n",
    "plt.ylabel('y_test')\n",
    "plt.show()\n",
    "#DIRECTIONAL ACCURACY #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  \n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "tp5, fp5, tn5, fn5 = 0, 0, 0, 0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i]>0):\n",
    "        if(y_test[i]>0):\n",
    "            tp+=1\n",
    "        if(y_test[i]<0):\n",
    "            fp+=1\n",
    "        if(y_pred[i]>=5):\n",
    "            if(y_test[i]>0):\n",
    "                tp5+=1\n",
    "            if(y_test[i]<0):\n",
    "                fp5+=1\n",
    "    if(y_pred[i]<0):\n",
    "        if(y_test[i]<0):\n",
    "            tn+=1\n",
    "        if(y_test[i]>0):\n",
    "            fn+=1\n",
    "        if(y_pred[i]<=-5):\n",
    "            if(y_test[i]<0):\n",
    "                tn5+=1\n",
    "            if(y_test[i]>0):\n",
    "                fn5+=1\n",
    "directionalAccuracy = ((tp+tn)/(tp+fp+tn+fn))*10000//1/100\n",
    "print('Directional Accuracy:\\t\\t',directionalAccuracy)\n",
    "directionalAccuracy5guess = ((tp5+tn5)/(tp5+fp5+tn5+fn5))*10000//1/100\n",
    "print('Directional Accuracy >(+/-)5:\\t',directionalAccuracy5guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('LSTM_Dr3_24step_2.keras')\n",
    "#loaded_model.save('LSTM_Dr3_24step_2.keras')\n",
    "loaded_model.save('LSTM_Dr3_6step_8_2.keras')\n",
    "#loaded_model.save('LSTM_Dr3_6step_8.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directional Accuracy: 51.08\n"
     ]
    }
   ],
   "source": [
    "#SCATTERPLOT #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  #SCATTERPLOT  \n",
    "plt.scatter(y_pred, y_test, s=1)\n",
    "plt.grid()\n",
    "plt.axis('tight')\n",
    "plt.title('Testing Outputs')\n",
    "plt.xlabel('y_pred')\n",
    "match tType:\n",
    "    case 'D':\n",
    "        plt.xlim(-20,20)\n",
    "        plt.ylim(-20,20)\n",
    "    case _:\n",
    "        plt.xlim(0,5)\n",
    "        plt.ylim(0,5)\n",
    "plt.ylabel('y_test')\n",
    "plt.show()\n",
    "#DIRECTIONAL ACCURACY #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  #DIRECTIONAL ACCURACY  \n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "tp5, fp5, tn5, fn5 = 0, 0, 0, 0\n",
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i]>0):\n",
    "        if(y_test[i]>0):\n",
    "            tp+=1\n",
    "        if(y_test[i]<0):\n",
    "            fp+=1\n",
    "        if(y_pred[i]>=5):\n",
    "            if(y_test[i]>0):\n",
    "                tp5+=1\n",
    "            if(y_test[i]<0):\n",
    "                fp5+=1\n",
    "    if(y_pred[i]<0):\n",
    "        if(y_test[i]<0):\n",
    "            tn+=1\n",
    "        if(y_test[i]>0):\n",
    "            fn+=1\n",
    "        if(y_pred[i]<=-5):\n",
    "            if(y_test[i]<0):\n",
    "                tn5+=1\n",
    "            if(y_test[i]>0):\n",
    "                fn5+=1\n",
    "directionalAccuracy = ((tp+tn)/(tp+fp+tn+fn))*10000//1/100\n",
    "directionalAccuracy5guess = ((tp5+tn5)/(tp5+fp5+tn5+fn5))*10000//1/100\n",
    "print('Directional Accuracy:\\t\\t',directionalAccuracy)\n",
    "print('Directional Accuracy >(+/-)5:\\t',directionalAccuracy5guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3019, 48, 31) \n",
      "y shape: (3019,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=LSTM_testtest.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m y_vals \u001b[38;5;241m=\u001b[39m y_test\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX shape:\u001b[39m\u001b[38;5;124m'\u001b[39m,X_vals\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124my shape:\u001b[39m\u001b[38;5;124m'\u001b[39m,y_vals\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 56\u001b[0m scores \u001b[38;5;241m=\u001b[39m walk_forward_validation(X_test, y_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM_testtest.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m avgScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m scores:\n",
      "Cell \u001b[1;32mIn[78], line 33\u001b[0m, in \u001b[0;36mwalk_forward_validation\u001b[1;34m(X, y, model, n_splits, test_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[:train_end], y[train_end:train_end \u001b[38;5;241m+\u001b[39m test_set_size]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Ensure that your model is recompiled and retrained in each fold\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model_copy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[0;32m     34\u001b[0m model_copy\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Assuming MSE for regression\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Fit the model on the current training set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\logan\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:193\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    190\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=LSTM_testtest.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def walk_forward_validation(X, y, model, n_splits=20, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for an LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): 3D array of features with shape (n_samples, time_steps, n_features)\n",
    "    y (np.ndarray): 1D array of labels with shape (n_samples,)\n",
    "    model (tf.keras.Model): Compiled LSTM model\n",
    "    n_splits (int): Number of walk-forward splits\n",
    "    test_size (float): Proportion of the data to use as the test set in each split\n",
    "\n",
    "    Returns:\n",
    "    list: MSE scores for each split\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    test_set_size = int(test_size * n_samples)\n",
    "    \n",
    "    mse_scores = []\n",
    "\n",
    "    # Split the data into n_splits segments\n",
    "    for i in range(n_splits):\n",
    "        # Define the index range for the training set (everything before the test set)\n",
    "        train_end = int((i + 1) * (n_samples - test_set_size) / n_splits)\n",
    "\n",
    "        # Define the test set\n",
    "        X_train, X_test = X[:train_end], X[train_end:train_end + test_set_size]\n",
    "        y_train, y_test = y[:train_end], y[train_end:train_end + test_set_size]\n",
    "\n",
    "        # Ensure that your model is recompiled and retrained in each fold\n",
    "        model_copy = tf.keras.models.load_model(model)\n",
    "        model_copy.compile(optimizer='adam', loss='mse')  # Assuming MSE for regression\n",
    "\n",
    "        # Fit the model on the current training set\n",
    "        model_copy.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model_copy.predict(X_test)\n",
    "\n",
    "        # Calculate the mean squared error for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "        print(f'Fold {i+1}/{n_splits}, MSE: {mse}')\n",
    "\n",
    "    return mse_scores\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have already defined `X`, `y`, and compiled your LSTM model\n",
    "# mse_scores = walk_forward_validation(X, y, lstm_model, n_splits=5, test_size=0.2)\n",
    "X_vals = X_test\n",
    "y_vals = y_test\n",
    "print('X shape:',X_vals.shape,'\\ny shape:',y_vals.shape)\n",
    "scores = walk_forward_validation(X_test, y_test, 'LSTM_testtest.keras')\n",
    "avgScore = 0\n",
    "for score in scores:\n",
    "    avgScore+=score\n",
    "avgScore/=len(scores)\n",
    "\n",
    "print('Average MSE:',avgScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vel5</th>\n",
       "      <th>vel10</th>\n",
       "      <th>vel15</th>\n",
       "      <th>vel30</th>\n",
       "      <th>vel60</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc10</th>\n",
       "      <th>acc15</th>\n",
       "      <th>acc30</th>\n",
       "      <th>acc60</th>\n",
       "      <th>...</th>\n",
       "      <th>volD60</th>\n",
       "      <th>vpm5</th>\n",
       "      <th>vpm10</th>\n",
       "      <th>vpm15</th>\n",
       "      <th>vpm30</th>\n",
       "      <th>vpm60</th>\n",
       "      <th>ToD</th>\n",
       "      <th>DoW</th>\n",
       "      <th>MO</th>\n",
       "      <th>Dr1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vel5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696269</td>\n",
       "      <td>0.566629</td>\n",
       "      <td>0.404792</td>\n",
       "      <td>0.292516</td>\n",
       "      <td>0.717757</td>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.396644</td>\n",
       "      <td>0.279971</td>\n",
       "      <td>0.209327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040104</td>\n",
       "      <td>0.399169</td>\n",
       "      <td>0.295397</td>\n",
       "      <td>0.252108</td>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>-0.006288</td>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.030087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel10</th>\n",
       "      <td>0.696269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.806148</td>\n",
       "      <td>0.574039</td>\n",
       "      <td>0.409941</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.715761</td>\n",
       "      <td>0.566122</td>\n",
       "      <td>0.401907</td>\n",
       "      <td>0.289201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056399</td>\n",
       "      <td>0.261988</td>\n",
       "      <td>0.431247</td>\n",
       "      <td>0.351803</td>\n",
       "      <td>0.258081</td>\n",
       "      <td>0.170056</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>-0.030580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel15</th>\n",
       "      <td>0.566629</td>\n",
       "      <td>0.806148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702885</td>\n",
       "      <td>0.499764</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.711311</td>\n",
       "      <td>0.494297</td>\n",
       "      <td>0.351262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063678</td>\n",
       "      <td>0.211814</td>\n",
       "      <td>0.339981</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.313587</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>-0.010801</td>\n",
       "      <td>-0.006752</td>\n",
       "      <td>-0.021263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel30</th>\n",
       "      <td>0.404792</td>\n",
       "      <td>0.574039</td>\n",
       "      <td>0.702885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706232</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.707956</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>0.146675</td>\n",
       "      <td>0.227690</td>\n",
       "      <td>0.295953</td>\n",
       "      <td>0.459010</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>-0.015124</td>\n",
       "      <td>-0.005017</td>\n",
       "      <td>-0.009727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vel60</th>\n",
       "      <td>0.292516</td>\n",
       "      <td>0.409941</td>\n",
       "      <td>0.499764</td>\n",
       "      <td>0.706232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040721</td>\n",
       "      <td>0.105265</td>\n",
       "      <td>0.158133</td>\n",
       "      <td>0.204836</td>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.452845</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>-0.019204</td>\n",
       "      <td>-0.009746</td>\n",
       "      <td>-0.002594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc5</th>\n",
       "      <td>0.717757</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>0.301999</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.012246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc10</th>\n",
       "      <td>0.488963</td>\n",
       "      <td>0.715761</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395404</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.320990</td>\n",
       "      <td>0.125447</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.004608</td>\n",
       "      <td>-0.026688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc15</th>\n",
       "      <td>0.396644</td>\n",
       "      <td>0.566122</td>\n",
       "      <td>0.711311</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.395404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004650</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021239</td>\n",
       "      <td>0.152865</td>\n",
       "      <td>0.253008</td>\n",
       "      <td>0.335755</td>\n",
       "      <td>-0.012689</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>-0.020336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc30</th>\n",
       "      <td>0.279971</td>\n",
       "      <td>0.401907</td>\n",
       "      <td>0.494297</td>\n",
       "      <td>0.707956</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>-0.004650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056943</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.163888</td>\n",
       "      <td>0.213747</td>\n",
       "      <td>0.339243</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>-0.011194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc60</th>\n",
       "      <td>0.209327</td>\n",
       "      <td>0.289201</td>\n",
       "      <td>0.351262</td>\n",
       "      <td>0.498343</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>0.011057</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>0.072164</td>\n",
       "      <td>0.108224</td>\n",
       "      <td>0.140920</td>\n",
       "      <td>0.214447</td>\n",
       "      <td>0.323144</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch12</th>\n",
       "      <td>0.377976</td>\n",
       "      <td>0.478170</td>\n",
       "      <td>0.541923</td>\n",
       "      <td>0.635179</td>\n",
       "      <td>0.636602</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>0.099823</td>\n",
       "      <td>0.134281</td>\n",
       "      <td>0.262216</td>\n",
       "      <td>0.451728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065496</td>\n",
       "      <td>0.302808</td>\n",
       "      <td>0.408260</td>\n",
       "      <td>0.475668</td>\n",
       "      <td>0.566148</td>\n",
       "      <td>0.538376</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>-0.014696</td>\n",
       "      <td>-0.010189</td>\n",
       "      <td>-0.007544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stochDiff6012</th>\n",
       "      <td>0.400457</td>\n",
       "      <td>0.507834</td>\n",
       "      <td>0.565856</td>\n",
       "      <td>0.607925</td>\n",
       "      <td>0.442418</td>\n",
       "      <td>0.065284</td>\n",
       "      <td>0.129730</td>\n",
       "      <td>0.194854</td>\n",
       "      <td>0.417469</td>\n",
       "      <td>0.403139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091359</td>\n",
       "      <td>0.251414</td>\n",
       "      <td>0.350791</td>\n",
       "      <td>0.411406</td>\n",
       "      <td>0.479059</td>\n",
       "      <td>0.379592</td>\n",
       "      <td>0.012056</td>\n",
       "      <td>-0.008342</td>\n",
       "      <td>-0.010490</td>\n",
       "      <td>-0.009730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSIhl_diff</th>\n",
       "      <td>0.029998</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>-0.003205</td>\n",
       "      <td>-0.030880</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>0.019105</td>\n",
       "      <td>0.023221</td>\n",
       "      <td>0.026310</td>\n",
       "      <td>-0.011032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592633</td>\n",
       "      <td>0.023018</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.025213</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.180068</td>\n",
       "      <td>-0.020364</td>\n",
       "      <td>-0.138535</td>\n",
       "      <td>-0.006143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSIhl_diffROC</th>\n",
       "      <td>0.035536</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>-0.002526</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>0.047545</td>\n",
       "      <td>0.004153</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>-0.002836</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268701</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>-0.001552</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>-0.004357</td>\n",
       "      <td>-0.002434</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>-0.010908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol</th>\n",
       "      <td>-0.034446</td>\n",
       "      <td>-0.061487</td>\n",
       "      <td>-0.072707</td>\n",
       "      <td>-0.096548</td>\n",
       "      <td>-0.092960</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>-0.008032</td>\n",
       "      <td>-0.006791</td>\n",
       "      <td>-0.043577</td>\n",
       "      <td>-0.028476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402226</td>\n",
       "      <td>-0.020267</td>\n",
       "      <td>-0.025863</td>\n",
       "      <td>-0.031096</td>\n",
       "      <td>-0.041812</td>\n",
       "      <td>-0.050274</td>\n",
       "      <td>0.037538</td>\n",
       "      <td>0.037112</td>\n",
       "      <td>0.546729</td>\n",
       "      <td>-0.012484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol10</th>\n",
       "      <td>-0.024375</td>\n",
       "      <td>-0.050011</td>\n",
       "      <td>-0.068859</td>\n",
       "      <td>-0.097354</td>\n",
       "      <td>-0.098282</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.006932</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.039403</td>\n",
       "      <td>-0.031610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294425</td>\n",
       "      <td>-0.019573</td>\n",
       "      <td>-0.025661</td>\n",
       "      <td>-0.031523</td>\n",
       "      <td>-0.043353</td>\n",
       "      <td>-0.052847</td>\n",
       "      <td>0.043069</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.573243</td>\n",
       "      <td>-0.011404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol15</th>\n",
       "      <td>-0.019875</td>\n",
       "      <td>-0.039884</td>\n",
       "      <td>-0.059506</td>\n",
       "      <td>-0.094437</td>\n",
       "      <td>-0.101208</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.009675</td>\n",
       "      <td>-0.032366</td>\n",
       "      <td>-0.032690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>-0.019550</td>\n",
       "      <td>-0.025131</td>\n",
       "      <td>-0.031419</td>\n",
       "      <td>-0.044165</td>\n",
       "      <td>-0.054638</td>\n",
       "      <td>0.048250</td>\n",
       "      <td>0.040585</td>\n",
       "      <td>0.591939</td>\n",
       "      <td>-0.010286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol30</th>\n",
       "      <td>-0.015631</td>\n",
       "      <td>-0.028293</td>\n",
       "      <td>-0.040582</td>\n",
       "      <td>-0.079453</td>\n",
       "      <td>-0.105122</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>-0.007295</td>\n",
       "      <td>-0.033583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>-0.019288</td>\n",
       "      <td>-0.024235</td>\n",
       "      <td>-0.030175</td>\n",
       "      <td>-0.044407</td>\n",
       "      <td>-0.057573</td>\n",
       "      <td>0.062838</td>\n",
       "      <td>0.043465</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>-0.011967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol60</th>\n",
       "      <td>-0.016707</td>\n",
       "      <td>-0.027100</td>\n",
       "      <td>-0.036276</td>\n",
       "      <td>-0.061492</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.019555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050688</td>\n",
       "      <td>-0.021449</td>\n",
       "      <td>-0.026577</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.043558</td>\n",
       "      <td>-0.058607</td>\n",
       "      <td>0.091407</td>\n",
       "      <td>0.046908</td>\n",
       "      <td>0.642290</td>\n",
       "      <td>-0.013805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD10</th>\n",
       "      <td>-0.045696</td>\n",
       "      <td>-0.039452</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.020902</td>\n",
       "      <td>-0.009628</td>\n",
       "      <td>-0.025341</td>\n",
       "      <td>-0.032221</td>\n",
       "      <td>-0.023755</td>\n",
       "      <td>-0.019910</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545014</td>\n",
       "      <td>-0.035195</td>\n",
       "      <td>-0.014472</td>\n",
       "      <td>-0.019168</td>\n",
       "      <td>-0.013615</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>-0.044984</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.018263</td>\n",
       "      <td>0.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD15</th>\n",
       "      <td>-0.046466</td>\n",
       "      <td>-0.054973</td>\n",
       "      <td>-0.045964</td>\n",
       "      <td>-0.031205</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>-0.011361</td>\n",
       "      <td>-0.042100</td>\n",
       "      <td>-0.033771</td>\n",
       "      <td>-0.030971</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701749</td>\n",
       "      <td>-0.036479</td>\n",
       "      <td>-0.028739</td>\n",
       "      <td>-0.026636</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-0.012053</td>\n",
       "      <td>-0.060366</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.019381</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD30</th>\n",
       "      <td>-0.045175</td>\n",
       "      <td>-0.061389</td>\n",
       "      <td>-0.065355</td>\n",
       "      <td>-0.053528</td>\n",
       "      <td>-0.022932</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>-0.025325</td>\n",
       "      <td>-0.038965</td>\n",
       "      <td>-0.052715</td>\n",
       "      <td>-0.017033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894664</td>\n",
       "      <td>-0.034789</td>\n",
       "      <td>-0.036498</td>\n",
       "      <td>-0.042076</td>\n",
       "      <td>-0.034900</td>\n",
       "      <td>-0.024396</td>\n",
       "      <td>-0.088542</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.026703</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volD60</th>\n",
       "      <td>-0.040104</td>\n",
       "      <td>-0.056399</td>\n",
       "      <td>-0.063678</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>-0.040721</td>\n",
       "      <td>-0.001119</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>-0.021239</td>\n",
       "      <td>-0.056943</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034729</td>\n",
       "      <td>-0.040941</td>\n",
       "      <td>-0.049075</td>\n",
       "      <td>-0.054675</td>\n",
       "      <td>-0.046031</td>\n",
       "      <td>-0.136767</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>-0.002607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm5</th>\n",
       "      <td>0.399169</td>\n",
       "      <td>0.261988</td>\n",
       "      <td>0.211814</td>\n",
       "      <td>0.146675</td>\n",
       "      <td>0.105265</td>\n",
       "      <td>0.301999</td>\n",
       "      <td>0.191872</td>\n",
       "      <td>0.152865</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.072164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.599395</td>\n",
       "      <td>0.477815</td>\n",
       "      <td>0.329781</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>-0.019992</td>\n",
       "      <td>-0.005129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm10</th>\n",
       "      <td>0.295397</td>\n",
       "      <td>0.431247</td>\n",
       "      <td>0.339981</td>\n",
       "      <td>0.227690</td>\n",
       "      <td>0.158133</td>\n",
       "      <td>-0.006754</td>\n",
       "      <td>0.320990</td>\n",
       "      <td>0.253008</td>\n",
       "      <td>0.163888</td>\n",
       "      <td>0.108224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040941</td>\n",
       "      <td>0.599395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739195</td>\n",
       "      <td>0.474857</td>\n",
       "      <td>0.292856</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.009220</td>\n",
       "      <td>-0.024274</td>\n",
       "      <td>-0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm15</th>\n",
       "      <td>0.252108</td>\n",
       "      <td>0.351803</td>\n",
       "      <td>0.446810</td>\n",
       "      <td>0.295953</td>\n",
       "      <td>0.204836</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.125447</td>\n",
       "      <td>0.335755</td>\n",
       "      <td>0.213747</td>\n",
       "      <td>0.140920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049075</td>\n",
       "      <td>0.477815</td>\n",
       "      <td>0.739195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>-0.010489</td>\n",
       "      <td>-0.029213</td>\n",
       "      <td>-0.003253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm30</th>\n",
       "      <td>0.187244</td>\n",
       "      <td>0.258081</td>\n",
       "      <td>0.313587</td>\n",
       "      <td>0.459010</td>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.007287</td>\n",
       "      <td>-0.012689</td>\n",
       "      <td>0.339243</td>\n",
       "      <td>0.214447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054675</td>\n",
       "      <td>0.329781</td>\n",
       "      <td>0.474857</td>\n",
       "      <td>0.612409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.598460</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>-0.039529</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vpm60</th>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.170056</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.452845</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>0.323144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046031</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.292856</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.598460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>-0.013850</td>\n",
       "      <td>-0.050378</td>\n",
       "      <td>-0.003207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ToD</th>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136767</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>0.024778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.006287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DoW</th>\n",
       "      <td>-0.006288</td>\n",
       "      <td>-0.008862</td>\n",
       "      <td>-0.010801</td>\n",
       "      <td>-0.015124</td>\n",
       "      <td>-0.019204</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>-0.009220</td>\n",
       "      <td>-0.010489</td>\n",
       "      <td>-0.012419</td>\n",
       "      <td>-0.013850</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>-0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>-0.005597</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>-0.006752</td>\n",
       "      <td>-0.005017</td>\n",
       "      <td>-0.009746</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>-0.004608</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>-0.019992</td>\n",
       "      <td>-0.024274</td>\n",
       "      <td>-0.029213</td>\n",
       "      <td>-0.039529</td>\n",
       "      <td>-0.050378</td>\n",
       "      <td>0.044723</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr1</th>\n",
       "      <td>-0.030087</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.021263</td>\n",
       "      <td>-0.009727</td>\n",
       "      <td>-0.002594</td>\n",
       "      <td>-0.012246</td>\n",
       "      <td>-0.026688</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>-0.011194</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.005129</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>-0.003253</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.003207</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   vel5     vel10     vel15     vel30     vel60      acc5  \\\n",
       "vel5           1.000000  0.696269  0.566629  0.404792  0.292516  0.717757   \n",
       "vel10          0.696269  1.000000  0.806148  0.574039  0.409941 -0.000033   \n",
       "vel15          0.566629  0.806148  1.000000  0.702885  0.499764  0.007407   \n",
       "vel30          0.404792  0.574039  0.702885  1.000000  0.706232  0.007091   \n",
       "vel60          0.292516  0.409941  0.499764  0.706232  1.000000  0.009827   \n",
       "acc5           0.717757 -0.000033  0.007407  0.007091  0.009827  1.000000   \n",
       "acc10          0.488963  0.715761  0.289400  0.011623  0.012296 -0.013115   \n",
       "acc15          0.396644  0.566122  0.711311  0.000013  0.004736  0.003432   \n",
       "acc30          0.279971  0.401907  0.494297  0.707956 -0.000029  0.000202   \n",
       "acc60          0.209327  0.289201  0.351262  0.498343  0.702112  0.011057   \n",
       "stoch12        0.377976  0.478170  0.541923  0.635179  0.636602  0.062749   \n",
       "stochDiff6012  0.400457  0.507834  0.565856  0.607925  0.442418  0.065284   \n",
       "RSIhl_diff     0.029998  0.018920  0.014242 -0.003205 -0.030880  0.023451   \n",
       "RSIhl_diffROC  0.035536  0.002013  0.002452 -0.002526 -0.000746  0.047545   \n",
       "vol           -0.034446 -0.061487 -0.072707 -0.096548 -0.092960  0.011658   \n",
       "vol10         -0.024375 -0.050011 -0.068859 -0.097354 -0.098282  0.014531   \n",
       "vol15         -0.019875 -0.039884 -0.059506 -0.094437 -0.101208  0.010969   \n",
       "vol30         -0.015631 -0.028293 -0.040582 -0.079453 -0.105122  0.005635   \n",
       "vol60         -0.016707 -0.027100 -0.036276 -0.061492 -0.100000  0.002979   \n",
       "volD10        -0.045696 -0.039452 -0.031596 -0.020902 -0.009628 -0.025341   \n",
       "volD15        -0.046466 -0.054973 -0.045964 -0.031205 -0.013122 -0.011361   \n",
       "volD30        -0.045175 -0.061389 -0.065355 -0.053528 -0.022932 -0.003340   \n",
       "volD60        -0.040104 -0.056399 -0.063678 -0.069082 -0.040721 -0.001119   \n",
       "vpm5           0.399169  0.261988  0.211814  0.146675  0.105265  0.301999   \n",
       "vpm10          0.295397  0.431247  0.339981  0.227690  0.158133 -0.006754   \n",
       "vpm15          0.252108  0.351803  0.446810  0.295953  0.204836  0.009999   \n",
       "vpm30          0.187244  0.258081  0.313587  0.459010  0.309917  0.010539   \n",
       "vpm60          0.122011  0.170056  0.208718  0.297900  0.452845  0.005033   \n",
       "ToD            0.007345  0.010114  0.012498  0.017696  0.025193  0.000411   \n",
       "DoW           -0.006288 -0.008862 -0.010801 -0.015124 -0.019204 -0.000164   \n",
       "MO            -0.005597 -0.006926 -0.006752 -0.005017 -0.009746 -0.001095   \n",
       "Dr1           -0.030087 -0.030580 -0.021263 -0.009727 -0.002594 -0.012246   \n",
       "\n",
       "                  acc10     acc15     acc30     acc60  ...    volD60  \\\n",
       "vel5           0.488963  0.396644  0.279971  0.209327  ... -0.040104   \n",
       "vel10          0.715761  0.566122  0.401907  0.289201  ... -0.056399   \n",
       "vel15          0.289400  0.711311  0.494297  0.351262  ... -0.063678   \n",
       "vel30          0.011623  0.000013  0.707956  0.498343  ... -0.069082   \n",
       "vel60          0.012296  0.004736 -0.000029  0.702112  ... -0.040721   \n",
       "acc5          -0.013115  0.003432  0.000202  0.011057  ... -0.001119   \n",
       "acc10          1.000000  0.395404  0.004165  0.011784  ... -0.014793   \n",
       "acc15          0.395404  1.000000 -0.004650  0.001396  ... -0.021239   \n",
       "acc30          0.004165 -0.004650  1.000000  0.003486  ... -0.056943   \n",
       "acc60          0.011784  0.001396  0.003486  1.000000  ... -0.031837   \n",
       "stoch12        0.099823  0.134281  0.262216  0.451728  ... -0.065496   \n",
       "stochDiff6012  0.129730  0.194854  0.417469  0.403139  ... -0.091359   \n",
       "RSIhl_diff     0.019105  0.023221  0.026310 -0.011032  ... -0.592633   \n",
       "RSIhl_diffROC  0.004153  0.005933 -0.002836 -0.001631  ... -0.268701   \n",
       "vol           -0.008032 -0.006791 -0.043577 -0.028476  ...  0.402226   \n",
       "vol10          0.006932 -0.000592 -0.039403 -0.031610  ...  0.294425   \n",
       "vol15          0.016860  0.009675 -0.032366 -0.032690  ...  0.211861   \n",
       "vol30          0.012062  0.021460 -0.007295 -0.033583  ...  0.074166   \n",
       "vol60          0.005914  0.009768  0.012955 -0.019555  ... -0.050688   \n",
       "volD10        -0.032221 -0.023755 -0.019910 -0.006221  ...  0.545014   \n",
       "volD15        -0.042100 -0.033771 -0.030971 -0.009095  ...  0.701749   \n",
       "volD30        -0.025325 -0.038965 -0.052715 -0.017033  ...  0.894664   \n",
       "volD60        -0.014793 -0.021239 -0.056943 -0.031837  ...  1.000000   \n",
       "vpm5           0.191872  0.152865  0.102190  0.072164  ... -0.034729   \n",
       "vpm10          0.320990  0.253008  0.163888  0.108224  ... -0.040941   \n",
       "vpm15          0.125447  0.335755  0.213747  0.140920  ... -0.049075   \n",
       "vpm30          0.007287 -0.012689  0.339243  0.214447  ... -0.054675   \n",
       "vpm60          0.002171 -0.000914 -0.030884  0.323144  ... -0.046031   \n",
       "ToD            0.000022  0.000070 -0.000141  0.000812  ... -0.136767   \n",
       "DoW           -0.000255 -0.000236 -0.002206 -0.001878  ...  0.002594   \n",
       "MO            -0.004608 -0.004520  0.002634  0.000152  ...  0.055587   \n",
       "Dr1           -0.026688 -0.020336 -0.011194  0.000266  ... -0.002607   \n",
       "\n",
       "                   vpm5     vpm10     vpm15     vpm30     vpm60       ToD  \\\n",
       "vel5           0.399169  0.295397  0.252108  0.187244  0.122011  0.007345   \n",
       "vel10          0.261988  0.431247  0.351803  0.258081  0.170056  0.010114   \n",
       "vel15          0.211814  0.339981  0.446810  0.313587  0.208718  0.012498   \n",
       "vel30          0.146675  0.227690  0.295953  0.459010  0.297900  0.017696   \n",
       "vel60          0.105265  0.158133  0.204836  0.309917  0.452845  0.025193   \n",
       "acc5           0.301999 -0.006754  0.009999  0.010539  0.005033  0.000411   \n",
       "acc10          0.191872  0.320990  0.125447  0.007287  0.002171  0.000022   \n",
       "acc15          0.152865  0.253008  0.335755 -0.012689 -0.000914  0.000070   \n",
       "acc30          0.102190  0.163888  0.213747  0.339243 -0.030884 -0.000141   \n",
       "acc60          0.072164  0.108224  0.140920  0.214447  0.323144  0.000812   \n",
       "stoch12        0.302808  0.408260  0.475668  0.566148  0.538376  0.012769   \n",
       "stochDiff6012  0.251414  0.350791  0.411406  0.479059  0.379592  0.012056   \n",
       "RSIhl_diff     0.023018  0.023544  0.025213  0.017613  0.001776  0.180068   \n",
       "RSIhl_diffROC  0.008598 -0.001552  0.000344 -0.004357 -0.002434  0.005355   \n",
       "vol           -0.020267 -0.025863 -0.031096 -0.041812 -0.050274  0.037538   \n",
       "vol10         -0.019573 -0.025661 -0.031523 -0.043353 -0.052847  0.043069   \n",
       "vol15         -0.019550 -0.025131 -0.031419 -0.044165 -0.054638  0.048250   \n",
       "vol30         -0.019288 -0.024235 -0.030175 -0.044407 -0.057573  0.062838   \n",
       "vol60         -0.021449 -0.026577 -0.031855 -0.043558 -0.058607  0.091407   \n",
       "volD10        -0.035195 -0.014472 -0.019168 -0.013615 -0.008109 -0.044984   \n",
       "volD15        -0.036479 -0.028739 -0.026636 -0.019305 -0.012053 -0.060366   \n",
       "volD30        -0.034789 -0.036498 -0.042076 -0.034900 -0.024396 -0.088542   \n",
       "volD60        -0.034729 -0.040941 -0.049075 -0.054675 -0.046031 -0.136767   \n",
       "vpm5           1.000000  0.599395  0.477815  0.329781  0.207445  0.004742   \n",
       "vpm10          0.599395  1.000000  0.739195  0.474857  0.292856  0.003320   \n",
       "vpm15          0.477815  0.739195  1.000000  0.612409  0.383153  0.007189   \n",
       "vpm30          0.329781  0.474857  0.612409  1.000000  0.598460  0.012997   \n",
       "vpm60          0.207445  0.292856  0.383153  0.598460  1.000000  0.024778   \n",
       "ToD            0.004742  0.003320  0.007189  0.012997  0.024778  1.000000   \n",
       "DoW           -0.007764 -0.009220 -0.010489 -0.012419 -0.013850  0.000294   \n",
       "MO            -0.019992 -0.024274 -0.029213 -0.039529 -0.050378  0.044723   \n",
       "Dr1           -0.005129 -0.003280 -0.003253  0.000037 -0.003207  0.006287   \n",
       "\n",
       "                    DoW        MO       Dr1  \n",
       "vel5          -0.006288 -0.005597 -0.030087  \n",
       "vel10         -0.008862 -0.006926 -0.030580  \n",
       "vel15         -0.010801 -0.006752 -0.021263  \n",
       "vel30         -0.015124 -0.005017 -0.009727  \n",
       "vel60         -0.019204 -0.009746 -0.002594  \n",
       "acc5          -0.000164 -0.001095 -0.012246  \n",
       "acc10         -0.000255 -0.004608 -0.026688  \n",
       "acc15         -0.000236 -0.004520 -0.020336  \n",
       "acc30         -0.002206  0.002634 -0.011194  \n",
       "acc60         -0.001878  0.000152  0.000266  \n",
       "stoch12       -0.014696 -0.010189 -0.007544  \n",
       "stochDiff6012 -0.008342 -0.010490 -0.009730  \n",
       "RSIhl_diff    -0.020364 -0.138535 -0.006143  \n",
       "RSIhl_diffROC  0.000161  0.001681 -0.010908  \n",
       "vol            0.037112  0.546729 -0.012484  \n",
       "vol10          0.039122  0.573243 -0.011404  \n",
       "vol15          0.040585  0.591939 -0.010286  \n",
       "vol30          0.043465  0.622879 -0.011967  \n",
       "vol60          0.046908  0.642290 -0.013805  \n",
       "volD10         0.000776  0.018263  0.002834  \n",
       "volD15         0.001110  0.019381  0.000511  \n",
       "volD30         0.001874  0.026703  0.001492  \n",
       "volD60         0.002594  0.055587 -0.002607  \n",
       "vpm5          -0.007764 -0.019992 -0.005129  \n",
       "vpm10         -0.009220 -0.024274 -0.003280  \n",
       "vpm15         -0.010489 -0.029213 -0.003253  \n",
       "vpm30         -0.012419 -0.039529  0.000037  \n",
       "vpm60         -0.013850 -0.050378 -0.003207  \n",
       "ToD            0.000294  0.044723  0.006287  \n",
       "DoW            1.000000  0.001395 -0.000859  \n",
       "MO             0.001395  1.000000 -0.005728  \n",
       "Dr1           -0.000859 -0.005728  1.000000  \n",
       "\n",
       "[32 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
