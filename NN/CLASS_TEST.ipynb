{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTED FEATURES: \n",
      "Index(['FullK', 'diffKD', 'OB', 'OS', 'vol', 's15', 's30', 's60', 'ToD',\n",
      "       'Inertias', 'percBB', 'spreadRSI', 'ADX', 'RSI', 'Wpercent', 'acc'],\n",
      "      dtype='object')\n",
      "TESTING FOR: \n",
      "bull15\n",
      "OCCURANCES IN RAW DATA FOR bull15: \n",
      "{'in': 23257, 'up': 2443}\n",
      "ins -\t 23257 \n",
      "insMatch -\t 2443 \n",
      "non-ins -\t 2443\n",
      "PERCENT & WEIGHTS:\n",
      "INS\t-\t50.0 %\n",
      "Non-INS\t-\t50.0 %\n",
      "OCCURANCES IN RAW DATA FOR bull15: \n",
      "{'in': 2443, 'up': 2443}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m    115\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39msaving\u001b[38;5;241m.\u001b[39mregister_keras_serializable()\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_loss\u001b[39m(y_true, y_pred):\n\u001b[0;32m    119\u001b[0m         \n\u001b[0;32m    120\u001b[0m         \u001b[38;5;66;03m# Standard binary cross-entropy\u001b[39;00m\n\u001b[0;32m    121\u001b[0m         bce \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mbinary_crossentropy(y_true, y_pred)\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;66;03m# Define penalties for false negatives (y_true = 1, y_pred = 0)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras' has no attribute 'saving'"
     ]
    }
   ],
   "source": [
    "#JJ McCauley + LOGAN KELSCH \n",
    "#TEST NN 1\n",
    "\n",
    "#IMPORT LIBRARIES-------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import GlorotUniform\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "#hahaha dont turn this on with high epoch or else\n",
    "#tf.config.experimental.set_memory_growth\n",
    "\n",
    "#LOAD DATA FROM CSV-------------------------------------------------------\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('NEED_TO_APPEND.csv')\n",
    "\n",
    "\n",
    "#testing random feature drops\n",
    "data = data.drop(columns='FT')\n",
    "#data = data.drop(columns='FullK')\n",
    "#data = data.drop(columns='diffKD')\n",
    "#data = data.drop(columns='OB')\n",
    "#data = data.drop(columns='OS')\n",
    "#data = data.drop(columns='vol')\n",
    "#data = data.drop(columns='s15')\n",
    "#data = data.drop(columns='s30')\n",
    "#data = data.drop(columns='s60')\n",
    "#data = data.drop(columns='ToD')\n",
    "#data = data.drop(columns='Inertias')\n",
    "#data = data.drop(columns='percBB')\n",
    "#data = data.drop(columns='spreadRSI')\n",
    "#data = data.drop(columns='ADX')\n",
    "#data = data.drop(columns='RSI')\n",
    "#data = data.drop(columns='Wpercent')\n",
    "#data = data.drop(columns='acc')\n",
    "\n",
    "#TEMP DROP PRE-DUAL-OUTPUT NN\n",
    "\n",
    "#data = data.drop(columns='bull15')\n",
    "data = data.drop(columns='bear15')\n",
    "\n",
    "data = data.drop(columns='bull30')\n",
    "data = data.drop(columns='bear30')\n",
    "\n",
    "data = data.drop(columns='bull60')\n",
    "data = data.drop(columns='bear60')\n",
    "\n",
    "#confirming X and Y features post training\n",
    "Xfeatures = data.columns[:-1]\n",
    "Yfeatures = data.columns[-1]\n",
    "print(\"TESTED FEATURES: \")\n",
    "print(Xfeatures)\n",
    "print(\"TESTING FOR: \")\n",
    "print(Yfeatures)\n",
    "\n",
    "#DATA OPTIMIZATION------------------------------------------------------\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(data.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "NinRows = data.drop(data[data['bull15'] == 'in'].index)\n",
    "\n",
    "inRows = data.drop(data[data['bull15'] != 'in'].index)\n",
    "\n",
    "inRowsMatch = inRows.loc[0:2740]\n",
    "\n",
    "print('ins -\\t',inRows.index.size,'\\ninsMatch -\\t',\\\n",
    "      inRowsMatch.index.size,'\\nnon-ins -\\t',NinRows.index.size)\n",
    "\n",
    "optData = pd.concat([NinRows, inRowsMatch],axis=0)\n",
    "\n",
    "percIn = inRowsMatch.size/optData.size\n",
    "percNin = NinRows.size/optData.size\n",
    "weight_for_0 = .5\n",
    "weight_for_1 = .5\n",
    "cw = {0: weight_for_1, 1: weight_for_0}\n",
    "\n",
    "print(\"PERCENT & WEIGHTS:\\nINS\\t-\\t\",percIn*100,\" %\\nNon-INS\\t-\\t\",percNin*100,\" %\",sep='')\n",
    "\n",
    "print(\"OCCURANCES IN RAW DATA FOR \", Yfeatures, \": \", sep='')\n",
    "unique, counts = np.unique(optData.iloc[:, -1].values, return_counts=True)\n",
    "print(dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "\n",
    "#PROCESS THE DATA-------------------------------------------------------\n",
    "\n",
    "# Separate features and target\n",
    "X = optData.iloc[:, :-1].values\n",
    "y = optData.iloc[:, -1].values\n",
    "\n",
    "\n",
    "\n",
    "#Encoding data\n",
    "labelencoder = LabelBinarizer()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "        \n",
    "        # Standard binary cross-entropy\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        # Define penalties for false negatives (y_true = 1, y_pred = 0)\n",
    "        false_negatives_penalty = 0.0  # Penalty for false negatives\n",
    "        false_positives_penalty = 5.0  # Penalty for false positives\n",
    "        \n",
    "        # Define reward for true positives (y_true = 1, y_pred = 1)\n",
    "        true_positives_reward = -2.0  # Negative value to reduce the loss when TP happens\n",
    "\n",
    "        # Calculate false negatives and false positives\n",
    "        false_negatives = y_true * (1 - y_pred)\n",
    "        false_positives = (1 - y_true) * y_pred\n",
    "        \n",
    "        # Calculate true positives\n",
    "        true_positives = y_true * y_pred\n",
    "        \n",
    "        # Apply penalties and rewards\n",
    "        penalties = false_negatives_penalty * false_negatives + false_positives_penalty * false_positives\n",
    "        rewards = true_positives_reward * true_positives\n",
    "        \n",
    "        # Return combined loss (penalize FNs and FPs, reward TPs)\n",
    "        return bce + penalties + rewards\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('tupleTrain.keras',custom_objects={\"custom_loss_fn\":custom_loss})\n",
    "\n",
    "\n",
    "\n",
    "#predicting the test set results\n",
    "threshold = 0.5\n",
    "y_pred = loaded_model.predict(X)\n",
    "y_pred = (y_pred > threshold)\n",
    "\n",
    "#making a confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "\n",
    "[print(x) for x in y_pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
